# Java 基础

## 面向对象

- 什么是面向对象
  - 面向对象，是把我们要做的事情当作一个整体，我们所关心的是这个整体，在整个整体上发生的是行为和属性，而相对的面向对象，所关心的不是这个整体，而是具体的行为和属性，一步一步怎么实现的调用，哪些函数。
  - 而在我的认知中，面向对象在实际的代码中是更有利于全局开发的，一个登陆模块可以当成是一个对象、一个订单模块可以当作是一个对象，这样有利于我们思维理解整个项目，而如果是用面向过程理解整个项目，那么是不好的，因为它的关注点不在项目，而在具体的实现步骤。
- 面向对象基本特征
  - 面向对象的基本特征是**封装、继承、多态**、抽象，这个是从实际代码逻辑中进行解释的。
  - **封装**就是，就代码封装在内部，构成一个整体功能，只提供某个方法给外面访问，在php中我们写的public function ；protected function 等就是给该函数方法设置访问权限，而function内部就是存放你这块代码的功能逻辑。
  - **继承**：继承是解决代码的复用，是类和类之间的关系，使得子类具备父类中得方法，比如有一个类方法，他的功能是买菜，还有一个类，他的功能是洗菜，如果洗菜得类继承了买菜得类，那么原本这个类只有洗菜功能，现在同样具备买菜功能。
  - **多态**：多态其实就是同一个方法，外面同一种请求，但是环境不同，该方法对外响应得结果是不同，举个例子，在电脑桌面上粘贴复制和和在记事本中也是粘贴复制，这个行为都是一样得，但是结果是不一样得，这个就是多态。
  - **抽象**：其实完成一个项目，只有面向对象是不可能完成得，我来讲一下面向对象关注点和面向过程关注点，某个公司要在市场上运行，要这A区部署什么，B区在部署什么，C区部署什么，共同配合，他看的是整体，大局发展观，这个就是面向对象，而A区具体执行什么，哪些过程，每一步环节怎么做，这个是面向过程，这个就是一种抽象，面向对象不看过程，我只要看着要达到这个目标要有哪些部署行为和属性，整体而言的，所以自然关注不到哪些与主题关系不大的东西。但并不是面向过程不中要，面向对象可以想象成具体指导某件事的方法论，而面向过程就是执行具体的方法论。

参考文档：

1. [什么是面向对象，面向对象有哪些特征？- 知乎](https://zhuanlan.zhihu.com/p/94378439)

## 父子类加载顺序

1. 父类静态成员变量
2. 父类静态代码块
3. 子类静态成员变量
4. 子类静态代码块
5. 父类非静态成员变量 
6. 父类非静态代码块
7. 父类构造方法
8. 子类非静态成员变量
9. 子类非静态代码块
10. 子类构造方法

这里帮大家小结几个特点：

1. 成员变量 > 代码块 > 构造方法（构造器）。
2. 静态（共有） > 非静态（私有）。
3. 子类静态 > 父类非静态（私有）。

参考文档：

1. [Java父子类加载顺序｜8月更文挑战 - 掘金](https://juejin.cn/post/6991707135117967373)

## final

- final在Java中是一个保留的关键字，可以声明成员变量、方法、类以及本地变量。
  - 一旦你将引用声明作final，你将不能改变这个引用了，编译器会检查代码，如果你试图将变量再次初始化的话，编译器会报编译错误。
- 使用方式
  - **final 变量**
    - final成员变量表示常量，只能被赋值一次，赋值后值不再改变（final要求地址值不能改变）
      - 当final修饰一个基本数据类型时，表示该基本数据类型的值一旦在初始化后便不能发生变化；
      - 如果final修饰一个引用类型时，则在对其初始化之后便不能再让其指向其他对象了，但该引用所指向的对象的内容是可以发生变化的。
        - 本质上是一回事，因为引用的值是一个地址，final要求值，即地址的值不发生变化。
    - final修饰一个成员变量（属性），必须要显示初始化。这里有两种初始化方式
      - 一种是在变量声明的时候初始化；
      - 第二种方法是在声明变量的时候不赋初值，但是要在这个变量所在的类的所有的构造函数中对这个变量赋初值。
    - **还可以修饰形参变量、局部变量，表示作用域内不可变**
  - **final 方法**
    - 使用final方法的原因有两个。
      - 第一个原因是把方法锁定，以防任何继承类修改它的含义，**不能被重写**；
      - 第二个原因是效率，**final方法比非final方法要快**，因为在编译的时候已经**静态绑定**了，不需要在运行时再动态绑定。
      - （注：**类的private方法会隐式地被指定为final方法**）
  - **final 类**
    - 当用final修饰一个类时，表明**这个类不能被继承**。
    - final类中的成员变量可以根据需要设为final，但是要注意**final类中的所有成员方法都会被隐式地指定为final方法**。
    - 在使用final修饰类的时候，要注意谨慎选择，除非这个类真的在以后不会用来继承或者出于安全的考虑，尽量不要将类设计为final类。
- **final关键字的好处：**
  - （1）final关键字提高了性能。JVM和Java应用都会缓存final变量。
  - （2）final变量可以安全的在多线程环境下进行共享，而不需要额外的同步开销。
  - （3）使用final关键字，JVM会对方法、变量及类进行优化。
- **关于final的重要知识点**
  - final关键字可以用于成员变量、本地变量、方法以及类。
  - final成员变量必须在声明的时候初始化或者在构造器中初始化，否则就会报编译错误。
  - 你不能够对final变量再次赋值。
  - 本地变量必须在声明时赋值。（final 形参的话不需要，调用方法的时候入参会传）
  - **在匿名类中所有变量都必须是final变量**（等效的也可以）。
  - final方法不能被重写。
  - final类不能被继承。
  - final关键字不同于finally关键字，后者用于异常处理。
  - final关键字容易与finalize()方法搞混，后者是在Object类中定义的方法，是在垃圾回收之前被JVM调用的方法。
  - **接口中声明的所有变量本身是final的**。（默认 public static final 的）
  - **final和abstract这两个关键字是反相关的**，final类就不可能是abstract的。
  - **final方法在编译阶段绑定，称为静态绑定(static binding)**。
  - 没有在声明时初始化final变量的称为**空白final变量(blank final variable)**，它们必须在构造器中初始化，或者调用this()初始化。不这么做的话，编译器会报错“final变量(变量名)需要进行初始化”。
  - 将类、方法、变量声明为final能够提高性能，这样JVM就有机会进行估计，然后优化。
  - 按照Java代码惯例，final变量就是常量，而且通常常量名要大写。
  - 对于集合对象声明为final指的是引用不能被更改，但是你可以向其中增加，删除或者改变内容。
- **final原理**
  - 对于final域，编译器和处理器要遵守两个重排序规则：
    1. 在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序
       - （先写入final变量，后调用该对象引用）
       - 原因：**编译器会在final域的写之后，插入一个StoreStore屏障**
    2. 初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。
       - （先读对象的引用，后读final变量）
       - **编译器会在读final域操作的前面插入一个LoadLoad屏障**
  - **第一种情况**
    - 写普通域的操作被编译器重排序到了构造函数之外
    - 而写 final 域的操作，被写 final 域的重排序规则“限定”在了构造函数之内，读线程 B 正确的读取了 final 变量初始化之后的值。
    - 写 final 域的重排序规则可以确保：**在对象引用为任意线程可见之前，对象的 final 域已经被正确初始化过了，而普通域不具有这个保障**。
  - **第二种情况**
    - 读对象的普通域的操作被处理器重排序到读对象引用之前
    - 而读 final 域的重排序规则会把读对象 final 域的操作“限定”在读对象引用之后，此时该 final 域已经被 A 线程初始化过了，这是一个正确的读取操作。
    - 读 final 域的重排序规则可以确保：**在读一个对象的 final 域之前，一定会先读包含这个 final 域的对象的引用**。
  - **如果 final 域是引用类型**
    - 对于引用类型，写 final 域的重排序规则对编译器和处理器增加了如下约束：
      - **在构造函数内对一个 final 引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序**。
- **final、finally、 finalize 区别**
  - final 可以用来修饰类、方法、变量，分别有不同的意义，final修饰的class代表不可以继承扩展，final的变量是不可以修改的，而final的方法也是不可以重写的（override）。
  - finally 则是Java保证重点代码一定要被执行的一种机制。我们可以使用try-finally或者try-catch-finally来进行类似关闭JDBC连接、保证unlock锁等动作。
  - finalize 是基础类java.lang.Object的一个方法，它的设计目的是保证对象在被垃圾收集前完成特定资源的回收。finalize机制现在已经不推荐使用，并且在JDK 9开始被标记为deprecated。

参考文档：

1. [JAVA面试50讲之2：final关键字的底层原理是什么？- 腾讯云](https://cloud.tencent.com/developer/article/1379380)

## 反射

- 具体使用、API
  - Class 类
    - **Class 实例的三种方式**
      - .class
      - 对象 .getClass()
      - Class.forName()
    - 常用方法
      - newInstance() 通过类的无参构造方法创建对象
      - getDeclaredXxxs() 获得类的所有的（属性/构造器/方法等）
      - getFields() 获得类的public类型的属性。
      - getField(String name) 获得类的指定属性
      - getMethods() 获得类的public类型的方法
      - getMethod (String name,Class [] args) 获得类的指定方法
      - getConstrutors() 获得类的public类型的构造方法
      - getConstrutor(Class[] args) 获得类的特定构造方法
      - getName() 获得类的完整名字
      - getPackage() 获取此类所属的包
      - getSuperclass() 获得此类的父类对应的Class对象
  - Field 类
    - set()
    - get()
  - Method 类
    - invoke()
  - Constructor 类
    - newInstance()
  - AccessibleObject
    - **setAccessible(boolean)**
- 使用场景
  - Spring AOP 动态代理
  - BeanUtils 拷贝
  - Spring IOC 加载 XML 配置文件
  - Spring 等框架的注解式使用
- 原理
  - 在程序运行时动态加载类并获取类的详细信息，从而操作类或对象的属性和方法。
  - 本质是 JVM 得到 class 对象之后，再通过 class 对象进行反编译，从而获取对象的各种信息。
- 缺点
  - 安全
  - 性能

参考文档：

1. [2024年面试准备-1-Java篇.md](./2024年面试准备-1-Java篇.md) 反射

## Channel

- 通道（Channel）是 **java.nio** 的第二个主要创新。

  - 它们既不是一个扩展也不是一项增强，而是全新、极好的Java I/O示例，提供与I/O服务的直接连接。
  - Channel用于在字节缓冲区和位于通道另一侧的实体（通常是一个文件或套接字）之间有效地传输数据。

- 通道是访问I/O服务的导管。

  - I/O可以分为广义的两大类别：File I/O和Stream I/O。
  - 那么相应地有两种类型的通道也就不足为怪了，它们是文件（file）通道和套接字（socket）通道。
  - 我们看到在api里有一个 **FileChannel** 类和三个 socket 通道类：**SocketChannel**、**ServerSocketChannel** 和 **DatagramChannel**。

- 通道可以以多种方式创建。

  - Socket 通道有可以直接创建新 socket 通道的工厂方法。
  - 但是一个FileChannel对象却只能通过在一个打开的RandomAccessFile、FileInputStream或FileOutputStream对象上调用getChannel( )方法来获取。**你不能直接创建一个 FileChannel 对象**。

- SocketChannel

  - 可以设置 SocketChannel 为非阻塞模式（non-blocking mode）.设置之后，就可以在异步模式下调用connect(), read() 和write()了。

    ```java
    socketChannel.configureBlocking(false);
    ```

  - 如果 SocketChannel 在非阻塞模式下，此时调用connect()，该方法可能在连接建立之前就返回了。为了确定连接是否建立，可以调用 finishConnect() 的方法。

- **Scatter/Gather**

  - 通道提供了一种被称为Scatter/Gather的重要新功能（有时也被称为矢量I/O）。它是指在多个缓冲区上实现一个简单的I/O操作。
  - 对于一个write操作而言，数据是从几个缓冲区按顺序抽取（称为gather）并沿着通道发送的。缓冲区本身并不需要具备这种gather的能力（通常它们也没有此能力）。该gather过程的效果就好比全部缓冲区的内容被连结起来，并在发送数据前存放到一个大的缓冲区中。
  - 对于read操作而言，从通道读取的数据会按顺序被散布（称为scatter）到多个缓冲区，将每个缓冲区填满直至通道中的数据或者缓冲区的最大空间被消耗完。
  - scatter / gather经常用于需要将传输的数据分开处理的场合，例如传输一个由消息头和消息体组成的消息，你可能会将消息体和消息头分散到不同的buffer中，这样你可以方便的处理消息头和消息体。
  - 使用得当的话，Scatter/Gather会是一个极其强大的工具。它允许你委托操作系统来完成辛苦活：将读取到的数据分开存放到多个存储桶（bucket）或者将不同的数据区块合并成一个整体。
    - 这是一个巨大的成就，因为操作系统已经被高度优化来完成此类工作了。它节省了您来回移动数据的工作，也就避免了缓冲区拷贝和减少了您需要编写、调试的代码数量。

参考文档：

1. [详解java NIO之Channel（通道） - 知乎](https://zhuanlan.zhihu.com/p/351327314)

# Java 集合

## ArrayList

- 继承关系

  ```mermaid
  classDiagram
  	AbstractList <|-- ArrayList
  	List <|.. ArrayList
  	RandomAccess <|.. ArrayList
  	Serializable <|.. ArrayList
  	Cloneable <|.. ArrayList
  	AbstractCollection <|-- AbstractList
  	Collection <|.. AbstractCollection
  	Collection <|-- List
  	Iterable <|-- Collection
  ```

  - 继承 `AbstractList<E>` 抽象类
    - 继承 `AbstractCollection<E>` 抽象类
      - 实现 `Collection<E>` 接口
        - 继承 `Iterable<T>` 接口
  - 实现 `List<E>` 接口
    - 继承 `Collection<E>` 接口
      - 继承 `Iterable<T>` 接口
  - 实现 `RandomAccess` 接口
  - 实现 `Serializable` 接口
  - 实现 `Cloneable` 接口

- 和 LinkedList 对比

  - ArrayList 基于数组，LinkedList 基于双向链表
    - 随机访问：ArrayList 实现 RandomAccess 标识接口，按索引访问 O(1)；LinkedList O(n)
    - 插入删除：ArrayList 插入删除 O(n)【除了尾插尾删 O(1)】; LinkedList 插入删除 O(n)【除了头尾插删 O(1)】（因为不能随机访问，得 O(n) 到达指定位置；LinkedList 注意特殊情况：使用 Iterator 删除时就都是 O(1) 了）
    - 内存占用：ArrayList 开辟的空间大于等于实际使用元素个数；LinkedList 元素节点大小比 ArrayList 大
  - 线程安全：都是线程不安全的
  - 一般场景都是直接用 ArrayList，LinkedList 对比之下基本没优势。（15年 LinkedList 作者发推说自己写了但自己也不用）
    - LinkedList 优势
      - 作为队列（Queue）使用
      - 作为栈使用
      - 头插删、Iterator 删除时 O(1)
      - 无需扩容

- 初始化长度

  - 以无参数构造方法创建 `ArrayList` 时
    - 实际上初始化赋值的是一个空数组。
    - 当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。（即扩容逻辑）
  - int 有参构造方法直接指定大小（小于0 异常），0 的时候也是空数组
  - Collection 有参构造方法长度和入参大小一样

- 扩容

  - **空数组直接扩容到 10**
  - 其他按照 **1.5 倍扩容**（grow() `int newCapacity = oldCapacity + (oldCapacity >> 1)`）
    - 注意例外的情况：比如 1，1.5 倍后还是 1。这时走特殊判断：检查新容量是否大于最小需要容量（原元素数 + 1），若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量。即 1 扩容到 2。
  - 如果新容量 - `MAX_ARRAY_SIZE`（`= Integer.MAX_VALUE - 8`）> 0，则执行 `hugeCapacity(minCapacity)` 方法（minCapacity 就是原元素数量 + 1）：
    - 如果入参 `minCapacity` 大于 `MAX_ARRAY_SIZE`，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 `MAX_ARRAY_SIZE`。
  - 可以使用 ensureCapacity() 方法在大量添加元素前主动指定大小

- 去重

  - 用 HashSet + for 循环去重
  - Stream：`lits.stream().distinct().collect(Collections.toList())`

参考文档：

1. [2024年面试准备-2-集合篇.md](./2024年面试准备-2-集合篇.md) ArrayList

## LinkedList

- 和 ArrayList 对比

  - 参考上面 ArrayList 中的具体分析

- 继承关系

  - ```mermaid
    classDiagram
        AbstractSequentialList <|-- LinkedList
        AbstractList <|-- AbstractSequentialList
        AbstractCollection <|-- AbstractList
        Collection <|.. AbstractCollection
        List <|.. LinkedList
        List <|.. AbstractList
        Collection <|-- List
        Cloneable <|.. LinkedList
        Serializable <|.. LinkedList
        Deque <|.. LinkedList
        Queue <|-- Deque
        Collection <|-- Queue
        Iterable <|-- Collection
    ```

- 优势

  - 作为队列（Queue）使用
    - 可以使用 **ArrayDeque** 替换（推荐，刷过力扣看过大佬代码就知道）
      - 数据结构：**循环数组**
      - 初始化和扩容：**初始 16，扩容 2 倍**
      - 也是线程不安全
      - 查询 O(1) 插入删除 O(n)
      - 插入新元素时不像 LinkedList 需要创建新的节点对象，但是可能扩容
  - 作为栈使用
    - 同样可以替换为 ArrayDeque 类（推荐）
    - 不推荐使用 Stack 类
      - 继承 Vector，方法有锁，效率慢
  - 头插删、Iterator 删除时 O(1)
  - 无需扩容

参考文档：

1. [2024年面试准备-2-集合篇.md](./2024年面试准备-2-集合篇.md) LinkedList

## HashMap

- 继承关系

  ```mermaid
  classDiagram
  	AbstractMap <|-- HashMap
  	Map <|.. AbstractMap
  	Map <|.. HashMap
  	Cloneable <|.. HashMap
  	Serializable <|.. HashMap
  ```

- 实现原理

  - 数组 + 链表/红黑树

    - table 数组：

      ```java
      transient Node<K,V>[] table;
      ```

    - Java 8 引入的红黑树

- **key 可以为 null，value 也可以为 null**

- 哈希冲突

  - HashMap 使用的是**链地址法**
  - 所有的方法
    - 开放定址法
      - 线性探测法
        - ThreadLocal：内部的 ThreadLocalMap 就是用的这种
      - 平方探测法（二次探测）
    - 再哈希法
    - 链地址法（也叫拉链法）
      - Redis 的 hash 也是
    - 建立公共溢出区

- put() 方法详解

  - 利用 key 的 hashCode() 在 hash() 中运算出 hash 值，再利用 & 计算出数组的 index;
  - 如果没碰撞直接放到桶里
  - 如果碰撞了，在链表或红黑树中判断是否存在 key。如果节点已经存在就替换 old value (保证 key 的唯⼀性)
  - 如果不存在，则如果是链表的形式就使用尾插法插入新节点（Java 8 起）
    - Java 7 的时候还是头插法，多线程扩容时会有链表死循环的风险
  - 如果插入会导致链表过长(大于等于 `TREEIFY_THRESHOLD`)，就把链表转换成红黑树（Java 8 起）；
  - 如果桶满了(超过 loadFactor * currentCapacity)，就要 resize()

- 重要变量

  - `DEFAULT_INITIAL_CAPACITY` 
    - Table 数组的初始化长度： `1 << 4` `2^4=16`
  - `MAXIMUM_CAPACITY`
    - Table 数组的最大长度： `1<<30` `2^30=1073741824`
  - `DEFAULT_LOAD_FACTOR`
    - 负载因子：默认值为 `0.75`。 当元素的总个数 > 当前数组的长度 * 负载因子。数组会进行扩容，扩容为原来的两倍
  - `TREEIFY_THRESHOLD`
    - 链表树化阈值： 默认值为 `8` 。表示在一个node（Table）节点下的值的个数大于 8 时候，会将链表转换成为红黑树。
  - `UNTREEIFY_THRESHOLD`
    - 红黑树链化阈值： 默认值为 `6` 。 表示在进行扩容期间，单个 Node 节点下的红黑树节点的个数小于 6 时候，会将红黑树转化成为链表。
  - `MIN_TREEIFY_CAPACITY = 64`
    - **最小树化阈值**，当 Table 所有元素超过改值，才会进行树化（为了防止前期阶段频繁扩容和树化过程冲突）。否则进行扩容

- 为什么长度要是 2 的 n 次方？

  - 如何保证的

    - 初始长度
      - 默认 2^4 = 16
      - 指定长度时，设置 threshold（扩容的阈值）的 tableSizeFor() 方法也会把值调整为 `MAXIMUM_CAPACITY`（2 ^ 30）以内的最接近入参的 2 的 n 次方数字。第一次 put 引起扩容初始化 table 时，就会以这个值为 table 数组大小
    - 扩容按 2 倍扩容
      - 一个桶中的元素正好只会对应分散到两个新桶

  - 计算 hash 值

    - 不是简单调用 hashCode()：

      ```java
      static final int hash(Object key) {
       	int h;
          return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
      }
      ```

  - 目的：都是 2 的 n 次方的话，计算所属哈希桶时，**可以使用 & 按位与优化 % 取模计算**（hash & 15 == hash % 16）

- Java 7 和 Java 8 的 HashMap 区别

  - 哈希冲突：链表 -> 链表/红黑树

    - Java 8 引入树化机制：链表长度大于等于 8 时，全表超过 64 个节点后，转红黑树；没超过最小树化阈值就扩容；红黑树节点数量小于等于 6 时，退化回链表。

  - 插入链表时：头插法 -> 尾插法

    - 避免多线程下扩容可能形成死循环链表

  - 扩容和插入时机：扩容后插入，转移数据时单独计算 -> 扩容前插入，转移数据时统一计算

    - 统一计算也是防止了扩容时形成死循环链表

  - 扩容后的存储位置：按 hashCode() 扰动处理 再 (h & length -1) 重新计算一遍 -> 扩容后的桶索引要么是原索引，要么是原索引+原容量

  - hash() 的实现：四次右移连续 ^  -> hashCode() ^ 一次右移

    - Java 7：

      ```java
      h ^= (h >>> 20) ^ (h >>> 12);
      return h ^ (h >>> 7) ^ (h >>> 4);
      ```

    - Java 8：

      ```java
      int h;
      return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
      ```

  - 初始化方法：单独方法 `inflateTable()` -> 直接在第一次 `put()` 时利用 `resize()` 初始化 table


参考文档：

1. [2024年面试准备-2-集合篇.md](./2024年面试准备-2-集合篇.md) HashMap

## ConcurrentHashMap

- 继承关系

  ```mermaid
  classDiagram
  	AbstractMap <|-- ConcurrentHashMap
  	Map <|.. AbstractMap
  	ConcurrentMap <|.. ConcurrentHashMap
  	Map <|-- ConcurrentMap
  	Serializable <|.. ConcurrentHashMap
  ```

- 线程安全

- **不支持 key、value 为 null**

  - 反证 value 不能为 null
    - 如果支持 value 为 null，外界使用时需要保证 containsKey() 和 get() 两次操作之间没有其他多线程在操作，否则 get() 的时候无法判断 null 的含义是真的 value 为 null 还是没有对应的 key。
    - 这就导致用户使用 ConcurrentHashMap 集合需要自己保证 containsKey() 和 get() 操作的原子性，增加了开发负担，所以在设计上直接禁止了 null
  - 而 key 不能为 null，主要就是 JUC 作者 Doug 不喜欢 null，所以设计之初就不允许 null 的 key 存在
  - **线程安全的 ConcurrentSkipListMap、HashTable 也是一样的禁止两者为 null**

- get()

  - 不需要加锁
    - 因为相关共享变量（不管是 7 的 HashEntry 还是 8 的 Node）都用 volatile 修饰，保证了多线程下的可见性
  - Java 7 源码逻辑
    - 首先，根据 key 计算出 hash 值定位到具体的 Segment 
    - 再根据 hash 值获取定位 HashEntry 对象
    - 对 HashEntry 对象进行链表遍历，找到对应元素。
  - Java 8 源码逻辑
    - 根据计算出来的 hashcode 寻址，如果就在桶上首节点，那么直接返回值。
    - 如果是红黑树那就按照树的方式获取值。
    - 都不满足那就按照链表的方式遍历获取值。

- put()

  - Java 7 源码逻辑
    - 先定位到相应的 Segment s，然后再对 s 进行 s.put() 操作。
    - 首先会尝试获取锁 `tryLock()`，如果获取失败肯定就有其他线程存在竞争，则利用 `scanAndLockForPut()` 自旋获取锁。
    - 尝试自旋获取锁。
    - 如果重试的次数达到了 `MAX_SCAN_RETRIES` 则改为阻塞锁获取，保证能获取成功。
    - 拿到锁后，得到 table 对应位置上的 HashEntry。如果非空，则遍历该 HashEntry，如果不为空则判断传入的 key 和当前遍历的 key 是否相等，相等则覆盖旧 value
    - HashEntry 为空则需要新建一个 HashEntry 并加入到 Segment 中，同时会判断是否需要扩容。
  - Java 8 源码逻辑
    - 根据 key 计算出 hashcode，然后开始遍历 table；
    - 判断是否需要初始化；
    - f 即为当前 key 定位出的 Node，如果为 null，表示当前位置可以写入数据，**利用 CAS 尝试写入，失败则自旋保证成功**。
      - 而在执行写操作时，会先尝试使用 CAS 操作来无锁地修改数据，如果 CAS 操作失败，则会使用 **synchronized** 来获取对应 Node 的锁，并在获取锁后进行数据的修改操作。
    - 如果当前位置的 hash 值 == MOVED == -1, 说明其他线程正在扩容，则需要参与一起扩容 `helpTransfer()`。
    - 如果都不满足，则利用 synchronized 锁住 f 节点，判断是链表还是红黑树，遍历写入数据。
    - 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。

- Java 7 和 8 的区别

  - 结构：数组+链表 -> 数组+链表/红黑树
  - 节点：**HashEntry** 类 -> **Node** 类（对 val 和 next 属性使用 volatile 修饰）
  - 锁：继承 ReentrantLock 的 **Segment 类，并发度 16**（自定义的话，取大于设置值的最小 2 的幂指数） -> **没有分段锁了，采用 CAS + synchronized 实现**，并发度取决于数组大小

- 扩容、哈希冲突树化等部分参数和 HashMap 都一样

  - 链表树化阈值 8
  - 最小树化容量 64
  - 初始大小 16
  - 复杂因子 0.75

参考文档：

1. [2024年面试准备-2-集合篇.md](./2024年面试准备-2-集合篇.md) ConcurrentHashMap

## Hashtable

- 较为远古的使用Hash算法的容器结构了，现在基本已被淘汰

- 注意一下类名是 Hashtable, t 小写！

- 继承结构

  ```mermaid
  classDiagram
  	Dictionary <|--Hashtable
  	Map <|.. Hashtable
  	Cloneable <|.. Hashtable
  	Serializable <|.. Hashtable
  ```

- 线程安全

  - 几乎所有都加了 synchronized 锁
  - 不允许 key、value 为 null，理由参考 ConcurrentHashMap

- 结构：数组 + 单向链表

- 默认初始长度 11

- 默认加载因子 0.75

- 求 hash 值：

  ```java
  int hash = key.hashCode();
  int index = (hash & 0x7FFFFFFF) % tab.length;
  ```

- 扩容机制：

  - 默认的阈值是选取 `initialCapacity * loadFactor` 和 `Integer.MAX_VALUE - 8 + 1` 的最小值
  - 新的长度 = 原长度 * 2倍 + 1
  - 头插法链表

- 目前不推荐使用，单线程可以用 HashMap，多线程用 ConcurrentHashMap

参考文档：

1. [2024年面试准备-2-集合篇.md](./2024年面试准备-2-集合篇.md) Hashtable

# JVM

## 对象结构

- **查看对象头的神器: jol-core**

  - 介绍一款可以在代码中计算`java`对象的大小以及查看`java`对象内存布局的工具包:`jol-core`，`jol`为`java object layout`的缩写，即java对象布局。
  - 使用只需要到`maven`仓库`http://mvnrepository.com`搜索`java object layout`，选择想要使用的版本，将依赖添加到项目中即可。

- **对象头（Header）**

  - **对象标记：MarkWord**

    - （32 位是 4 字节 = 32 bits，64 位系统是 8 字节 = 64 bits）

    - 其内容是一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等

    - X 32 位（看一下即可，不用学了，以 64 位为准）（0：31）

    - √ 64 位重要

      - （0：45）

        ```
        |-----------------------------------------------------------------------------------------------------------------|
        |                                             Object Header(128bits)                                              |
        |-----------------------------------------------------------------------------------------------------------------|
        |                                   Mark Word(64bits)               |  Klass Word(64bits)    |      State         |
        |-----------------------------------------------------------------------------------------------------------------|
        |    unused:25|identity_hashcode:31|unused:1|age:4|biase_lock:0| 01 | OOP to metadata object |      Nomal         |
        |-----------------------------------------------------------------------------------------------------------------|
        |    thread:54|      epoch:2       |unused:1|age:4|biase_lock:1| 01 | OOP to metadata object |      Biased        |
        |-----------------------------------------------------------------------------------------------------------------|
        |                        ptr_to_lock_record:62                 | 00 | OOP to metadata object | Lightweight Locked |
        |-----------------------------------------------------------------------------------------------------------------|
        |                       ptr_to_heavyweight_monitor:62          | 10 | OOP to metadata object | Heavyweight Locked |
        |-----------------------------------------------------------------------------------------------------------------|
        |                                                              | 11 | OOP to metadata object |    Marked for GC   |
        |-----------------------------------------------------------------------------------------------------------------|
        ```

        无锁：25bit unused、31bit 对象 hashCode、1bit Cms_free、4bit 对象分代年龄、1bit（是否偏向锁）0、2bit（锁标志位）01

        偏向锁：54bit threadId（偏向锁的线程 ID）、2bit Epoch、1bit Cms_free、4bit 对象分代年龄、1bit（是否偏向锁）1、2bit（锁标志位）01

        轻量级锁：62bit 指向栈中锁的记录的指针、2bit（锁标志位）00

        重量级锁：62bit 指向重量级锁的指针、2bit（锁标志位）10

        GC 标志：62bit 空、2bit（锁标志位）11

      - oop.hpp（2：17）

        - markOop.hpp（2：43）

      - markword（64 位）分布图，对象布局、GC 回收和后面的锁升级就是对象标记 MarkWord 里面标志位的变化（5：42）

  - **类元信息（Klass Word，又叫类型指针）**：Class 对象指针

    - （32 位系统或开压缩指针是 4 字节 = 32 bits，64 位系统不开指针压缩是 8 字节 = 64 bits）
    - 其指向的位置是对象对应的Class对象（其对应的元数据对象）的内存地址

  - **数组长度**

    - （长度 4 字节 = 32 bits）
    - 如果对象是一个 Java 数组，那在对象头中还必须有一块用于记录数组长度的数据
      - 因为虚拟机可以通过普通 Java 对象的元数据信息确定Java对象的大小
      - 但是如果数组的长度是不确定的，将无法通过元数据中的信息推断出数组的大小。

- **实例数据（Instance Data）**

  - 对象实际数据（实际数据大小）
    - 这里面包括了对象的所有成员变量，其大小由各个成员变量的大小决定，比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（默认压缩指针的情况下，不压的话 8 字节）
  - 即我们在程序代码里面所定义的各种类型的字段内容，无论是从父类继承下来的，还是在子类中定义的字段都必须记录起来
    - 这部分的存储顺序会受到虚拟机分配策略参数（-XX：FieldsAllocationStyle 参数）和字段在 Java 源码中定义顺序的影响
    - HotSpot虚拟机默认的分配顺序为 longs/doubles、ints、shorts/chars、bytes/booleans、oops（Ordinary Object Pointers，OOPs），从以上默认的分配策略中可以看到，相同宽度的字段总是被分配到一起存放、
    - 在满足这个前提条件的情况下，在父类中定义的变量会出现在子类之前

- **对齐填充（Padding）**

  - 可选（按 8 字节对齐）
  - 任何对象的大小都必须是 8 字节的整数倍

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景一：Java 中的 String 类占用多大的内存空间 - Java 对象的结构
2. [《尚硅谷JUC并发编程（对标阿里P6~P7）》笔记.md]() P117
3. [java并发编程——JAVA对象头（含32位虚拟机与64位虚拟机）- CSDN](https://blog.csdn.net/e891377/article/details/108905401)

## 指针压缩

- 压缩指针相关说明命令
  - java -XX:+PrintCommandLineFlags -version
  - **默认开启压缩**说明（1：34）
    - -XX:+UseCompressedClassPointers
    - 结果（2：29）
  - 上述表示开启了类型指针的压缩，以节约空间，假如不加压缩？？？
  - 手动关闭压缩再看看
    - -XX:-UseCompressedClassPointers
    - 结果（7：55）
  - 在网上大家很多都看到过这样一句话：“**JVM内存最好不要超过32G**”。
    - **会导致压缩指针失效**
- **哪些信息会被压缩？**
  1. 对象的全局静态变量(即类属性)
  2. 对象头信息:64位平台下，原生对象头大小为16字节，压缩后为12字节
  3. 对象的引用类型:64位平台下，引用类型本身大小为8字节，压缩后为4字节
  4. 对象数组类型:64位平台下，数组类型本身大小为24字节，压缩后16字节
- **哪些信息不会被压缩？**
  1. 指向非Heap的对象指针
  2. 局部变量、传参、返回值、NULL指针
- 为什么对象指针(Klass Word，也叫类元信息、类型指针)，可以用8字节存也可以用4字节存？4字节存不会有什么问题吗？
- 分情况讨论
  - **1、不开启指针压缩**
    - 采用8字节（64位）存储真实内存地址，比之前采用4字节（32位）压缩存储地址带来的问题：
      - 增加了GC开销：64位对象引用需要占用更多的堆空间，留给其他数据的空间将会减少，
        从而加快了GC的发生，更频繁的进行GC。
      - 降低CPU缓存命中率：64位对象引用增大了，CPU能缓存的oop将会更少，从而降低了CPU缓存的效率。
  - **2、开启指针压缩**
    - 既然64位存储内存地址，会导致了这么多问题，那么我们可以不可以找一种方法，既使用之前的4字节（32位）存指针地址，又可以扩大内存的方法呢？
      - 答案就是采用指针压缩技术！！
      - 4个字节，32位，可以表示2^32 个地址，如果这个地址是真实内存地址的话，那么由于CPU寻址的最小单位是byte，也就是 2^32 byte = 4GB。
      - 如果内存地址是指向 bit的话，32位的最大寻址范围其实是 512MB，但是由于内存里，将8bit为一组划分，所以内存地址就其实是指向的8bit为一组的byte地址，所以32位可以表示的容量就扩充了8倍，就变成了4GB。
    - 4字节，8位最大表示4GB内存。那么Java是怎么做到 4个字节表示32GB呢？怎有扩大了8倍？？？
      - 这就要使用到之前提到的Java的**对齐填充机制**了。
        - Java的8字节对齐填充，就像是内存的8bit为一组，变为1byte一样。
        - 这里的压缩指针，不是真实的操作系统内存地址，而是Java进行8byte映射之后的地址，所以也相对于操作系统的指针有进行的8倍的扩容。
      - 将java堆内存进行8字节划分
        - java对象的指针地址就可以不用存对象的真实的64位地址了，而是可以存一个映射地址编号。
        - 这样4字节就可以表示出2^32个地址，而每一个地址对应的又是8byte的内存块。
          所以，再乘以8以后，一换算，就可以表示出32GB的内存空间。
    - 也就解释了为什么当**内存大于32GB时，开启指针压缩的参数会失效**！
      - 所以也网上建议大家在64位系统系下，JAVA的堆内存设置最好不要超过32G，一旦超过32G后，指针压缩就会失效，然后带来GC的触发频次变高，而且造成空间浪费。

参考文档：

1. [《尚硅谷JUC并发编程（对标阿里P6~P7）》笔记.md]() P120
2. [聊一聊JAVA指针压缩的实现原理（图文并茂，让你秒懂）- CSDN](https://blog.csdn.net/liujianyangbj/article/details/108049482)
3. [jvm压缩指针原理以及32g内存压缩指针失效详解 - CSDN](https://blog.csdn.net/lioncatch/article/details/105919666)

## 对象的访问定位

- **句柄**
  - Java 堆中将可能会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自具体的地址信息
  - 好处：reference 中存储的是稳定句柄地址
    - 在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而 reference 本身不需要被修改
- **直接指针**
  - Java 堆中对象的内存布局就必须考虑如何放置访问类型数据的相关信息，reference 中存储的直接就是对象地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销
  - 好处：速度更快
    - 它节省了一次指针定位的时间开销，由于对象访问在 Java 中非常频繁，因此这类开销积少成多也是一项极为可观的执行成本
  - **HotSpot 主要使用该方式进行对象访问**
    - 有例外情况，如果使用了 Shenandoah 收集器的话也会有一次额外的转发

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 2.3.3 对象的访问定位

## 对象什么时候进入老年代

- **大对象直接进入老年代**
  - 大对象就是指需要大量连续内存空间的Java对象，最典型的大对象便是那种很长的字符串，或者元素数量很庞大的数组
    - 本节例子中的byte[]数组就是典型的大对象
  - 大对象对虚拟机的内存分配来说就是一个不折不扣的坏消息，比遇到一个大对象更加坏的消息就是遇到一群“朝生夕灭”的“短命大对象”，我们写程序的时候应注意避免。
  - 在Java虚拟机中要避免大对象的原因
    - 在分配空间时，它容易导致内存明明还有不少空间时就提前触发垃圾收集，以获取足够的连续空间才能安置好它们
    - 当复制对象时，大对象就意味着高额的内存复制开销。
  - HotSpot 虚拟机提供了 **-XX:PretenureSizeThreshold** 参数，指定大于该设置值的对象直接在老年代分配，这样做的目的就是避免在 Eden 区及两个 Survivor 区之间来回复制，产生大量的内存复制操作。
    - -XX:PretenureSizeThreshold 参数只对 Serial 和 ParNew 两款新生代收集器有效，HotSpot 的其他新生代收集器，如 Parallel Scavenge 并不支持这个参数。
- **长期存活的对象将进入老年代**
  - 虚拟机给每个对象定义了一个对象年龄（Age）计数器，存储在对象头中
  - 对象通常在 Eden 区里诞生，如果经过第一次 Minor GC 后仍然存活，并且能被 Survivor 容纳的话，该对象会被移动到 Survivor 空间中，并且将其对象年龄设为 1 岁。对象在 Survivor 区中每熬过一次 Minor GC，年龄就增加 1 岁，当它的年龄增加到一定程度（**默认为 15**），就会被晋升到老年代中。
  - 对象晋升老年代的年龄阈值，可以通过参数 **-XX:MaxTenuringThreshold** 设置。
- **动态对象年龄判定**
  - 为了能更好地适应不同程序的内存状况，HotSpot 虚拟机并不是永远要求对象的年龄必须达到 -XX:MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 空间中**相同年龄所有对象大小的总和大于 Survivor 空间的一半**，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到 -XX:MaxTenuringThreshold 中要求的年龄。
- 题外话 - 其他分配方式：**栈上分配、TLAB、PLAB**
  - 栈上分配
    - 对于这些其他线程不会访问的对象，我们能不能让线程自己分配在它自己的栈空间上？这样不就可以节省不少交互时间了
    - 参考后续**逃逸分析**相关
  - TLAB
    - **TLAB（Thread Local Allocation Buffer），即线程本地分配缓存**。这是一块线程专用的内存分配区域，TLAB 占用的是 eden 区的空间。在 TLAB 启用的情况下（默认开启），JVM 会为每一个线程分配一块 TLAB 区域。
  - PLAB
    - **PLAB（Promotion Local Allocation Buffers），即晋升本地分配缓存**。PLAB 的作用于 TLAB 类似，都是为了加速对象分配效率，避免多线程竞争而诞生的。 只不过 **PLAB 是应用于对象晋升到 Survivor 区或老年代**。与 TLAB 类似，每个线程都有独立的 PLAB 区。
  - 对象内存分配流程
    - **尝试栈上分配 -> 尝试 TLAB 分配 -> 是否可以直接进入老年代 -> Eden 分配**


参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 3.8.2 大对象直接进入老年代 ~ 3.8.4 动态对象年龄判定

## 类加载时机和过程

- 一个类型从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期将会经历**加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（Using）和卸载（Unloading）**七个阶段
  - 其中**验证、准备、解析**三个部分统称为**连接（Linking）**
  - **加载、验证、准备、初始化和卸载这五个阶段的顺序是确定的**，类型的加载过程必须按照这种顺序按部就班地开始
    - 请注意，这里笔者写的是按部就班地“开始”，而不是按部就班地“进行”或按部就班地“完成”，强调这点是因为这些阶段通常都是互相交叉地混合进行的，会在一个阶段执行的过程中调用、激活另一个阶段。
  - **而解析阶段则不一定**：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定特性（也称为动态绑定或晚期绑定）。
- 关于在什么情况下需要开始类加载过程的第一个阶段“加载”，《Java虚拟机规范》中并没有进行强制约束，这点可以交给虚拟机的具体实现来自由把握
- 但是对于初始化阶段，《Java虚拟机规范》则是严格规定了**有且只有**六种情况必须立即对类进行“初始化”（而加载、验证、准备自然需要在此之前开始）：这六种场景中的行为称为**对一个类型进行主动引用**。
  - 遇到**new、getstatic、putstatic或invokestatic这四条字节码指令**时，如果类型没有进行过初始化，则需要先触发其初始化阶段。
    - 能够生成这四条指令的典型Java代码场景有：
      - 使用 **new 关键字**实例化对象的时候。
      - 读取或设置一个类型的**静态字段**（被final修饰、已在编译期把结果放入常量池的静态字段除外）的时候。
      - 调用一个类型的**静态方法**的时候。
  - 使用 **java.lang.reflect 包**的方法对类型进行**反射调用**的时候，如果类型没有进行过初始化，则需要先触发其初始化。
  - 当初始化类的时候，如果发现其**父类**还没有进行过初始化，则需要先触发其父类的初始化。
  - 当虚拟机启动时，用户需要指定一个要执行的**主类（包含main()方法的那个类）**，虚拟机会先初始化这个主类。
  - 当使用JDK 7新加入的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果为**REF_getStatic、REF_putStatic、REF_invokeStatic、REF_newInvokeSpecial四种类型的方法句柄**，并且这个方法句柄对应的类没有进行过初始化，则需要先触发其初始化。
  - 当一个接口中定义了JDK 8新加入的**默认方法（被default关键字修饰的接口方法）**时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 7.2 类加载的时机

## 类加载过程 1：加载

- 在加载阶段，Java虚拟机需要完成以下三件事情：
  - 通过一个类的全限定名来获取定义此类的**二进制字节流**。
    - 它并没有指明二进制字节流必须得从某个Class文件中获取，确切地说是根本没有指明要从哪里获取、如何获取。许多举足轻重的Java技术都建立在这一基础之上，例如：
      - 从ZIP压缩包中读取，这很常见，最终成为日后JAR、EAR、WAR格式的基础。
      - 从网络中获取，这种场景最典型的应用就是Web Applet。
      - 运行时计算生成，这种场景使用得最多的就是动态代理技术，在java.lang.reflect.Proxy中，就是用了ProxyGenerator.generateProxyClass()来为特定接口生成形式为“*$Proxy”的代理类的二进制字节流。
      - 由其他文件生成，典型场景是JSP应用，由JSP文件生成对应的Class文件。
      - 从数据库中读取，这种场景相对少见些，例如有些中间件服务器（如SAP Netweaver）可以选择把程序安装到数据库中来完成程序代码在集群间的分发。
      - 可以从加密文件中获取，这是典型的防Class文件被反编译的保护措施，通过加载时解密Class文件来保障程序运行逻辑不被窥探。
  - 将这个字节流所代表的静态存储结构转化为**方法区的运行时数据结构**。
  - 在内存中生成一个代表这个类的**java.lang.Class对象**，作为方法区这个类的各种数据的访问入口。
- 相对于类加载过程的其他阶段，**非数组类型的加载阶段（准确地说，是加载阶段中获取类的二进制字节流的动作）是开发人员可控性最强的阶段**。
  - 加载阶段既可以使用Java虚拟机里内置的引导类加载器来完成，也可以由用户自定义的类加载器去完成，开发人员通过定义自己的类加载器去控制字节流的获取方式（重写一个类加载器的findClass()或loadClass()方法），实现根据自己的想法来赋予应用程序获取运行代码的动态性。
- 对于数组类而言，情况就有所不同，**数组类本身不通过类加载器创建**，它是由Java虚拟机直接在内存中动态构造出来的。但数组类与类加载器仍然有很密切的关系，因为**数组类的元素类型（ElementType，指的是数组去掉所有维度的类型）最终还是要靠类加载器来完成加载**
  - 一个数组类（下面简称为C）创建过程遵循以下规则：
    - 如果数组的组件类型（Component Type，指的是数组去掉一个维度的类型，注意和前面的元素类型区分开来）是引用类型，那就递归采用本节中定义的加载过程去加载这个组件类型，数组C将被标识在加载该组件类型的类加载器的类名称空间上（这点很重要，在7.4节会介绍，一个类型必须与类加载器一起确定唯一性）。
    - 如果数组的组件类型不是引用类型（例如int[]数组的组件类型为int），Java虚拟机将会把数组C标记为与引导类加载器关联。
    - 数组类的可访问性与它的组件类型的可访问性一致，如果组件类型不是引用类型，它的数组类的可访问性将默认为public，可被所有的类和接口访问到。
  - 加载阶段结束后，Java虚拟机外部的二进制字节流就按照虚拟机所设定的格式存储在方法区之中了，方法区中的数据存储格式完全由虚拟机实现自行定义
- 加载阶段与连接阶段的部分动作（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的一部分，这两个阶段的开始时间仍然保持着固定的先后顺序。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 7.3.1 加载

## 类加载过程 2：验证

- **验证是连接阶段的第一步**，这一阶段的目的是确保Class文件的字节流中包含的信息符合《Java虚拟机规范》的全部约束要求，保证这些信息被当作代码运行后不会危害虚拟机自身的安全。
- 验证阶段大致上会完成下面四个阶段的检验动作
  - **文件格式验证**
    - 第一阶段要验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理
    - 这一阶段可能包括下面这些验证点：
      - 是否以魔数0xCAFEBABE开头。
      - 主、次版本号是否在当前Java虚拟机接受范围之内。
      - 常量池的常量中是否有不被支持的常量类型（检查常量tag标志）。
      - 指向常量的各种索引值中是否有指向不存在的常量或不符合类型的常量。
      - CONSTANT_Utf8_info型的常量中是否有不符合UTF-8编码的数据。
      - Class文件中各个部分及文件本身是否有被删除的或附加的其他信息。
      - ……
    - 该验证阶段的主要目的是保证输入的字节流能正确地解析并存储于方法区之内，格式上符合描述一个Java类型信息的要求。
    - 这阶段的验证是基于二进制字节流进行的，只有通过了这个阶段的验证之后，这段字节流才被允许进入Java虚拟机内存的方法区中进行存储
      - 所以后面的三个验证阶段全部是基于方法区的存储结构上进行的，不会再直接读取、操作字节流了。
  - **元数据验证**
    - 第二阶段是对字节码描述的信息进行语义分析，以保证其描述的信息符合《Java语言规范》的要求
    - 这个阶段可能包括的验证点如下：
      - 这个类是否有父类（除了java.lang.Object之外，所有的类都应当有父类）。
      - 这个类的父类是否继承了不允许被继承的类（被final修饰的类）。
      - 如果这个类不是抽象类，是否实现了其父类或接口之中要求实现的所有方法。
      - 类中的字段、方法是否与父类产生矛盾（例如覆盖了父类的final字段，或者出现不符合规则的方法重载，例如方法参数都一致，但返回值类型却不同等）。
      - ……
    - 第二阶段的主要目的是对类的元数据信息进行语义校验，保证不存在与《Java语言规范》定义相悖的元数据信息。
  - **字节码验证**
    - 第三阶段是整个验证过程中最复杂的一个阶段，主要目的是通过数据流分析和控制流分析，确定程序语义是合法的、符合逻辑的。
    - 这阶段就要对类的方法体（Class文件中的Code属性）进行校验分析，保证被校验类的方法在运行时不会做出危害虚拟机安全的行为，例如：
      - 保证任意时刻操作数栈的数据类型与指令代码序列都能配合工作，例如不会出现类似于“在操作栈放置了一个int类型的数据，使用时却按long类型来加载入本地变量表中”这样的情况。
      - 保证任何跳转指令都不会跳转到方法体以外的字节码指令上。
      - 保证方法体中的类型转换总是有效的，例如可以把一个子类对象赋值给父类数据类型，这是安全的，但是把父类对象赋值给子类数据类型，甚至把对象赋值给与它毫无继承关系、完全不相干的一个数据类型，则是危险和不合法的。
      - ……
    - 由于数据流分析和控制流分析的高度复杂性，Java虚拟机的设计团队为了避免过多的执行时间消耗在字节码验证阶段中，在JDK 6之后的Javac编译器和Java虚拟机里进行了一项联合优化，把尽可能多的校验辅助措施挪到Javac编译器里进行。
      - 具体做法是给方法体Code属性的属性表中新增加了一项名为“StackMapTable”的新属性，这项属性描述了方法体所有的基本块（Basic Block，指按照控制流拆分的代码块）开始时本地变量表和操作栈应有的状态，在字节码验证期间，Java虚拟机就不需要根据程序推导这些状态的合法性，只需要检查StackMapTable属性中的记录是否合法即可。
      - 这样就将字节码验证的类型推导转变为类型检查，从而节省了大量校验时间。
      - 而到了 JDK 7 之后，尽管虚拟机中仍然保留着类型推导验证器的代码，但是对于主版本号大于50（对应JDK6）的Class文件，使用类型检查来完成数据流分析校验则是唯一的选择，不允许再退回到原来的类型推导的校验方式。
  - **符号引用验证**
    - 最后一个阶段的校验行为发生在虚拟机将符号引用转化为直接引用的时候，**这个转化动作将在连接的第三阶段——解析阶段中发生**。
    - 符号引用验证可以看作是对类自身以外（常量池中的各种符号引用）的各类信息进行匹配性校验，通俗来说就是，该类是否缺少或者被禁止访问它依赖的某些外部类、方法、字段等资源。
    - 本阶段通常需要校验下列内容：
      - 符号引用中通过字符串描述的全限定名是否能找到对应的类。
      - 在指定类中是否存在符合方法的字段描述符及简单名称所描述的方法和字段。
      - 符号引用中的类、字段、方法的可访问性（private、protected、public、`<package>`）是否可被当前类访问。
    - 符号引用验证的主要目的是确保解析行为能正常执行
      - 如果无法通过符号引用验证，Java虚拟机将会抛出一个java.lang.IncompatibleClassChangeError的子类异常，典型的如：java.lang.IllegalAccessError、java.lang.NoSuchFieldError、java.lang.NoSuchMethodError等。
- 验证阶段对于虚拟机的类加载机制来说，是一个非常重要的、但却不是必须要执行的阶段，因为验证阶段只有通过或者不通过的差别，只要通过了验证，其后就对程序运行期没有任何影响了。
  - 如果程序运行的全部代码（包括自己编写的、第三方包中的、从外部加载的、动态生成的等所有代码）都已经被反复使用和验证过，在生产环境的实施阶段就可以考虑**使用 -Xverify:none 参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间**。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 7.3.2 验证

## 类加载过程 3：准备

- 准备阶段是**正式为类中定义的变量（即静态变量，被static修饰的变量）分配内存并设置类变量初始值的阶段**
  - 从概念上讲，这些变量所使用的内存都应当在方法区中进行分配，但必须注意到方法区本身是一个逻辑上的区域
    - 在JDK 7及之前，HotSpot使用永久代来实现方法区时，实现是完全符合这种逻辑概念的；
    - 而在JDK 8及之后，类变量则会随着Class对象一起存放在Java堆中，这时候“类变量在方法区”就完全是一种对逻辑概念的表述了
- 关于准备阶段，还有两个容易产生混淆的概念笔者需要着重强调
  - 首先是这时候进行内存分配的仅包括类变量，而**不包括实例变量**，实例变量将会在对象实例化时随着对象一起分配在Java堆中
  - 其次是这里所说的初始值**“通常情况”下是数据类型的零值**
    - “特殊情况”：如果类字段的字段属性表中存在 ConstantValue 属性，那在准备阶段变量值就会被初始化为 **ConstantValue 属性所指定的初始值**
      - 目前 Oracle 公司实现的 Javac 编译器的选择是，如果**同时使用 final 和 static 来修饰一个变量**（按照习惯，这里称“常量”更贴切），并且这个变量的数据类型是**基本类型或者 java.lang.String** 的话，就将会生成 ConstantValue 属性来进行初始化；
      - 如果这个变量没有被 final 修饰，或者并非基本类型及字符串，则将会选择在 `<clinit>()` 方法中进行初始化。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 7.3.3 准备

## 类加载过程 4：解析

- 解析阶段是**Java虚拟机将常量池内的符号引用替换为直接引用的过程**
  - 符号引用在第6章讲解 Class 文件格式的时候已经出现过多次，在Class文件中它以CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info等类型的常量出现
  - 那解析阶段中所说的直接引用与符号引用又有什么关联呢？
    - **符号引用（Symbolic References）**：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。
    - **直接引用（Direct References）**：直接引用是可以直接指向目标的指针、相对偏移量或者是一个能间接定位到目标的句柄。
- 《Java虚拟机规范》之中并未规定解析阶段发生的具体时间，只要求了在执行ane-warray、checkcast、getfield、getstatic、instanceof、invokedynamic、invokeinterface、invoke-special、invokestatic、invokevirtual、ldc、ldc_w、ldc2_w、multianewarray、new、putfield和putstatic这17个用于操作符号引用的字节码指令之前，先对它们所使用的符号引用进行解析。
- 对同一个符号引用进行多次解析请求是很常见的事情，**除 invokedynamic 指令以外**，虚拟机实现可以**对第一次解析的结果进行缓存**，譬如在运行时直接引用常量池中的记录，并把常量标识为已解析状态，从而避免解析动作重复进行。
  - 不过对于invokedynamic指令，上面的规则就不成立了。当碰到某个前面已经由invokedynamic指令触发过解析的符号引用时，并不意味着这个解析结果对于其他invokedynamic指令也同样生效。
  - 因为 invokedynamic 指令的目的本来就是用于动态语言支持，它对应的引用称为“动态调用点限定符（Dynamically-Computed Call Site Specifier）”，这里“动态”的含义是指必须等到程序实际运行到这条指令时，解析动作才能进行。
  - 相对地，其余可触发解析的指令都是“静态”的，可以在刚刚完成加载阶段，还没有开始执行代码时就提前进行解析。
- 解析动作主要针对**类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符这7类符号引用**进行
  - 分别对应于常量池的CONSTANT_Class_info、CON-STANT_Fieldref_info、CONSTANT_Methodref_info、CONSTANT_InterfaceMethodref_info、CONSTANT_MethodType_info、CONSTANT_MethodHandle_info、CONSTANT_Dyna-mic_info和CONSTANT_InvokeDynamic_info 8种常量类型
  - 1.类或接口的解析
  - 2.字段解析
  - 3.方法解析
  - 4.接口方法解析

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 7.3.4 解析

## 类加载过程 5：初始化

- 类的初始化阶段是类加载过程的最后一个步骤，之前介绍的几个类加载的动作里，除了在加载阶段用户应用程序可以通过自定义类加载器的方式局部参与外，其余动作都完全由Java虚拟机来主导控制。
  - 直到初始化阶段，Java虚拟机才真正开始执行类中编写的Java程序代码，将主导权移交给应用程序。
- 进行准备阶段时，变量已经赋过一次系统要求的初始零值，而在初始化阶段，则会根据程序员通过程序编码制定的主观计划去初始化类变量和其他资源。
  - 我们也可以从另外一种更直接的形式来表达：**初始化阶段就是执行类构造器 `<clinit>()` 方法的过程**。
  - `<clinit>()` 并不是程序员在 Java 代码中直接编写的方法，它是 Javac 编译器的自动生成物
- `<clinit>()`
  - `<clinit>()` 方法是由编译器自动收集类中的**所有类变量的赋值动作和静态语句块（static{}块）中的语句**合并产生的，**编译器收集的顺序是由语句在源文件中出现的顺序决定的**，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问
  - `<clinit>()` 方法与类的构造函数（即在虚拟机视角中的实例构造器 `<init>()` 方法）不同，它不需要显式地调用父类构造器，**Java虚拟机会保证在子类的 `<clinit>()` 方法执行前，父类的 `<clinit>()` 方法已经执行完毕**。因此在Java虚拟机中第一个被执行的 `<clinit>()` 方法的类型肯定是 java.lang.Object。
  - 由于父类的 `<clinit>()` 方法先执行，也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作
  - `<clinit>()` 方法对于类或接口来说并不是必需的，如果一个类中没有静态语句块，也没有对变量的赋值操作，那么编译器可以不为这个类生成 `<clinit>()` 方法。
  - 接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此接口与类一样都会生成 `<clinit>()` 方法。但**接口与类不同的是，执行接口的 `<clinit>()` 方法不需要先执行父接口的 `<clinit>()` 方法，因为只有当父接口中定义的变量被使用时，父接口才会被初始化**。此外，接口的实现类在初始化时也一样不会执行接口的  `<clinit>()` 方法。
  - Java 虚拟机必须保证一个类的 `<clinit>()` 方法在多线程环境中被正确地加锁同步，如果多个线程同时去初始化一个类，那么只会有其中一个线程去执行这个类的 `<clinit>()` 方法，其他线程都需要阻塞等待，直到活动线程执行完毕 `<clinit>()` 方法。如果在一个类的 `<clinit>()` 方法中有耗时很长的操作，那就可能造成多个进程阻塞，在实际应用中这种阻塞往往是很隐蔽的。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 7.3.5 初始化

## 即时编译器 / Java 为什么既是编译型也是解释型的

- 在Java技术下谈“编译期”而没有具体上下文语境的话，其实是一句很含糊的表述
  - 因为它可能是指一个前端编译器（叫“编译器的前端”更准确一些）把 `*.java` 文件转变成 `*.class` 文件的过程；
  - 也可能是指Java虚拟机的即时编译器（常称JIT编译器，Just In Time Compiler）运行期把字节码转变成本地机器码的过程；
  - 还可能是指使用静态的提前编译器（常称AOT编译器，Ahead Of Time Compiler）直接把程序编译成与目标机器指令集相关的二进制代码的过程。
- 下面笔者列举了这3类编译过程里一些比较有代表性的编译器产品：
  - 前端编译器：JDK 的 Javac、Eclipse JDT中的增量式编译器（ECJ）。
  - **即时编译器：HotSpot虚拟机的C1、C2编译器，Graal编译器。**
  - 提前编译器：JDK的Jaotc、GNU Compiler for the Java（GCJ）、Excelsior JET。
- 即时编译器
  - 目前主流的两款商用Java虚拟机（HotSpot、OpenJ9）里，Java程序最初都是通过**解释器（Interpreter）**进行解释执行的，当虚拟机发现某个方法或代码块的运行特别频繁，就会把这些代码认定为**“热点代码”（Hot Spot Code）**，为了提高热点代码的执行效率，在运行时，虚拟机将会把这些代码编译成本地机器码，并以各种手段尽可能地进行代码优化，运行时完成这个任务的后端编译器被称为**即时编译器**。
- 解释器与编译器
  - 尽管并不是所有的Java虚拟机都采用解释器与编译器并存的运行架构，但目前主流的商用Java虚拟机，譬如HotSpot、OpenJ9等，内部都同时包含解释器与编译器
  - 解释器与编译器两者各有优势：
    - 当程序需要迅速启动和执行的时候，解释器可以首先发挥作用，省去编译的时间，立即运行。
    - 当程序启动后，随着时间的推移，编译器逐渐发挥作用，把越来越多的代码编译成本地代码，这样可以减少解释器的中间损耗，获得更高的执行效率。
  - 当程序运行环境中内存资源限制较大，可以使用解释执行节约内存（如部分嵌入式系统中和大部分的JavaCard应用中就只有解释器的存在），反之可以使用编译执行来提升效率。
  - 同时，解释器还可以作为编译器激进优化时后备的“逃生门”（如果情况允许，HotSpot虚拟机中也会采用不进行激进优化的客户端编译器充当“逃生门”的角色），让编译器根据概率选择一些不能保证所有情况都正确，但大多数时候都能提升运行速度的优化手段，当激进优化的假设不成立，如加载了新类以后，类型继承结构出现变化、出现“罕见陷阱”（Uncommon Trap）时可以通过逆优化（Deoptimization）退回到解释状态继续执行，因此在整个Java虚拟机执行架构里，解释器与编译器经常是相辅相成地配合工作
  - **HotSpot虚拟机中内置了两个（或三个）即时编译器**
    - 其中有两个编译器存在已久，分别被称为“**客户端编译器”（Client Compiler）**和“**服务端编译器”（Server Compiler）**，或者**简称为C1编译器和C2编译器**（部分资料和JDK源码中C2也叫Opto编译器）
    - 第三个是在 JDK 10 时才出现的、长期目标是代替C2的**Graal编译器**。Graal编译器目前还处于实验状态，本章将安排出专门的小节对它讲解与实战，在本节里，我们将重点关注传统的C1、C2编译器的工作过程。
  - 在分层编译（Tiered Compilation）的工作模式出现以前，HotSpot 虚拟机通常是采用解释器与其中一个编译器直接搭配的方式工作，程序使用哪个编译器，只取决于虚拟机运行的模式，HotSpot 虚拟机会根据自身版本与宿主机器的硬件性能自动选择运行模式，用户也可以使用“-client”或“-server”参数去强制指定虚拟机运行在客户端模式还是服务端模式。
  - 无论采用的编译器是客户端编译器还是服务端编译器，解释器与编译器搭配使用的方式在虚拟机中被称为**“混合模式”（Mixed Mode）**
    - 用户也可以使用参数“-Xint”强制虚拟机运行于**“解释模式”（Interpreted Mode）**，这时候编译器完全不介入工作，全部代码都使用解释方式执行。
    - 另外，也可以使用参数“-Xcomp”强制虚拟机运行于**“编译模式”（Compiled Mode）**，这时候将优先采用编译方式执行程序，但是解释器仍然要在编译无法进行的情况下介入执行过程。
    - 可以通过虚拟机的“-version”命令的输出结果显示出这三种模式
  - 由于即时编译器编译本地代码需要占用程序运行时间，通常要编译出优化程度越高的代码，所花费的时间便会越长；而且想要编译出优化程度更高的代码，解释器可能还要替编译器收集性能监控信息，这对解释执行阶段的速度也有所影响。
    - 为了在程序启动响应速度与运行效率之间达到最佳平衡，HotSpot 虚拟机在编译子系统中加入了**分层编译**的功能，分层编译的概念其实很早就已经提出，但直到 JDK 6 时期才被初步实现，后来一直处于改进阶段，最终在 JDK 7 的服务端模式虚拟机中作为默认编译策略被开启。
    - **分层编译根据编译器编译、优化的规模与耗时，划分出不同的编译层次**，其中包括：
      - 第0层。程序纯解释执行，并且解释器不开启性能监控功能（Profiling）。
      - 第1层。使用客户端编译器将字节码编译为本地代码来运行，进行简单可靠的稳定优化，不开启性能监控功能。
      - 第2层。仍然使用客户端编译器执行，仅开启方法及回边次数统计等有限的性能监控功能。
      - 第3层。仍然使用客户端编译器执行，开启全部性能监控，除了第2层的统计信息外，还会收集如分支跳转、虚方法调用版本等全部的统计信息。
      - 第4层。使用服务端编译器将字节码编译为本地代码，相比起客户端编译器，服务端编译器会启用更多编译耗时更长的优化，还会根据性能监控信息进行一些不可靠的激进优化。
    - 以上层次并不是固定不变的，根据不同的运行参数和版本，虚拟机可以调整分层的数量。
    - **实施分层编译后，解释器、客户端编译器和服务端编译器就会同时工作，热点代码都可能会被多次编译，用客户端编译器获取更高的编译速度，用服务端编译器来获取更好的编译质量**，在解释执行的时候也无须额外承担收集性能监控信息的任务，而在服务端编译器采用高复杂度的优化算法时，客户端编译器可先采用简单优化来为它争取更多的编译时间。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 10.1 概述、11.2 即时编译器

## 栈上分配、逃逸分析

- Java中的对象和数组都是在堆上分配的吗？
  - 你可以这样回答：Java中的对象不一定是在堆上分配的，因为JVM通过**逃逸分析**，能够分析出一个新对象的使用范围，并以此确定是否要将这个对象分配到堆上。
  - 如果JVM发现某些对象没有逃逸出方法，就很有可能被优化成在**栈上分配**。
- 先以官方的形式来说下什么是逃逸分析。
  - 逃逸分析就是：一种确定指针动态范围的静态分析，它可以分析在程序的哪些地方可以访问到指针。
  - 在JVM的即时编译语境下，逃逸分析将判断新建的对象是否逃逸。即时编译判断对象是否逃逸的依据：
    - 一种是对象是否被存入堆中（静态字段或者堆中对象的实例字段）
    - 另一种就是对象是否被传入未知代码。
- 逃逸分析示例：
  - 一种典型的对象逃逸就是：**对象被复制给成员变量或者静态变量**，可能被外部使用，此时变量就发生了逃逸。
  - 另一种典型的场景就是：**对象通过return语句返回**。如果对象通过return语句返回了，此时的程序并不能确定这个对象后续会不会被使用，外部的线程可以访问到这个变量，此时对象也发生了逃逸。
- 逃逸分析优点
  - 逃逸分析的优点总体上来说可以分为三个：
    - **对象可能分配在栈上**
      - JVM通过逃逸分析，分析出新对象的使用范围，就可能将对象在栈上进行分配。
      - 栈分配可以快速地在栈帧上创建和销毁对象，不用再将对象分配到堆空间，可以有效地减少 JVM 垃圾回收的压力。
    - **分离对象或标量替换**
      - 当JVM通过逃逸分析，确定要将对象分配到栈上时，即时编译可以将对象打散，将对象替换为一个个很小的局部变量，我们将这个打散的过程叫做**标量替换**。
      - 将对象替换为一个个局部变量后，就可以非常方便的在栈上进行分配了。
      - 标量替换在 JDK1.8 中也是默认开启的(-XX:+EliminateAllocations)
    - **消除同步锁**
      - 如果JVM通过逃逸分析，发现一个对象只能从一个线程被访问到，则访问这个对象时，可以不加同步锁。如果程序中使用了synchronized锁，则JVM会将synchronized锁消除
      - 这里，需要注意的是：这种情况针对的是synchronized锁，而对于Lock锁，则JVM并不能消除。
      - 要开启同步消除，需要加上 -XX:+EliminateLocks 参数。(在 JDK1.8 中是默认开启的)
        - 因为这个参数依赖逃逸分析，所以同时要打开 -XX:+DoEscapeAnalysis 选项（**JDK 1.7之后默认开启**）。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景十七：Java 中的对象和数组都是在堆上分配的吗？

## 缓存一致性 / 引出内存模型

- 由于计算机的存储设备与处理器的运算速度有着几个数量级的差距，所以现代计算机系统都不得不加入一层或多层读写速度尽可能接近处理器运算速度的高速缓存（Cache）来作为内存与处理器之间的缓冲：将运算需要使用的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步回内存之中，这样处理器就无须等待缓慢的内存读写了。
- 基于高速缓存的存储交互很好地解决了处理器与内存速度之间的矛盾，但是也为计算机系统带来更高的复杂度，它引入了一个新的问题：**缓存一致性（Cache Coherence）**。
  - 在多路处理器系统中，每个处理器都有自己的高速缓存，而它们又共享同一主内存（Main Memory），这种系统称为共享内存多核系统（Shared Memory Multiprocessors System）。**当多个处理器的运算任务都涉及同一块主内存区域时，将可能导致各自的缓存数据不一致。**
  - 如果真的发生这种情况，那同步回到主内存时该以谁的缓存数据为准呢？**为了解决一致性的问题，需要各个处理器访问缓存时都遵循一些协议，在读写时要根据协议来进行操作**，这类协议有 MSI、MESI（Illinois Protocol）、MOSI、Synapse、Firefly及Dragon Protocol等。
    - **MESI 详解**
      - MESI 指的是缓存行的四种状态（Modified，Exclusive，Shared， Invalid），用 2 个 bit 表示。
- 从本章开始，我们将会频繁见到“**内存模型**”一词，它可以理解为在特定的操作协议下，对特定的内存或高速缓存进行读写访问的过程抽象。
  - 不同架构的物理机器可以拥有不一样的内存模型，而Java虚拟机也有自己的内存模型，并且与这里介绍的内存访问操作及硬件的缓存访问操作具有高度的可类比性。
- 除了增加高速缓存之外，为了使处理器内部的运算单元能尽量被充分利用，处理器可能会对输入代码进行**乱序执行（Out-Of-Order Execution）优化**，处理器会在计算之后将乱序执行的结果重组，保证该结果与顺序执行的结果是一致的，但并不保证程序中各个语句计算的先后顺序与输入代码中的顺序一致，因此如果存在一个计算任务依赖另外一个计算任务的中间结果，那么其顺序性并不能靠代码的先后顺序来保证。
  - 与处理器的乱序执行优化类似，Java虚拟机的即时编译器中也有**指令重排序（Instruction Reorder）优化**。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 12.2 硬件的效率与一致性

## Java 内存模型

- 《Java虚拟机规范》中曾试图定义一种**“Java内存模型” （Java Memory Model，JMM）**来屏蔽各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。

  - 在此之前，主流程序语言（如C和C++等）直接使用物理硬件和操作系统的内存模型。因此，由于不同平台上内存模型的差异，有可能导致程序在一套平台上并发完全正常，而在另外一套平台上并发访问却经常出错，所以在某些场景下必须针对不同的平台来编写程序

- 定义Java内存模型并非一件容易的事情

  - 这个模型必须定义得足够严谨，才能让Java的并发内存访问操作不会产生歧义；
  - 但是也必须定义得足够宽松，使得虚拟机的实现能有足够的自由空间去利用硬件的各种特性（寄存器、高速缓存和指令集中某些特有的指令）来获取更好的执行速度。

- 经过长时间的验证和修补，直至 **JDK 5**（实现了JSR-133 ）发布后，Java内存模型才终于成熟、完善起来了。

- 主内存与工作内存

  - **Java 内存模型的主要目的是定义程序中各种变量的访问规则，即关注在虚拟机中把变量值存储到内存和从内存中取出变量值这样的底层细节**。
    - 此处的变量（Variables）与Java编程中所说的变量有所区别，它包括了实例字段、静态字段和构成数组对象的元素，但是不包括局部变量与方法参数，因为后者是线程私有的，不会被共享，自然就不会存在竞争问题。
    - 为了获得更好的执行效能，Java内存模型并没有限制执行引擎使用处理器的特定寄存器或缓存来和主内存进行交互，也没有限制即时编译器是否要进行调整代码执行顺序这类优化措施。
  - Java内存模型规定了所有的变量都存储在**主内存（Main Memory）**中（此处的主内存与介绍物理硬件时提到的主内存名字一样，两者也可以类比，但物理上它仅是虚拟机内存的一部分）。
  - 每条线程还有自己的**工作内存（Working Memory**，可与前面讲的处理器高速缓存类比）
    - 线程的工作内存中保存了被该线程使用的变量的主内存副本
    - 线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的数据。
    - 不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成
  - 这里所讲的主内存、工作内存与第2章所讲的Java内存区域中的Java堆、栈、方法区等并不是同一个层次的对内存的划分，这两者基本上是没有任何关系的。
    - 如果两者一定要勉强对应起来，那么从变量、主内存、工作内存的定义来看，**主内存主要对应于Java堆中的对象实例数据部分**，而**工作内存则对应于虚拟机栈中的部分区域**。
    - 从更基础的层次上说，**主内存直接对应于物理硬件的内存**，而为了获取更好的运行速度，虚拟机（或者是硬件、操作系统本身的优化措施）可能会**让工作内存优先存储于寄存器和高速缓存中**，因为程序运行时主要访问的是工作内存。

- 内存间交互操作

  - 关于主内存与工作内存之间具体的交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存这一类的实现细节，Java内存模型中定义了以下8种操作来完成。
  - Java虚拟机实现时必须保证下面提及的每一种操作都是原子的、不可再分的（对于double和long类型的变量来说，load、store、read和write操作在某些平台上允许有例外，这个问题在12.3.4节会专门讨论）
    - **lock（锁定）**：作用于主内存的变量，它把一个变量标识为一条线程独占的状态。
    - **unlock（解锁）**：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。
    - **read（读取）**：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用。
    - **load（载入）**：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。
    - **use（使用）**：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。
    - **assign（赋值）**：作用于工作内存的变量，它把一个从执行引擎接收的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。
    - **store（存储）**：作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的write操作使用。
    - **write（写入）**：作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。
  - 如果要把一个变量从主内存拷贝到工作内存，那就要按**顺序执行 read 和 load 操作**，如果要把变量从工作内存同步回主内存，就要按**顺序执行 store 和 write 操作**。
    - 注意，Java内存模型只要求上述两个操作必须按顺序执行，但不要求是连续执行。
    - 也就是说read与load之间、store与write之间是可插入其他指令的，如对主内存中的变量a、b进行访问时，一种可能出现的顺序是read a、read b、load b、load a。
  - 除此之外，Java内存模型还规定了在执行上述8种基本操作时必须满足如下规则：
    - 不允许read和load、store和write操作之一单独出现，即不允许一个变量从主内存读取了但工作内存不接受，或者工作内存发起回写了但主内存不接受的情况出现。
    - 不允许一个线程丢弃它最近的assign操作，即变量在工作内存中改变了之后必须把该变化同步回主内存。
    - 不允许一个线程无原因地（没有发生过任何assign操作）把数据从线程的工作内存同步回主内存中。·一个新的变量只能在主内存中“诞生”，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量，换句话说就是对一个变量实施use、store操作之前，必须先执行assign和load操作。
    - 一个变量在同一个时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。
    - 如果对一个变量执行lock操作，那将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作以初始化变量的值。
    - 如果一个变量事先没有被lock操作锁定，那就不允许对它执行unlock操作，也不允许去unlock一个被其他线程锁定的变量。
    - 对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store、write操作）。
  - 这8种内存访问操作以及上述规则限定，再加上稍后会介绍的专门针对volatile的一些特殊规定，就已经能准确地描述出Java程序中哪些内存访问操作在并发下才是安全的。
    - 这种定义相当严谨，但也是极为烦琐，实践起来更是无比麻烦。
    - 可能部分读者阅读到这里已经对多线程开发产生恐惧感了，后来Java设计团队大概也意识到了这个问题，将Java内存模型的操作简化为read、write、lock和unlock四种，但这只是语言描述上的等价化简，Java内存模型的基础设计并未改变，即使是这四操作种，对于普通用户来说阅读使用起来仍然并不方便。
    - 不过读者对此无须过分担忧，除了进行虚拟机开发的团队外，大概没有其他开发人员会以这种方式来思考并发问题，我们只需要理解 Java 内存模型的定义即可。
    - 12.3.6节将介绍这种定义的一个等效判断原则——**先行发生原则**，用来确定一个操作在并发环境下是否安全的。

- **对于 volatile 型变量的特殊规则**

  - 关键字volatile可以说是Java虚拟机提供的最轻量级的同步机制
  - 当一个变量被定义成volatile之后，它将具备两项特性：
    - 第一项是**保证此变量对所有线程的可见性**
      - 这里的“可见性”是指当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。
      - 而普通变量并不能做到这一点，普通变量的值在线程间传递时均需要通过主内存来完成。
      - 比如，线程A修改一个普通变量的值，然后向主内存进行回写，另外一条线程B在线程A回写完成了之后再对主内存进行读取操作，新变量值才会对线程B可见。
      - 但是Java里面的运算操作符并非原子操作，这导致volatile变量的运算在并发下一样是不安全的
        - 问题就出在自增运算“race++”之中，我们用Javap反编译这段代码后会得到代码清单12-2所示，发现只有一行代码的increase()方法在Class文件中是由4条字节码指令构成（return指令不是由race++产生的，这条指令可以不计算）
        - 实事求是地说，笔者使用字节码来分析并发问题仍然是不严谨的，因为即使编译出来只有一条字节码指令，也并不意味执行这条指令就是一个原子操作。
          - 一条字节码指令在解释执行时，解释器要运行许多行代码才能实现它的语义。
          - 如果是编译执行，一条字节码指令也可能转化成若干条本地机器码指令。
        - 此处使用-XX：+PrintAssembly参数输出反汇编来分析才会更加严谨一些，但是考虑到读者阅读的方便性，并且字节码已经能很好地说明问题，所以此处使用字节码来解释。
      - 由于volatile变量只能保证可见性，在不符合以下两条规则的运算场景中，我们仍然要通过加锁（使用synchronized、java.util.concurrent中的锁或原子类）来保证原子性：
        - 运算结果并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值。
        - 变量不需要与其他的状态变量共同参与不变约束。
    - 使用volatile变量的第二个语义是**禁止指令重排序优化**
      - 普通的变量仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致。
        - 因为在同一个线程的方法执行过程中无法感知到这点，这就是Java内存模型中描述的所谓“线程内表现为串行的语义”（Within-Thread As-If-Serial Semantics）。
      - 通过对比发现，关键变化在于有 volatile 修饰的变量，赋值后（前面 `mov %eax，0x150(%esi)` 这句便是赋值操作）多执行了一个“`lock addl $0x0，(%esp)`”操作，这个操作的作用相当于一个**内存屏障（Memory Barrier或Memory Fence**，指重排序时不能把后面的指令重排序到内存屏障之前的位置，**注意不要与第3章中介绍的垃圾收集器用于捕获变量访问的内存屏障互相混淆**），只有一个处理器访问内存时，并不需要内存屏障；但如果有两个或更多处理器访问同一块内存，且其中有一个在观测另一个，就需要内存屏障来保证一致性了。
        - 这里的关键在于 lock 前缀，查询IA32手册可知，它的作用是将本处理器的缓存写入了内存，该写入动作也会引起别的处理器或者别的内核无效化（Invalidate）其缓存，这种操作相当于对缓存中的变量做了一次前面介绍Java内存模式中所说的“store和write”操作[4]。所以通过这样一个空操作，可让前面volatile变量的修改对其他处理器立即可见。
  - 我们再回头来看看Java内存模型中对volatile变量定义的特殊规则的定义。假定T表示一个线程，V和W分别表示两个volatile型变量，那么在进行read、load、use、assign、store和write操作时需要满足如下规则：
    - 只有当线程T对变量V执行的前一个动作是load的时候，线程T才能对变量V执行use动作；并且，只有当线程T对变量V执行的后一个动作是use的时候，线程T才能对变量V执行load动作。线程T对变量V的use动作可以认为是和线程T对变量V的load、read动作相关联的，必须连续且一起出现。
      - 这条规则要求在工作内存中，每次使用V前都必须先从主内存刷新最新的值，用于保证能看见其他线程对变量V所做的修改。
    - 只有当线程T对变量V执行的前一个动作是assign的时候，线程T才能对变量V执行store动作；并且，只有当线程T对变量V执行的后一个动作是store的时候，线程T才能对变量V执行assign动作。线程T对变量V的assign动作可以认为是和线程T对变量V的store、write动作相关联的，必须连续且一起出现。
      - 这条规则要求在工作内存中，每次修改V后都必须立刻同步回主内存中，用于保证其他线程可以看到自己对变量V所做的修改。
    - 假定动作A是线程T对变量V实施的use或assign动作，假定动作F是和动作A相关联的load或store动作，假定动作P是和动作F相应的对变量V的read或write动作；与此类似，假定动作B是线程T对变量W实施的use或assign动作，假定动作G是和动作B相关联的load或store动作，假定动作Q是和动作G相应的对变量W的read或write动作。如果A先于B，那么P先于Q。
      - 这条规则要求volatile修饰的变量不会被指令重排序优化，从而保证代码的执行顺序与程序的顺序相同。

- ##### 针对 long 和 double 型变量的特殊规则

  - Java内存模型要求lock、unlock、read、load、assign、use、store、write这八种操作都具有原子性，但是对于64位的数据类型（long和double），在模型中特别定义了一条宽松的规定：**允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行**，即允许虚拟机实现自行选择是否要保证64位数据类型的load、store、read和write这四个操作的原子性，这就是所谓的“**long和double的非原子性协定”（Non-Atomic Treatment of double and long Variables）**。
  - **在目前主流平台下商用的64位Java虚拟机中并不会出现非原子性访问行为**，但是对于32位的Java虚拟机，譬如比较常用的32位x86平台下的HotSpot虚拟机，对long类型的数据确实存在非原子性访问的风险。
  - 笔者的看法是，在实际开发中，除非该数据有明确可知的线程竞争，否则我们在编写代码时一般不需要因为这个原因刻意把用到的long和double变量专门声明为volatile。

- **原子性、可见性与有序性**

  - Java内存模型是围绕着在并发过程中如何处理原子性、可见性和有序性这三个特征来建立的，我们逐个来看一下哪些操作实现了这三个特性。
    - 1.**原子性（Atomicity）**
      - 由Java内存模型来直接保证的原子性变量操作包括read、load、assign、use、store和write这六个，我们大致可以认为，**基本数据类型的访问、读写都是具备原子性的**（例外就是long和double的非原子性协定，读者只要知道这件事情就可以了，无须太过在意这些几乎不会发生的例外情况）。
      - 如果应用场景需要一个**更大范围的原子性保证（经常会遇到），Java内存模型还提供了lock和unlock操作来满足这种需求**，尽管虚拟机未把lock和unlock操作直接开放给用户使用，但是却提供了**更高层次的字节码指令monitorenter和monitorexit**来隐式地使用这两个操作。
        - 这两个字节码指令反映到Java代码中就是同步块——synchronized关键字，因此在synchronized块之间的操作也具备原子性。
    - 2.**可见性（Visibility）**
      - 可见性就是指当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改。上文在讲解volatile变量的时候我们已详细讨论过这一点。
      - **Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式来实现可见性的，无论是普通变量还是volatile变量都是如此**。
        - 普通变量与volatile变量的区别是，**volatile的特殊规则保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新**。因此我们可以说volatile保证了多线程操作时变量的可见性，而普通变量则不能保证这一点。
      - 除了volatile之外，Java还有两个关键字能实现可见性，它们是 **synchronized 和 final**。
        - **同步块的可见性是由“对一个变量执行 unlock 操作之前，必须先把此变量同步回主内存中（执行 store、write 操作）”这条规则获得的**。
        - 而final关键字的可见性是指：**被 final 修饰的字段在构造器中一旦被初始化完成，并且构造器没有把“this”的引用传递出去（this引用逃逸是一件很危险的事情，其他线程有可能通过这个引用访问到“初始化了一半”的对象），那么在其他线程中就能看见final字段的值**。
    - 3.**有序性（Ordering）**
      - Java内存模型的有序性在前面讲解volatile时也比较详细地讨论过了，Java程序中天然的有序性可以总结为一句话：**如果在本线程内观察，所有的操作都是有序的**；**如果在一个线程中观察另一个线程，所有的操作都是无序的**。
        - 前半句是指“线程内似表现为串行的语义”（Within-Thread As-If-Serial Semantics），后半句是指“指令重排序”现象和“工作内存与主内存同步延迟”现象。
      - Java语言提供了 **volatile 和 synchronized** 两个关键字来保证线程之间操作的有序性
        - **volatile关键字本身就包含了禁止指令重排序的语义**
        - **而synchronized则是由“一个变量在同一个时刻只允许一条线程对其进行lock操作”这条规则获得的**，这个规则决定了持有同一个锁的两个同步块只能串行地进入。

- ##### 先行发生原则

  - 如果Java内存模型中所有的有序性都仅靠 volatile 和 synchronized 来完成，那么有很多操作都将会变得非常啰嗦，但是我们在编写Java并发代码的时候并没有察觉到这一点，这是因为Java语言中有一个**“先行发生”（Happens-Before）**的原则。
    - 这个原则非常重要，它是判断数据是否存在竞争，线程是否安全的非常有用的手段。
    - 依赖这个原则，我们可以通过几条简单规则一揽子解决并发环境下两个操作之间是否可能存在冲突的所有问题，而不需要陷入Java内存模型苦涩难懂的定义之中。
  - 现在就来看看“先行发生”原则指的是什么。先行发生是Java内存模型中定义的两项操作之间的偏序关系，比如说操作A先行发生于操作B，其实就是说在发生操作B之前，操作A产生的影响能被操作B观察到，“影响”包括修改了内存中共享变量的值、发送了消息、调用了方法等。
  - 下面是Java内存模型下一些“天然的”先行发生关系，这些先行发生关系无须任何同步器协助就已经存在，可以在编码中直接使用。如果两个操作之间的关系不在此列，并且无法从下列规则推导出来，则它们就没有顺序性保障，虚拟机可以对它们随意地进行重排序。
    - **程序次序规则（Program Order Rule）**：在一个线程内，按照控制流顺序，书写在前面的操作先行发生于书写在后面的操作。注意，这里说的是控制流顺序而不是程序代码顺序，因为要考虑分支、循环等结构。
    - **管程锁定规则（Monitor Lock Rule）**：一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。这里必须强调的是“同一个锁”，而“后面”是指时间上的先后。
    - **volatile变量规则（Volatile Variable Rule）**：对一个volatile变量的写操作先行发生于后面对这个变量的读操作，这里的“后面”同样是指时间上的先后。
    - **线程启动规则（Thread Start Rule）**：Thread对象的start()方法先行发生于此线程的每一个动作。
    - **线程终止规则（Thread Termination Rule）**：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread::join()方法是否结束、Thread::isAlive()的返回值等手段检测线程是否已经终止执行。
    - **线程中断规则（Thread Interruption Rule）**：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread::interrupted()方法检测到是否有中断发生。
    - **对象终结规则（Finalizer Rule）**：一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize()方法的开始。
    - **传递性（Transitivity）**：如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论。
  - Java语言无须任何同步手段保障就能成立的先行发生规则有且只有上面这些

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 12.3 Java 内存模型

## 指令重排序的种类

1. **编译器优化重排**：编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。
2. **指令级的并行重排**：现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。
3. **内存系统的重排**：由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。

注意：

- 单线程环境里面确保最终执行结果和代码顺序的结果一致
- 处理器在进行重排序时，必须要考虑指令之间的`数据依赖性`
- 多线程环境中线程交替执行，由于编译器优化重排的存在，两个线程中使用的变量能否保证一致性是无法确定的，结果无法预测。

参考文档：

1. [指令重排序 - CSDN](https://blog.csdn.net/qianhuan_/article/details/108791208)

## JMM 的内存屏障

- 是什么

  - （5：30）

    **内存屏障（也称内存栅栏，屏障指令等，是一类同步屏障指令，是 CPU 或编译器在对内存随机访问的操作中的一个同步点，使得此点之前的所有读写操作都执行后才可以开始执行此点之后的操作）**，避免代码重排序。

    内存屏障其实就是一种 JVM 指令，Java 内存模型的重排规则会要求 Java 编译器在生成 JVM 指令时插入特定的**内存屏障指令**，通过这些内存屏障指令，volatile 实现了 Java 内存模型中的可见性和有序性（禁重排），但 volatile 无法保证原子性。

- 内存屏障分类

  - 一句话

    - 上一章讲解过 happens-before 先行发生原则，类似接口规范，落地？
    - 落地靠什么？
      - 你凭什么可以保证？你管用吗？

  - 粗分 2 种

    - **读屏障（Load Barrier）**
      - 在读指令之前插入读屏障，让工作内存或 CPU 高速缓存当中的缓存数据失效，重新回到主内存中获取最新数据
    - **写屏障（Store Barrier）**
      - 在写指令之后插入写屏障，强制把写缓冲区的数据刷回到主内存中

  - 细分 4 种

    - C++ 源码分析

      - IDEA 工具里面找 Unsafe.class
        - Unsafe.java
          - Unsafe.cpp（13：43）
            - OrderAccess.hpp（14：29）
              - orderAccess_linux_x86.inline.hpp（14：54）

    - orderAccess_linux_x86.inline.hpp（15：26）

    - 四大屏障分别是什么意思

      - （13：35）

        | 屏障类型   | 指令示例                   | 说明                                                         |
        | ---------- | -------------------------- | ------------------------------------------------------------ |
        | LoadLoad   | Load1; LoadLoad; Load2     | 保证 load1 的读取操作在 load2 及后续读取操作之前执行         |
        | StoreStore | Store1; StoreStore; Store2 | 在 store2 及其后的写操作执行前，保证 store1 的写操作已刷新到主内存 |
        | LoadStore  | Load1; LoadStore; Store2   | 在 store2 及其后的写操作执行前，保证 load1 的读操作已读取结束 |
        | StoreLoad  | Store1; StoreLoad; Load2   | 保证 store1 的写操作已刷新到主内存之后，load2 及其后的读操作才能执行 |

- 如下内容困难，可能会导致学生懵逼，课堂要讲解细致。分 2 次讲解 + 复习

  - 什么叫保证有序性？

    - 禁重排
      - 通过内存屏障禁重排
      - 上述说明（1：48）

  - happens-before 之 volatile 变量规则

    - （2：55）

      | 第一个操作  | 第二个操作：普通读写 | 第二个操作：volatile 读 | 第二个操作：volatile 写 |
      | ----------- | -------------------- | ----------------------- | ----------------------- |
      | 普通读写    | 可以重排             | 可以重排                | 不可以重排              |
      | volatile 读 | 不可以重排           | 不可以重排              | 不可以重排              |
      | volatile 写 | 可以重排             | 不可以重排              | 不可以重排              |

      当第一个操作为 volatile 读时，不论第二个操作是什么，都不能重排序。这个操作保证了 volatile 读之后的操作不会被重排到 volatile 读之前。

      当第二个操作为 volatile 写时，不论第一个操作是什么，都不能重排序。这个操作保证了 volatile 写之前的操作不会被重排到 volatile 写之后

      当第一个操作为 volatile 写时，第二个操作为 volatile 读时，不能重排

  - JMM 就将内存屏障插入策略分为 4 种规则

    - 读屏障
      - **在每个 volatile 读操作的后面插入一个 LoadLoad 屏障**
        - LoadLoad 屏障用来禁止处理器把上面的 volatile 读与下面的普通读重排序
      - **在每个 volatile 读操作的后面插入一个 LoadStore 屏障**
        - LoadStore 屏障用来禁止处理器把上面的 volatile 读与下面的普通写重排序
    - 对比图
    - 写屏障
      - **在每个 volatile 写操作的前面插入一个 StoreStore 屏障**
        - StoreStore 屏障可以保证在 volatile 写之前，其前面的所有普通写操作都已经刷新到主内存
      - **在每个 volatile 写操作的后面插入一个 StoreLoad 屏障**
        - StoreLoad 屏障的作用是避免 volatile 写与后面可能有的 volatile 读/写操作重排序

参考文档：

1. [《尚硅谷JUC并发编程（对标阿里P6~P7）》笔记.md]() P64 ~ 65、68

## Java 线程的实现

- Java线程的实现
  - 但从JDK 1.3起，“主流”平台上的“主流”商用Java虚拟机的线程模型普遍都被替换为基于操作系统原生线程模型来实现，即采用 **1：1 的线程模型**。
  - **以 HotSpot 为例，它的每一个 Java 线程都是直接映射到一个操作系统原生线程来实现的**，而且中间没有额外的间接结构，所以HotSpot自己是不会去干涉线程调度的（可以设置线程优先级给操作系统提供调度建议），全权交给底下的操作系统去处理，所以何时冻结或唤醒线程、该给线程分配多少处理器执行时间、该把线程安排给哪个处理器核心去执行等，都是由操作系统完成的，也都是由操作系统全权决定的。
- 操作系统支持怎样的线程模型，在很大程度上会影响上面的 Java 虚拟机的线程是怎样映射的，这一点在不同的平台上很难达成一致，因此《Java虚拟机规范》中才不去限定 Java 线程需要使用哪种线程模型来实现。
  - 线程模型只对线程的并发规模和操作成本产生影响，对 Java 程序的编码和运行过程来说，这些差异都是完全透明的。
- Java 线程调度
  - 线程调度是指系统为线程分配处理器使用权的过程，调度主要方式有两种，分别是协同式（Cooperative Threads-Scheduling）线程调度和抢占式（Preemptive Threads-Scheduling）线程调度。
    - **Java使用的线程调度方式就是抢占式调度**。
  - 不过，**线程优先级并不是一项稳定的调节手段**，很显然因为主流虚拟机上的Java线程是被映射到系统的原生线程上来实现的，所以**线程调度最终还是由操作系统说了算**。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 12.4.1 线程的实现、12.4.2 Java 线程调度

# 多线程

## 进程通信

- 管道( pipe )：
  - 管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。
- 有名管道 (namedpipe) ：
  - 有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。
- 信号量(semophore ) ：
  - 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。
- 消息队列( messagequeue ) ：
  - 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。
- 信号 (sinal ) 
  - 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。
- 共享内存(shared memory ) ：
  - 共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。
- 套接字(socket ) ：
  - 套接口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同设备及其间的进程通信。

参考文档：

1. [面试官：说说进程间通信和线程间通信的几种方式及区别 - 知乎](https://zhuanlan.zhihu.com/p/430069448)

## 线程通信

- 锁机制：包括互斥锁、条件变量、读写锁
- 信号量机制(Semaphore)：包括无名线程信号量和命名线程信号量
- 信号机制(Signal)：类似进程间的信号处理

参考文档：

1. [面试官：说说进程间通信和线程间通信的几种方式及区别 - 知乎](https://zhuanlan.zhihu.com/p/430069448)

## 线程和进程区别

- 调度：
  - 线程作为处理器调度和分配的基本单位
  - 而进程是作为拥有资源的基本单位
- 并发性
  - 不仅进程之间可以并发执行
  - 同一个进程的多个线程之间也可以并发执行
- 拥有资源
  - 进程是拥有资源的一个独立单位，有自己独立的地址空间
  - 线程不拥有系统资源，但可以访问隶属于进程的资源，共享进程的地址空间.
- 系统开销
  - 在创建或撤消进程时，由于系统都要为之分配和回收资源，导致系统的开销明显大于创建或撤消线程时的开销。

参考文档：

1. [操作系统：进程与线程之间的区别及联系](https://zhuanlan.zhihu.com/p/505594640)

## 协程

- 内核线程的局限

  - Java目前的并发编程机制就与上述架构趋势产生了一些矛盾，1：1的内核线程模型是如今Java虚拟机线程实现的主流选择，但是这种映射到操作系统上的线程天然的缺陷是切换、调度成本高昂，系统能容纳的线程数量也很有限。
  - 传统的Java Web服务器的线程池的容量通常在几十个到两百之间，当程序员把数以百万计的请求往线程池里面灌时，系统即使能处理得过来，但其中的切换损耗也是相当可观的。

- 协程的复苏

  - **内核线程的调度成本主要来自于用户态与核心态之间的状态转换**，而这两种状态转换的开销主要来自于响应中断、保护和恢复执行现场的成本。
  - 由于最初多数的用户线程是被设计成协同式调度（Cooperative Scheduling）的，所以它有了一个别名——**“协程”（Coroutine）**。
    - 又由于这时候的协程会完整地做调用栈的保护、恢复工作，所以今天也被称为**“有栈协程”（Stackful Coroutine）**，起这样的名字是为了便于跟后来的**“无栈协程”（Stackless Coroutine）**区分开。
    - 无栈协程不是本节的主角，不过还是可以简单提一下它的典型应用，即各种语言中的await、async、yield这类关键字。
      - 无栈协程本质上是一种有限状态机，状态保存在闭包里，自然比有栈协程恢复调用栈要轻量得多，但功能也相对更有限。
  - 协程的主要优势是轻量，无论是有栈协程还是无栈协程，都要比传统内核线程要轻量得多。
  - 协程当然也有它的局限，需要在应用层面实现的内容（调用栈、调度器这些）特别多，这个缺点就不赘述了。
    - 具体到Java语言，还会有一些别的限制，譬如HotSpot这样的虚拟机，Java调用栈跟本地调用栈是做在一起的。
    - 如果在协程中调用了本地方法，还能否正常切换协程而不影响整个线程？
    - 另外，如果协程中遇传统的线程同步措施会怎样？譬如Kotlin提供的协程实现，一旦遭遇synchronize关键字，那挂起来的仍将是整个线程。

- Java 的解决方案

  - 对于有栈协程，有一种特例实现名为纤程（Fiber），这个词最早是来自微软公司，后来微软还推出过系统层面的纤程包来方便应用做现场保存、恢复和纤程调度。
    - OpenJDK在2018年创建了Loom项目，这是Java用来应对本节开篇所列场景的官方解决方案，根据目前公开的信息，如无意外，日后该项目为Java语言引入的、与现在线程模型平行的新并发编程机制中应该也会采用“纤程”这个名字，不过这显然跟微软是没有任何关系的。(注：然而 JDK 21 实际引入时叫的是“虚拟线程”)
    - 从Oracle官方对“什么是纤程”的解释里可以看出，它就是一种典型的有栈协程
  - 在新并发模型下，一段使用纤程并发的代码会被分为两部分——执行过程（Continuation）和调度器（Scheduler）。

- Java 21 的落地实现：虚拟线程

  - 和 Kotlin 的对比

    - **Java 是有栈协程，协程上下文保存在与执行处独立的栈上**，所以异步函数可以直接像正常函数那样写，程序执行到阻塞部分时 JVM 内部自动挂起；
    - **Kotlin 是无栈协程，因为干预不了 JVM，所以协程上下文必须保存在堆上**，同时得显式声明函数是可挂起的，否则不知道什么函数支持挂起（suspend 在转译为字节码时会添加 Continuation 参数）。

  - Java 协程的调度是通过 **ForkJoinPool** 来实现的，因此也是具有 **work stealing** 机制的

  - 使用指南

    - **请大方使用同步阻塞 IO**

      - 在 “一个请求一个线程” 模型中使用平台线程的成本很高，因为平台线程与操作系统线程对应（操作系统线程是一种相对稀缺的资源），阻塞了平台线程，会让它无事可做一直处于阻塞中，这样就会造成很大的资源浪费。
      - 然而，在这个模型中使用虚拟线程就很合适，**因为虚拟线程非常廉价就算被阻塞也不会造成资源浪费**。因此在虚拟线程出来后，Java 的设计者是建议我们应该以简单的同步风格编写代码并使用阻塞 IO。

    - **避免池化虚拟线程**

      - 关于虚拟线程使用方面最难理解的一件事情就是，我们不应该池化虚拟线程。虽然虚拟线程具有与平台线程相同的行为，但虚拟线程和线程池其实是两种概念。

        - 平台线程是一种稀缺资源，因为它很宝贵。越宝贵的资源就越需要管理，管理平台线程最常见的方法是使用线程池。
        - 虚拟线程是一种非常廉价的资源，每个虚拟线程不应代表某些共享的、池化的资源，而应代表单一任务。在应用程序中，我们应该直接使用虚拟线程而不是通过线程池使用它。

      - 那么我们应该创建多少个虚拟线程嘞？答案是不必在乎虚拟线程的数量，我们有多少个并发任务就可以有多少个虚拟线程。

      - 建议使用虚拟线程执行器，代码如下：

        ```java
        try (var executor = Executors.newVirtualThreadPerTaskExecutor()) {
           Future<ResultA> f1 = executor.submit(task1);
           Future<ResultB> f2 = executor.submit(task2);
           // ... use futures
        }
        ```

        - 上面代码虽然仍使用 ExecutorService，但从 **Executors.newVirtualThreadPerTaskExecutor() 方法返回的执行器不再使用线程池**。它会为每个提交的任务都创建一个新的虚拟线程。
        - 此外，ExecutorService 本身是轻量级的，我们可以像创建任何简单对象一样直接创建一个新的 ExecutorService 对象而不必考虑复用。

    - **使用信号量限制并发**

      - 使用虚拟线程时，如果要限制访问某些服务的并发请求，则应该使用专门为此目的设计的 Semaphore 类。
      - 简单地使用信号量阻塞某些虚拟线程可能看起来与将任务提交到固定数量线程池有很大不同，但事实并非如此。
        - 将任务提交到等待任务池会将它们排队处理，信号量在内部(或任何其他阻塞同步构造)构造了一个阻塞线程队列，这些任务在阻塞线程队列上也会进行排队处理。
        - 我们可以将平台线程池认作是从等待任务队列中提取任务进行处理的工作人员，然后将虚拟线程视为任务本身，在任务或者线程可以执行之前将会被阻塞，但它们在计算机中的底层表示上实际是相同的。
        - 这里想告诉大家的就是不管是线程池的任务排队，还是信号量内部的线程阻塞，它们之间是由等效性的。在虚拟线程某些需要限制并发数场景下，直接使用信号量即可。

    - **不要在线程局部变量中缓存可重用对象**

      - 虚拟线程支持线程局部变量（ThreadLocal），就像平台线程一样。通常线程局部变量用于将一些特定于上下文的信息与当前运行的代码关联起来，例如当前事务和用户 ID。
        - 对于虚拟线程来说，使用线程局部变量是完全合理的。但是如果考虑更安全、更有效的线程局部变量，可以使用 Scoped Values。
      - 线程局部变量有一种用途与虚拟线程是不太适合的，那就是缓存可重用对象。
        - 可重用对象的创建成本通常很高，通常消耗大量内存且可变，还不是线程安全的。它们被缓存在线程局部变量中，以减少它们实例化的次数以及它们在内存中的实例数量，好处是它们可以被线程上不同时间运行的多个任务重用，避免昂贵对象的重复创建。
      - 对于线程局部变量缓存可重用对象的问题，没有什么好的通用替代方案，但对于 SimpleDateFormat，我们应该将其替换为 DateTimeFormatter。DateTimeFormatter 是不可变的，因此单个实例就可以由所有线程共享

    - **避免长时间和频繁的 synchronized**

      - 当前虚拟线程实现由一个限制是，在同步块或方法内执行 synchronized 阻塞操作会导致 JDK 的虚拟线程调度程序阻塞宝贵的操作系统线程，而如果阻塞操作是在同步块或方法外完成的，则不会被阻塞。我们称这种情况为 “Pinning”。
      - 如果阻塞操作既长期又频繁，则 “Pinning” 可能会对服务器的吞吐量产生不利影响。如果阻塞操作短暂（例如内存中操作）或不频繁则可能不会产生不利影响。
      - 为了检测可能有害的 “Pinning” 实例，（JDK Flight Recorder (JFR) 在 “Pinning” 阻塞时间超过 20 毫秒时，会发出 jdk.VirtualThreadPinned 事件。
        - 或者我们可以使用系统属性 jdk.tracePinnedThreads 在线程被 “Pinning” 阻塞时发出堆栈跟踪。
        - 启动 Java 程序时添加 -Djdk.tracePinnedThreads=full 运行，会在线程被 “Pinning” 阻塞时打印完整的堆栈跟踪，突出显示本机帧和持有监视器的帧。使用 -Djdk.tracePinnedThreads=short 运行，会将输出限制为仅有问题的帧。
      - 如果这些机制检测到既长期又频繁 “Pinning” 的地方，请在这些特定地方将 synchronized 替换为 ReentrantLock。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 12.5 Java 与协程
2. [为什么我认为 Java 虚拟线程不会取代 Kotlin 协程 - 知乎](https://zhuanlan.zhihu.com/p/647257179)
3. [Java 21 虚拟线程：使用指南（一）- 博客园](https://www.cnblogs.com/waynaqua/p/17935918.html)
4. [Java 新技术：虚拟线程使用指南（二）- 博客园](https://www.cnblogs.com/waynaqua/p/17954092)

## 查看线程/进程 CPU 使用率

- 那么首先需要获取这个进程的PID: `ps -ef| grep [process name]`
- 然后查看该进程的CPU: `top -p [PID]`
  - `-p <进程ID>`：仅显示指定进程ID的信息。
- 查看这个进程的各个进程的CPU: `top -H -p [PID]`
  - `-H`：在进程信息中显示线程详细信息。
  - `-d <秒数>`：指定 top 命令的刷新时间间隔，单位为秒。

参考文档：

1. [linux 下查看某一进程的cpu使用率和这个线程中各个线程的cpu使用率 - CSDN](https://blog.csdn.net/learners_sjt/article/details/82461079)
2. [Linux top 命令 - RUNOOB](https://www.runoob.com/linux/linux-comm-top.html)

## 确定创建线程数量

- 创建多少线程合适， 要看多线程具体的应用场景。一般来说，我们可以将程序分为：**CPU 密集型程序和 I/O 密集型程序**， 而针对于 CPU 密集型程序和 I/O 密集型程序，其计算最佳线程数的方法是不同的 。
- **CPU 密集型程序**
  - 对于CPU密集型计算， 多线程本质上是提升多核CPU的利用率， 所以对于一个4核的CPU， 每个核一个线程， 理论上创建4个线程就可以了， 再多创建线程也只是增加线程切换的成本。所以， 对于CPU密集型的计算场景， 理论上“线程的量=CPU核数”就是最合适的。但是在实际工作中， 一般会将线程数量设置为“CPU核数+1”， 这样的话， 当线程因为偶尔的内存页失效或其他原因导致阻塞时， 这个额外的线程可以顶上， 从而保证CPU的利用率 。
  - **所以，在 CPU 密集型的程序中，一般可以将线程数设置为CPU核数+1。**
- **I/O 密集型程序**
  - 对于I/O密集型的程序，最佳的线程数是与程序中CPU计算和I/O操作的耗时比相关。总体来说，可以将其总结为如下的公式。
  - **单核 CPU**
    - **最佳线程数 = 1 +（I/O 耗时 / CPU 耗时）** 
    - 我们令 R=I/O 耗时 / CPU 耗时， 可以这样理解：当线程 A 执行 IO 操作时， 另外 R 个线程正好执行完各自的 CPU 计算。这样 CPU 的利用率就达到了 100%。
  - **多核 CPU**
    - 多核CPU的最佳线程数在单核CPU最佳线程数的基础上，乘以CPU核数即可，如下所示。
    - **最佳线程数 = CPU核数 \* [ 1 +（I/O 耗时 / CPU 耗时）]** 
- **总结**
  - 上述公式计算的结果为最佳理论值，实际工作中还是要通过实际压测数据来找到最佳线程数，将硬件的性能发挥到极致。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景三十四：高并发场景下创建多少线程才合适？

## 死锁必要条件、预防、避免

- 四个必要条件
  - **互斥条件**：资源是独占的且排他使用，进程互斥使用资源，即任意时刻一个资源只能给一个进程使用，其他进程若申请一个资源，而该资源被另一进程占有时，则申请者等待直到资源被占有者释放。
  - **不可剥夺条件**：进程所获得的资源在未使用完毕之前，不被其他进程强行剥夺，而只能由获得该资源的进程资源释放。
  - **请求和保持条件**：进程每次申请它所需要的一部分资源，在申请新的资源的同时，继续占用已分配到的资源。
  - **循环等待条件**：在发生死锁时必然存在一个进程等待队列{P1,P2,…,Pn},其中P1等待P2占有的资源，P2等待P3占有的资源，…，Pn等待P1占有的资源，形成一个进程等待环路，环路中每一个进程所占有的资源同时被另一个申请，也就是前一个进程占有后一个进程所深情地资源。
  - 以上给出了导致死锁的四个必要条件，只要系统发生死锁则以上四个条件至少有一个成立。事实上循环等待的成立蕴含了前三个条件的成立，似乎没有必要列出然而考虑这些条件对死锁的预防是有利的，因为可以通过破坏四个条件中的任何一个来预防死锁的发生。
- 死锁预防
  - 我们可以通过破坏死锁产生的4个必要条件来 预防死锁，由于资源互斥是资源使用的固有特性是无法改变的。
  - **破坏“不可剥夺”条件**：一个进程不能获得所需要的全部资源时便处于等待状态，等待期间他占有的资源将被隐式的释放重新加入到 系统的资源列表中，可以被其他的进程使用，而等待的进程只有重新获得自己原有的资源以及新申请的资源才可以重新启动，执行。
  - **破坏”请求与保持条件“**：第一种方法静态分配即每个进程在开始执行时就申请他所需要的全部资源。第二种是动态分配即每个进程在申请所需要的资源时他本身不占用系统资源。
  - **破坏“循环等待”条件**：采用资源有序分配其基本思想是将系统中的所有资源顺序编号，将紧缺的，稀少的采用较大的编号，在申请资源时必须按照编号的顺序进行，一个进程只有获得较小编号的进程才能申请较大编号的进程。
- 死锁避免
  - 死锁避免的基本思想：系统对进程发出的每一个系统能够满足的资源申请进行动态检查，并根据检查结果决定是否分配资源，如果分配后系统可能发生死锁，则不予分配，否则予以分配，这是一种保证系统不进入死锁状态的动态策略。
  - 银行家算法
  - 安全状态、不安全状态

参考文档：

1. [死锁的四个必要条件](https://blog.csdn.net/jyy305/article/details/70077042)

## Java 线程状态

- Java语言定义了6种线程状态，在任意一个时间点中，一个线程只能有且只有其中的一种状态，并且可以通过特定的方法在不同状态之间转换。这6种状态分别是：
  - **新建（New）**：创建后尚未启动的线程处于这种状态。
  - **运行（Runnable）**：包括操作系统线程状态中的Running和Ready，也就是处于此状态的线程有可能正在执行，也有可能正在等待着操作系统为它分配执行时间。
  - **无限期等待（Waiting）**：处于这种状态的线程不会被分配处理器执行时间，它们要等待被其他线程显式唤醒。以下方法会让线程陷入无限期的等待状态：
    - 没有设置Timeout参数的Object::wait()方法；
    - 没有设置Timeout参数的Thread::join()方法；
    - LockSupport::park()方法。
  - **限期等待（Timed Waiting）**：处于这种状态的线程也不会被分配处理器执行时间，不过无须等待被其他线程显式唤醒，在一定时间之后它们会由系统自动唤醒。以下方法会让线程进入限期等待状态：
    - Thread::sleep()方法；
    - 设置了Timeout参数的Object::wait()方法；
    - 设置了Timeout参数的Thread::join()方法；
    - LockSupport::parkNanos()方法；
    - LockSupport::parkUntil()方法。
  - **阻塞（Blocked）**：线程被阻塞了，“阻塞状态”与“等待状态”的区别是“阻塞状态”在等待着获取到一个排它锁，这个事件将在另外一个线程放弃这个锁的时候发生；而“等待状态”则是在等待一段时间，或者唤醒动作的发生。在程序等待进入同步区域的时候，线程将进入这种状态。
  - **结束（Terminated）**：已终止线程的线程状态，线程已经结束执行。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 12.4.3 状态转换

## 有 synchronized 为什么还提供 Lock？

- 参考死锁的必要条件

  - 如果我们的程序使用synchronized关键字发生了死锁时，synchronized关键是是**无法破坏“不可剥夺”这个死锁的条件的**。这是因为synchronized申请资源的时候， 如果申请不到， 线程直接进入阻塞状态了， 而线程进入阻塞状态， 啥都干不了， 也释放不了线程已经占有的资源。

- 解决问题

  - （1）**能够响应中断**。
    - synchronized的问题是， 持有锁A后， 如果尝试获取锁B失败， 那么线程就进入阻塞状态， 一旦发生死锁， 就没有任何机会来唤醒阻塞的线程。
    - 但如果阻塞状态的线程能够响应中断信号， 也就是说当我们给阻塞的线程发送中断信号的时候， 能够唤醒它， 那它就有机会释放曾经持有的锁A。这样就破坏了不可剥夺条件了。
  - （2）**支持超时**。
    - 如果线程在一段时间之内没有获取到锁， 不是进入阻塞状态， 而是返回一个错误，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可剥夺条件。
  - （3）**非阻塞地获取锁**。
    - 如果尝试获取锁失败， 并不进入阻塞状态， 而是直接返回， 那这个线程也有机会释放曾经持有的锁。这样也能破坏不可剥夺条件。

- 体现在 Lock 接口上，就是 Lock 接口提供的三个方法

  - ```java
    // 支持中断的API
    void lockInterruptibly() throws InterruptedException;
    // 支持超时的API
    boolean tryLock(long time, TimeUnit unit) throws InterruptedException;
    // 支持非阻塞获取锁的API
    boolean tryLock();
    ```

- 例外，Lock下面有一个ReentrantLock，而**ReentrantLock支持公平锁和非公平锁**。

  - 锁的实现在本质上都对应着一个入口等待队列， 如果一个线程没有获得锁， 就会进入等待队列， 当有线程释放锁的时候， 就需要从等待队列中唤醒一个等待的线程。
    - 如果是公平锁， 唤醒的策略就是谁等待的时间长， 就唤醒谁， 很公平；
    - 如果是非公平锁， 则不提供这个公平保证， 有可能等待时间短的线程反而先被唤醒。 
  - 而Lock是支持公平锁的，synchronized不支持公平锁。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十：Java 中提供了 synchronized，为什么还要提供 Lock 呢？

## 局部变量是线程安全的

- 每个方法在调用栈里都会有自己独立的栈帧，每个栈帧里都有对应方法需要的参数和返回地址。当调用方法时，会创建新的栈帧，并压入调用栈；当方法返回时，对应的栈帧就会被自动弹出。
- 我们可以这样说：**栈帧是在调用方法时创建，方法返回时“消亡”。**
- **局部变量存放在哪里？**
  - 局部变量的作用域在方法内部，当方法执行完，局部变量也就没用了。可以这么说，方法返回时，局部变量也就“消亡”了。此时，我们会联想到调用栈的栈帧。没错，**局部变量就是存放在调用栈里的**。
- **调用栈与线程**
  - 两个线程就可以同时用不同的参数调用相同的方法。**那么问题来了，调用栈和线程之间是什么关系呢？答案是：每个线程都有自己独立的调用栈**。
- 此时，我们再看下文中开头的问题：**Java 方法内部的局部变量是否存在并发问题？答案是不存在并发问题！因为每个线程都有自己的调用栈，局部变量保存在线程各自的调用栈里，不会共享，自然也就不存在并发问题。**
- **线程封闭**
  - 方法里的局部变量，因为不会和其他线程共享，所以不会存在并发问题。这种解决问题的技术也叫做线程封闭。官方的解释为：仅在单线程内访问数据。由于不存在共享，所以即使不设置同步，也不会出现并发问题！

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景十一：为啥局部变量是线程安全的？

## 线程池：ThreadPoolExecutor 七大参数

ThreadPoolExecutor 7 大参数

1. int **corePoolSize**：

   - 线程池中的常驻核心线程数

2. int **maximumPoolSize**

   - 线程池能够容纳同时执行的最大线程数，此值必须大于等于 1

3. long **keepAliveTime**

   - 多余的空闲线程的存活时间。当前线程池数量超过 corePoolSize 时，当空闲时间达到 keepAliveTime 值时，多余空闲线程会被销毁直到只剩下 corePoolSize 个线程为止

4. TimeUnit **unit**

   - keepAliveTime 的单位。

5. `BlockingQueue<Runnable>` **workQueue**

   - 任务队列，被提交但尚未被执行的任务。

6. ThreadFactory **threadFactory**

   - 表示生成线程池中工作线程的线程工厂，用于创建线程一般用默认的即可。

7. RejectedExecutionHandler **handler**

   - 拒绝策略，表示当队列满了并且工作线程大于等于线程池的最大线程数（maximumPoolSize）时如何来拒绝

   - 线程池的拒绝策略你谈谈
     - 是什么（1：01）
     - JDK 内置的拒绝策略
       - **AbortPolicy**（默认）：直接抛出 RejectedExecutionException 异常阻止系统正常运行。
       - **CallerRunsPolicy**：“调用者运行”一种调节机制，该策略既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量。
       - **DiscardOldestPolicy**：抛弃队列中等待最久的任务，然后把当前任务加入队列中尝试再次提交当前 任务。
       - **DiscardPolicy**：直接丢弃任务，不予任何处理也不抛出异常。如果允许任务丢失，这是最好的一种方案。
     - 以上内置拒绝策略均实现了 **RejectedExecutionHandler** 接口

参考文档：

1. [《尚硅谷Java大厂面试题第2季》笔记.md](../视频笔记/《尚硅谷Java大厂面试题第2季》笔记.md) P49、P51

## 线程池：Executors 默认创建方法

- 了解

  - **Executors.newScheduledThreadPool(int)**

    - 创建一个核心线程固定大小的线程池，用于定时执行任务。核心线程数量固定，最大线程数量 Integer.MAX_VALUE。适用于定时执行任务的场景。

    - ```java
      return new ScheduledThreadPoolExecutor(corePoolSize);
      
      public ScheduledThreadPoolExecutor(int corePoolSize) {
          // 对应 ThreadPoolExecutor
          super(corePoolSize, Integer.MAX_VALUE,
                DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS,
                new DelayedWorkQueue());
      }
      ```

  - **Executors.newSingleThreadScheduledExecutor()**

    - 创建一个单线程的定时执行线程池。只包含一个线程，用于串行定时执行任务。

    - ```java
      return new DelegatedScheduledExecutorService
                  (new ScheduledThreadPoolExecutor(1));
      ```

  - java8 新出

    - Executors.newWorkStealingPool(int parallelism)

      - 创建一个工作窃取线程池，线程数量根据CPU核心数动态调整。适用于CPU密集型的任务。

      - java8 新增，使用目前机器上可用的处理器作为它的并行级别

      - ```java
        // 不传 parallelism 的重载方法，默认取 Runtime.getRuntime().availableProcessors()
        return new ForkJoinPool
            (parallelism,
             	ForkJoinPool.defaultForkJoinWorkerThreadFactory,
             	null, true);
        ```

- 重点

  - **Executors.newFixedThreadPool(int)**（19：58）

    - 创建一个固定大小的线程池，其中包含指定数量的线程。线程数量是固定的，不会自动扩展。适用于执行固定数量的长期任务。

    - 执行长期的任务，性能好很多

    - ```java
      return new ThreadPoolExecutor(nThreads, nThreads,
                                    0L, TimeUnit.MILLISECONDS,
                                    new LinkedBlockingQueue<Runnable>());
      ```

  - **Executors.newSingleThreadExecutor()**（20：39）

    - 创建一个单线程的线程池。这个线程池中只包含一个线程，用于串行执行任务。适用于需要按顺序执行任务的场景。

    - 一个任务一个任务执行的场景

    - ```java
      return new FinalizableDelegatedExecutorService
                  (new ThreadPoolExecutor(1, 1,
                                          0L, TimeUnit.MILLISECONDS,
                                          new LinkedBlockingQueue<Runnable>()));
      ```

  - **Executors.newCachedThreadPool()**（21：21）

    - 创建一个可缓存的线程池。这个线程池的线程数量可以根据需要自动扩展，如果有可用的空闲线程，就会重用它们；如果没有可用的线程，就会创建一个新线程。适用于执行大量的短期异步任务。

    - 适用：执行很多短期异步的小程序或者负载较轻的服务器

    - ```java
      return new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                                    60L, TimeUnit.SECONDS,
                                    new SynchronousQueue<Runnable>());
      ```

- 你在工作中单一的/固定数的/可变的三种创建线程池的方法，你用哪个多？超级大坑

  - 答案是一个都不用，我们生产上只能使用自定义的

  - Executors 中 JDK 已经给你提供了，为什么不用？

    - （5：10）

      阿里巴巴 Java 开发手册

      4、【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式

      说明：Executors 返回的线程池对象的弊端如下：

      1）FixedThreadPool 和 SingleThreadPool：

      允许的**请求队列长度**为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。

      2）CachedTheadPool 和 ScheduledThreadPool：

      允许的**创建线程数量**为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。

参考文档：

1. [《尚硅谷Java大厂面试题第2季》笔记.md](../视频笔记/《尚硅谷Java大厂面试题第2季》笔记.md) P47、P52
2. [【Java 基础篇】Executors工厂类详解 - CSDN](https://blog.csdn.net/qq_21484461/article/details/133101696)（感觉细节有点问题，还是优先参考源码）

## ForkJoinPool

- 使用场景

  - **ForkJoinPool 就是设计用来解决父子任务有依赖的并行计算问题的。**  

    - 类似于快速排序、二分查找、集合运算等有父子依赖的并行计算问题，都可以用 ForkJoinPool 来解决。

    - 对于 Fibonacci 数列问题，如果用 ForkJoinPool 来实现，其实现代码为：

      ```java
      @Slf4j
      public class ForkJoinDemo {
          // 1. 运行入口
          public static void main(String[] args) {
              int n = 20;
      
              // 为了追踪子线程名称，需要重写 ForkJoinWorkerThreadFactory 的方法
              final ForkJoinPool.ForkJoinWorkerThreadFactory factory = pool -> {
                  final ForkJoinWorkerThread worker = ForkJoinPool.defaultForkJoinWorkerThreadFactory.newThread(pool);
                  worker.setName("my-thread" + worker.getPoolIndex());
                  return worker;
              };
      
              //创建分治任务线程池，可以追踪到线程名称
              ForkJoinPool forkJoinPool = new ForkJoinPool(4, factory, null, false);
      
              // 快速创建 ForkJoinPool 方法
              // ForkJoinPool forkJoinPool = new ForkJoinPool(4);
      
              //创建分治任务
              Fibonacci fibonacci = new Fibonacci(n);
      
              //调用 invoke 方法启动分治任务
              Integer result = forkJoinPool.invoke(fibonacci);
              log.info("Fibonacci {} 的结果是 {}", n, result);
          }
      }
      
      // 2. 定义拆分任务，写好拆分逻辑
      @Slf4j
      class Fibonacci extends RecursiveTask<Integer> {
          final int n;
          Fibonacci(int n) {
              this.n = n;
          }
      
          @Override
          public Integer compute() {
              //和递归类似，定义可计算的最小单元
              if (n <= 1) {
                  return n;
              }
              // 想查看子线程名称输出的可以打开下面注释
              //log.info(Thread.currentThread().getName());
      
              Fibonacci f1 = new Fibonacci(n - 1);
              // 拆分成子任务
              f1.fork();
              Fibonacci f2 = new Fibonacci(n - 2);
              // f1.join 等待子任务执行结果
              return f2.compute() + f1.join();
          }
      }
      ```

    - 如上面代码所示，我们定义了一个 Fibonacci 类，继承了 RecursiveTask 抽象类。在 Fibonacci 类中，我们定义了拆分逻辑，并调用了 join() 等待子线程执行结果。

    - 上面代码中提到的 fork () 和 join () 是 ForkJoinPool 提供的 API 接口，主要用于执行任务以及等待子线程结果。关于其详细用法，我们稍后会讲到。

  - **除了用于处理父子任务有依赖的情形，其实 ForkJoinPool 也可以用于处理需要获取子任务执行结果的场景。**  

    - 例如：我们要计算 1 到 1 亿的和，为了加快计算的速度，我们自然想到算法中的分治原理，将 1 亿个数字分成 1 万个任务，每个任务计算 1 万个数值的综合，利用 CPU 的并发计算性能缩短计算时间。
    - 对比 ThreadPoolExecutor 和 ForkJoinPool 这两者的实现，可以发现它们都有任务拆分的逻辑，以及最终合并数值的逻辑。但 ForkJoinPool 相比 ThreadPoolExecutor 来说，做了一些实现上的封装，例如：
      - 不用手动去获取子任务的结果，而是使用 join () 方法直接获取结果。
      - 将任务拆分的逻辑，封装到 RecursiveTask 实现类中，而不是裸露在外。
    - 因此对于没有父子任务依赖，但是希望获取到子任务执行结果的并行计算任务，也可以使用 ForkJoinPool 来实现。**在这种情况下，使用 ForkJoinPool 实现更多是代码实现方便，封装做得更加好。**

- 使用指南

  - 使用 ForkJoinPool 来进行并行计算，主要分为两步：
    1. 定义 RecursiveTask 或 RecursiveAction 的任务子类。
    2. 初始化线程池及计算任务，丢入线程池处理，取得处理结果。
  - **首先，我们需要定义一个 RecursiveTask 或 RecursiveAction 的子类，然后再该类的 compute () 方法中定义拆分逻辑和计算逻辑。**  这两个抽象类的区别在于：前者有返回值，后者没有返回值。
    - 对于 compute () 方法的实现，核心是想清楚：怎么拆分成子任务？什么时候结束拆分？
  - **接着，初始化 ForkJoinPool 线程池，初始化计算任务，最后将任务丢入线程池中。**

- 原理解析

  - ForkJoinPool 的设计思想是分治算法，即将任务不断拆分（fork）成更小的任务，最终再合并（join）各个任务的计算结果。通过这种方式，可以充分利用 CPU 资源，再结合工作窃取算法（worksteal）整体提高执行效率。
  - 从图中可以看出 ForkJoinPool 要先执行完子任务才能执行上一层任务。**因此 ForkJoinPool 最适合有父子任务依赖的场景，其次就是需要获取子任务执行结果的场景。比如：Fibonacci 数列、快速排序、二分查找等。**

- 源码实现

  - ForkJoinPool 的主要实现类为：ForkJoinPool 和 ForkJoinTask 抽象类。
  - ForkJoinTask 实现了 Future 接口，可以用于获取处理结果。
  - ForkJoinTask 有两个抽象子类：RecursiveAction 和 RecursiveTask 抽象类，其区别在于前者没有返回值，后者有返回值，其类图如下所示。

- 窃取算法

  - 我们知道 ForkJoinPool 的父子任务之间是有依赖关系的，那么 ForkJoinPool 是如何实现的呢？**答案是：利用不同任务队列执行。**  
    - 在 ForkJoinPool 中有一个数组形式的成员变量 `workQueue[]`，其对应一个队列数组，每个队列对应一个消费线程。丢入线程池的任务，根据特定规则进行转发。
  - 这样就有一个问题：有些队列可能任务比较多，有些队列任务比较少，这样就会导致不同线程负载不一样，整体不够高效，怎么办呢？
    - **答案是：利用窃取算法，空闲的线程从尾部去消费其他队列的任务。**
    - 一般情况下，线程获取自己队列中的任务是 LIFO（Last Input First Output 后进先出）的方式，即类似于栈的操作方式。如下图所示，首先放入队列的时候先将任务 Push 进队列的头部（top），之后消费的时候在 pop 出队列头部（top）。
    - 而当某个线程对应的队列空闲时，该线程则去队列的底部（base）窃取（poll）任务到自己的队列，然后进行消费。**那问题来了：为什么不从头部（top）获取任务，而要从底部（base）获取任务呢？**  那是为了避免冲突！如果两个线程同时从顶部获取任务，那就会有多线程的冲突问题，就需要加锁操作，从而降低了执行效率。

参考文档：

1. [深入理解 ForkJoinPool：入门、使用、原理 - 掘金](https://juejin.cn/post/7150836399234236430)

# Spring

## 核心组件

- Data Access / Integration
  - JDBC
  - ORM
  - OXM
  - JMS
  - Transactions
- Web（MVC / Remoting）
  - Web
  - Servlet
  - Portlet
  - Struts
- AOP
- Aspects
- Instrumentation
- Core Container
  - Beans
  - Core
  - Context
  - Expression Language
- Test

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring 核心组件

## 常用模块

- 核心容器
  - 核心容器提供 Spring 框架的基本功能。核心容器的主要组件是 BeanFactory，它是工厂模式的实现。BeanFactory 使用控制反转（IOC）模式将应用程序的配置和依赖性规范与实际的应用程序代码分开。
- Spring 上下文
  - Spring 上下文是一个配置文件，向 Spring 框架提供上下文信息。Spring 上下文包括企业服务，例如 JNDI、EJB、电子邮件、国际化、校验和调度功能。
- Spring AOP
  - 通过配置管理特性，Spring AOP 模块直接将面向切面的编程功能集成到了 Spring 框架中。可以将一些通用任务，如安全、事务、日志等集中进行管理，提高了复用性和管理的便捷性。
- Spring DAO
  - 为 JDBC DAO 抽象层提供了有意义的异常层次结构，可用该结构来管理异常处理和不同数据库供应商抛出的错误消息。异常层次结构简化了错误处理，并且极大地降低了需要编写的异常代码数量（例如打开和关闭连接）。Spring DAO 的面向 JDBC 的异常遵从通用的 DAO 异常层次结构
- Spring ORM
  - Spring 框架插入了若干个 ORM 框架，从而提供了 ORM 的对象关系工具，其中包括 JDO、Hibernate 和 iBatis SQL Map。所有这些都遵从 Spring 的通用事务和 DAO 异常层次结构。
- Spring Web 模块
  - Web 上下文模块建立在应用程序上下文模块之上，为基于 Web 的应用程序提供了上下文。所以，Spring 框架支持与 Jakarta Structs 的集成。Web 模块还简化了处理多部分请求以及将请求参数绑定到域对象的工作。
- Spring MVC 框架
  - MVC 框架是一个全功能的构建 Web 应用程序的 MVC 实现。通过策略接口，MVC 框架便成为高度可配置的，MVC 容纳了大量视图技术，其中包括 JSP、Velocity、Tiles、iText 和 POI。

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring 常用模块

## 常用注解

- 1
  - @Controller
  - @RestController
    - 相当于 @Controller 和 @ResponseBody 的组合效果
  - @Component
  - @Repository
  - @Service
- 2
  - @ResponseBody
    - 异步请求
    - 该注解用于将 Controller 的方法返回的对象，通过适当的 HttpMessageConverter 转换为指定模式后，写入到 Response 对象的 body 数据区
    - 返回的数据不是 html 标签的页面，而是其他某种格式的数据（如 json、xml 等）时使用
  - @RequestMapping
    - 一个用来处理请求地址的注解，可用于类或方法上。用于类上，表示类中的所有响应请求的方法都是以该地址作为父路径
  - @Autowired
    - 它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。通过 @Autorwired 的使用来消除 set、get 方法
  - @PathVariable
    - 用于将请求 URL 中的模板变量映射到功能处理方法的参数上，即取出 url 模板中的变量做为参数
  - @RequestParam
    - 主要用于在 SpringMVC 后台控制层获取参数，类似一种是 request.getParameter("name")
  - @RequestHeader
    - 可以把 Request 请求 header 部分的值绑定到方法的参数上
- 3
  - @ModelAttribute
  - @SessionAttribute
    - 即将值放到 session 作用域中，写在 class 上面
  - @Valid
    - 实体数据校验，可以结合 hibernate validator 一起使用
  - @CookieValue
    - 用来获取 Cookie 中的值

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring 常用注解

## 依赖注入

- 构造器注入
  - 在构造器注入中，依赖关系通过类的构造函数传递。这意味着在创建对象时，依赖的对象实例会作为构造函数的参数传递进来。
  - 优点：
    1. 对象的依赖关系在创建时就被确定，对象一旦创建就不可变，有助于保持对象的一致性和可靠性。
    2. 在构造函数中明确声明依赖，可以使类的使用更加清晰，减少了后续对依赖的猜测。
- setter 注入
  - 在Setter注入中，依赖通过类的setter方法进行注入。这意味着你可以在对象创建后随时改变依赖关系。
  - 优点：
    1. 灵活性高，可以在运行时动态更改依赖关系。
    2. 允许逐步构建对象，不需要一次性提供所有依赖。
- 选择构造器注入还是Setter注入取决于以下因素：
  1. 不变性需求： 如果对象的依赖关系在创建后不应该更改，构造器注入是一个好的选择。
  2. 灵活性需求： 如果对象的依赖关系可能在运行时更改，Setter注入更为合适。
  3. 清晰性： 构造器注入通常更容易理解，因为依赖关系在对象创建时就被确定。
  4. 依赖数量： 如果类有大量的依赖，构造器注入可能更清晰，而不是在构造函数中添加大量的参数。
- 在实践中，有时也可以使用构造器注入和Setter注入的组合，以满足不同的需求。
- 总结：以上论点就是：
  - **构造器注入提倡不可变性：** 通过构造器注入对象，实现了对象初始化后的不可变性，同时确保所需依赖不为空。这有助于保持对象状态的稳定性。
  - **构造器注入促使代码质量提升：** 通过构造器注入，可以清晰地看到类的依赖关系，大量构造器参数说明当前类耦合过多、职责过多，从而促使编码者考虑是否需要重构，以提高代码质量和可维护性。
  - **Setter注入适用于可选依赖：** Setter注入主要用于可选依赖，这些依赖可以在类内部被合理默认赋值。然而，需要注意的是，Setter注入的对象需要进行非空检查，因为它们具有可变性。
  - **Setter注入支持对象的动态重配置：** 通过Setter注入，对象可以在运行时进行重新配置或重新注入。这使得Setter注入在JMX MBeans等需要动态管理的场景下变得特别有用。
- 顺便吐槽一下，网上有各种错误的关于 Spring Bean 依赖注入方式有多少种的回答，数字可以从三种开始一直往上加……
  - 但其实就只有我们提到的：**构造器**和 **setter** 两种。具体可以参考 Spring 官方文档：[**1.4.1. Dependency Injection**: https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#beans-factory-collaborators](https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#beans-factory-collaborators) 文档中可以看到：“DI exists in two major variants: [Constructor-based dependency injection](https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#beans-constructor-injection) and [Setter-based dependency injection](https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#beans-setter-injection).”，即“DI 存在两种：基于构造函数的依赖注入和基于 Setter 的依赖注入。”
  - 网上大多数错误的文章是把 Bean 相关的一些 Spring 使用方式单独作为一种来和这两种注入方式并列，个人认为其实并不合适。
    - 一种错误是把 **xml、注解**实现这些具体的**配置写法**作为单独的一种分离出来，这种错误较为明显。
    - 还有一种错误是误把 **`@Autowired` 或工厂方式**作为单独的种类
      - 其实前者是把**自动装配功能**错误理解成了注入方式的一种
      - 而后者是将 **Bean 的生成**与注入本身混淆了。
  - 个人认为以官方文档为准即可，分享一下相关思考供大家参考。

参考文档

1. [Spring Framework中的依赖注入：构造器注入 vs. Setter注入  - 腾讯云](https://cloud.tencent.com/developer/article/2358469)
2. [函数式编程和设计模式（Cake模式依赖注入）.md](../个人创作/技术文章/函数式编程和设计模式（Cake模式依赖注入）.md) 1 依赖注入

## 自动装配

- xml
  - autowire="byName"(按名称自动装配)
  - autowire="byType" (按类型自动装配)
- 使用注解
  1. @Autowired（按类型自动转配的，不支持id匹配）
     - @Qualifier（不能单独使用，加上@Qualifier则可以根据byName的方式自动装配）
  2. @Resource（先进行byName查找，失败；再进行byType查找）
- @Autowired与@Resource异同
  - 1、@Autowired与@Resource都可以用来装配bean。都可以写在字段上，或写在setter方法上。
  - 2、@Autowired默认按类型装配（属于spring规范），默认情况下必须要求依赖对象必须存在，如果要允许 null 值，可以设置它的required属性为false，如：@Autowired(required=false)，如果我们想使用名称装配可以结合@Qualifier注解进行使用
  - 3、@Resource（属于J2EE复返），默认按照名称进行装配，名称可以通过name属性进行指定。如果没有指定name属性，当注解写在字段上时，默认取字段名进行按照名称查找，如果注解写在setter方法上默认取属性名进行装配。当找不到与名称匹配的bean时才按照类型进行装配。但是需要注意的是，**如果name属性一旦指定，就只会按照名称进行装配**。
  - 它们的作用相同都是用注解方式注入对象，但执行顺序不同。@Autowired先byType，@Resource先byName。

参考文档：

1. [5.自动装配：autowire=“byName“ or “byType“ + 使用注解【@Autowired 、@Qualifier、 @Resource】- CSDN](https://blog.csdn.net/weixin_42214698/article/details/122781230)

## xml bean 标签 autowire 属性

| 模式        | 说明                                                         |
| ----------- | ------------------------------------------------------------ |
| no          | (默认)不采用autowire机制。当我们需要使用依赖注入，只能用 ref |
| byName      | 通过属性的名称自动装配（注入）。Spring会在容器中查找名称与bean属性名称一致的bean，并自动注入到bean属性中。当然bean的属性需要有setter方法。例如：bean A有个属性master，master的setter方法就是setMaster，A设置了autowire=“byName”，那么Spring就会在容器中查找名为master的bean通过setMaster方法注入到A中。 |
| byType      | 通过类型自动装配（注入）。Spring会在容器中查找类（Class）与bean属性类一致的bean，并自动注入到bean属性中，如果容器中包含多个这个类型的bean，Spring将抛出异常。如果没有找到这个类型的bean，那么注入动作将不会执行。 |
| constructor | 类似于byType，但是是通过构造函数的参数类型来匹配。假设bean A有构造函数A(B b, C c)，那么Spring会在容器中查找类型为B和C的bean通过构造函数A(B b, C c)注入到A中。与byType一样，如果存在多个bean类型为B或者C，则会抛出异常。但时与byType不同的是，如果在容器中找不到匹配的类的bean，将抛出异常，因为Spring无法调用构造函数实例化这个bean。 |
| autodetect  | 如果发现默认的构造方法，则用 constructor 模式，否则用 byType 模式 |
| default     | 采用父级标签的配置。（即beans的default-autowire属性）        |

参考文档：

1. [【Spring】Spring配置文件bean标签的autowire属性 - CSDN](https://blog.csdn.net/qq_30081043/article/details/107531417)

## IOC 容器实现（FactoryBean、ApplicationContext、WebApplication）

- 概念

  - Spring 通过一个配置文件描述 Bean 及 Bean 之间的依赖关系，利用 Java 语言的反射功能实例化 Bean 并建立 Bean 之间的依赖关系。
  - Spring 的 IoC 容器在完成这些底层工作的基础上，还提供了 Bean 实例缓存、生命周期管理、 Bean 实例代理、事件发布、资源装载等高级服务。

- Spring 容器高层视图

  - Spring 启动时读取应用程序提供的 Bean 配置信息，并在 Spring 容器中生成一份相应的 Bean 配置注册表，然后根据这张注册表实例化 Bean，装配好 Bean 之间的依赖关系，为上层应用提供准备就绪的运行环境。其中 Bean 缓存池为 HashMap 实现

- IOC 容器实现

  - **BeanFactory -** 框架基础设施

    - BeanFactory 是 Spring 框架的基础设施，面向 Spring 本身；ApplicationContext 面向使用 Spring 框架的开发者，几乎所有的应用场合我们都直接使用 ApplicationContext 而非底层的 BeanFactory。

    - ```mermaid
      classDiagram
      	DefaultListableBeanFactory <|-- XmlBeanFactory
      	BeanDefinitionRegistry <|.. DefaultListableBeanFactory
      	ConfigurableListableBeanFactory <|.. DefaultListableBeanFactory
      	ListableBeanFactory <|-- ConfigurableListableBeanFactory
      	BeanFactory <|-- ListableBeanFactory
      	ConfigurableBeanFactory <|-- ConfigurableListableBeanFactory
      	HierarchicalBeanFactory <|-- ConfigurableBeanFactory
      	BeanFactory <|-- HierarchicalBeanFactory
      	AbstractAutowireCapableBeanFactory <|-- DefaultListableBeanFactory
      	AbstractBeanFactory <|-- AbstractAutowireCapableBeanFactory
      	ConfigurableBeanFactory <|.. AbstractBeanFactory
      	DefaultSingletonBeanRegistry <|-- AbstractBeanFactory
      	SingletonBeanRegistry <|.. DefaultSingletonBeanRegistry
      	AutowireCapableBeanFactory <|.. AbstractAutowireCapableBeanFactory
      	BeanFactory <|-- AutowireCapableBeanFactory
      ```

    - 详解

      1. *BeanDefinitionRegistry* 注册表
         - Spring 配置文件中每一个节点元素在 Spring 容器里都通过一个 BeanDefinition 对象表示，它描述了 Bean 的配置信息。而 BeanDefinitionRegistry 接口提供了向容器手工注册 BeanDefinition 对象的方法。
      2. *BeanFactory* 顶层接口
         - 位于类结构树的顶端 ，它最主要的方法就是 getBean(String beanName)，该方法从容器中返回特定名称的 Bean，BeanFactory 的功能通过其他的接口得到不断扩展
      3. *ListableBeanFactory*
         - 该接口定义了访问容器中 Bean 基本信息的若干方法，如查看 Bean 的个数、获取某一类型 Bean 的配置名、查看容器中是否包括某一 Bean 等方法；
      4. *HierarchicalBeanFactory* 父子级联
         - 父子级联 IoC 容器的接口，子容器可以通过接口方法访问父容器； 通过 HierarchicalBeanFactory 接口， Spring 的 IoC 容器可以建立父子层级关联的容器体系，子容器可以访问父容器中的 Bean，但父容器不能访问子容器的 Bean。
         - Spring 使用父子容器实现了很多功能，比如在 Spring MVC 中，展现层 Bean 位于一个子容器中，而业务层和持久层的 Bean 位于父容器中。这样，展现层 Bean 就可以引用业务层和持久层的 Bean，而业务层和持久层的 Bean 则看不到展现层的 Bean。
      5. *ConfigurableBeanFactory*
         - 是一个重要的接口，增强了 IoC 容器的可定制性，它定义了设置类装载器、属性编辑器、容器初始化后置处理器等方法；
      6. *AutowireCapableBeanFactory* 自动装配
         - 定义了将容器中的 Bean 按某种规则（如按名字匹配、按类型匹配等）进行自动装配的方法；
      7. *SingletonBeanRegistry* 运行期间注册单例 *Bean*
         -  定义了允许在运行期间向容器注册单实例 Bean 的方法；对于单实例（ singleton）的 Bean 来说，BeanFactory 会缓存 Bean 实例，所以第二次使用 getBean() 获取 Bean 时将直接从 IoC 容器的缓存中获取 Bean 实例。Spring 在 DefaultSingletonBeanRegistry 类中提供了一个用于缓存单实例 Bean 的缓存器，它是一个用 HashMap 实现的缓存器，单实例的 Bean 以 beanName 为键保存在这个 HashMap 中。
      8. 依赖日志框架
         - 在初始化 BeanFactory 时，必须为其提供一种日志框架，比如使用 Log4J， 即在类路径下提供 Log4J 配置文件，这样启动 Spring 容器才不会报错。

  - **ApplicationContext** 面向开发应用

    - ApplicationContext 由 BeanFactory 派生而来，提供了更多面向实际应用的功能。

    - ApplicationContext 继承了 HierarchicalBeanFactory 和 ListableBeanFactory 接口，在此基础上，还通过多个其他的接口扩展了 BeanFactory 的功能：

      - ```mermaid
        classDiagram
        	AbstractXmlApplicationContext <|-- FileSystemXmlApplicationContext
        	AbstractXmlApplicationContext <|-- ClassPathXmlApplicationContext
        	AbstractRefreshableConfigApplicationContext <|-- AbstractXmlApplicationContext
        	AbstractRefreshableApplicationContext <|-- AbstractRefreshableConfigApplicationContext
        	AbstractApplicationContext <|-- AbstractRefreshableApplicationContext
        	ConfigurableApplicationContext <|.. AbstractApplicationContext
        	ApplicationContext <|-- ConfigurableApplicationContext
        	Lifecycle <|-- ConfigurableApplicationContext
        	ResourcePatternResolver <|-- ApplicationContext
        	ResourceLoader <|-- ResourcePatternResolver
        	MessageSource <|-- ApplicationContext
        	ApplicationEventPublisher <|-- ApplicationContext
        	HierachicalBeanFactory <|-- ApplicationContext
        	ListableBeanFactory <|-- ApplicationContext
        	BeanFactory <|-- HierachicalBeanFactory
        	BeanFactory <|-- ListableBeanFactory
        ```

    - 详解

      1. ClassPathXmlApplicationContext：默认从类路径加载配置文件
      2. FileSystemXmlApplicationContext：默认从文件系统中装载配置文件
      3. ApplicationEventPublisher：让容器拥有发布应用上下文事件的功能，包括容器启动事件、关闭事件等。
      4. MessageSource：为应用提供 i18n 国际化消息访问的功能；
      5. ResourcePatternResolver ： 所 有 ApplicationContext 实现类都实现了类似于 PathMatchingResourcePatternResolver 的功能，可以通过带前缀的 Ant 风格的资源文件路径装载 Spring 的配置文件。
      6. LifeCycle：该接口是 Spring 2.0 加入的，该接口提供了 start()和 stop()两个方法，主要用于控制异步处理过程。在具体使用时，该接口同时被 ApplicationContext 实现及具体 Bean 实现，ApplicationContext 会将 start/stop 的信息传递给容器中所有实现了该接口的 Bean，以达到管理和控制 JMX、任务调度等目的。
      7. ConfigurableApplicationContext 扩展于 ApplicationContext，它新增加了两个主要的方法：refresh()和 close()，让 ApplicationContext 具有启动、刷新和关闭应用上下文的能力。在应用上下文关闭的情况下调用 refresh()即可启动应用上下文，在已经启动的状态下，调用 refresh()则清除缓存并重新装载配置信息，而调用 close()则可关闭应用上下文。

  - **WebApplication** 体系架构

    - WebApplicationContext 是专门为 Web 应用准备的，它允许从相对于 Web 根目录的路径中装载配置文件完成初始化工作。从 WebApplicationContext 中可以获得 ServletContext 的引用，整个 Web 应用上下文对象将作为属性放置到 ServletContext 中，以便 Web 应用环境可以访问 Spring 应用上下文。

    - 在非 Web 应用的环境下，Bean 只有 singleton 和 prototype 两种作用域。WebApplicationContext 为 Bean 添加了三个新的作用域：request、session 和 global session。

    - ```mermaid
      classDiagram
      	AbstractRefreshableConfigApplicationContext <|-- AbstractXmlApplicationContext
      	AbstractRefreshableApplicationContext <|-- AbstractRefreshableConfigApplicationContext
      	AbstractApplicationContext <|-- AbstractRefreshableApplicationContext
      	ConfigurableApplicationContext <|.. AbstractApplicationContext
      	ApplicationContext <|-- ConfigurableApplicationContext
      	AbstractRefreshableWebApplicationContext <|-- XmlWebApplicationContext
      	AbstractRefreshableWebApplicationContext <|-- AnnotationWebApplicationContext
      	AbstractRefreshableWebApplicationContext <|-- GroovyWebApplicationContext
      	AbstractRefreshableConfigApplicationContext <|-- AbstractRefreshableWebApplicationContext
      	ConfigurableWebApplicationContext <|.. AbstractRefreshableWebApplicationContext
      	WebApplicationContext <|-- ConfigurableWebApplicationContext
      	ApplicationContext <|-- WebApplicationContext
      ```

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring IOC 原理
2. [【Spring】Bean的生命周期.md](./来自BlogBackup的面试资料/【Spring】Bean 的生命周期.md)

## bean 作用域

- **Spring Bean** 作用域
- Spring 3 中为 Bean 定义了 5 种作用域，分别为 **singleton（单例）、prototype（原型）、 request、session 和 global session**
- 5 种作用域说明如下：
  - **singleton**：单例模式（多线程下不安全）
    -  singleton：单例模式，Spring IoC 容器中只会存在一个共享的 Bean 实例，无论有多少个 Bean 引用它，始终指向同一对象。该模式在多线程下是不安全的。Singleton 作用域是 Spring 中的缺省作用域，也可以显示的将 Bean 定义为 singleton 模式
  - **prototype:**原型模式每次使用时创建
    - prototype:原型模式，每次通过 Spring 容器获取 prototype 定义的 bean 时，容器都将创建一个新的 Bean 实例，每个 Bean 实例都有自己的属性和状态，而 singleton 全局只有一个对象。根据经验，对有状态的bean使用prototype作用域，而对无状态的bean使用singleton 作用域。
  - **Request**：一次**request** 一个实例
    - request：在一次 Http 请求中，容器会返回该 Bean 的同一实例。而对不同的 Http 请求则会产生新的 Bean，而且该 bean 仅在当前 Http Request 内有效,当前 Http 请求结束，该 bean 实例也将会被销毁。
  - **session**
    - session：在一次 Http Session 中，容器会返回该 Bean 的同一实例。而对不同的 Session 请求则会创建新的实例，该 bean 实例仅在当前 Session 内有效。同 Http 请求相同，每一次 session 请求创建新的实例，而不同的实例之间不共享属性，且实例仅在自己的 session 请求内有效，请求结束，则实例将被销毁。
  - **global Session**
    - global Session：在一个全局的 Http Session 中，容器会返回该 Bean 的同一个实例，仅在使用 portlet context 时有效。

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring IOC 原理

## bean 生命周期

1. 实例化
   - 实例化一个 Bean，也就是我们常说的 new。
   - 在实例化**前后**可以执行 **InstantiationAwareBeanPostProcessor** 的 postProcessBeforeInstantiation()/postProcessAfterInstantiation()
2. 属性赋值
   - Bean 属性赋值（IOC 依赖注入）
     - 按照 Spring 上下文对实例化的 Bean 进行配置，也就是 IOC 注入。
   - **BeanNameAware**
     - 如果这个 Bean 已经实现了 BeanNameAware 接口，会调用它实现的 setBeanName(String) 方法，此处传递的就是 Spring 配置文件中 Bean 的 id 值
   - BeanClassLoaderAware
   - **BeanFactoryAware**
     - 如果这个 Bean 已经实现了 BeanFactoryAware 接口，会调用它实现的 setBeanFactory，setBeanFactory(BeanFactory)传递的是 Spring 工厂自身（可以用这个方式来获取其它 Bean，只需在 Spring 配置文件中配置一个普通的 Bean 就可以）。
   - EnvironmentAware
     - 设置environment在组件使用时调用
   - EmbeddedValueResolverAware
     - 设置StringValueResolver 用来解决嵌入式的值域问题
   - ResourceLoaderAware
   - ApplicationEventPublisherAware
   - MessageSourceAware
   - **ApplicationContextAware**
     - 如果这个 Bean 已经实现了 ApplicationContextAware 接口，会调用setApplicationContext(ApplicationContext)方法，传入 Spring 上下文（同样这个方式也可以实现步骤"BeanFactoryAware 实现" 的内容，但比它更好，因为 ApplicationContext 是 BeanFactory 的子接口，有更多的实现方法）
   - ServletContextAware
     - 运行时设置ServletContext，在普通bean初始化后调用，在InitializingBean.afterPropertiesSet之前调用，在 ApplicationContextAware 之后调用
     - 注：是在WebApplicationContext 运行时
3. 初始化
   - **InitializingBean 接口**
     - afterPropertiesSet() 方法，先于 init-method 执行
   - **init-method**
     - 如果 Bean 在 Spring 配置文件中配置了 init-method 属性会自动调用其配置的初始化方法。
   - **BeanPostProcessor 接口**
     - 如果这个 Bean 关联了 BeanPostProcessor 接口，在初始化**前后**将会调用 postProcessBeforeInitialization()/postProcessAfterInitialization() 方法。
   - 注：以上工作完成以后就可以应用这个 Bean 了，那这个 Bean 是一个 Singleton 的，所以一般情况下我们调用同一个 id 的 Bean 会是在内容地址相同的实例，当然在 Spring 配置文件中也可以配置非Singleton。
4. Destroy 过期自动清理阶段
   - 单例 bean
     - **DisposableBean 接口**
       - 当 Bean 不再需要时，会经过清理阶段，如果 Bean 实现了 **DisposableBean 这个接口**，会调用那个其实现的 destroy()方法；
     - **destroy-method** 自配置清理
       - 最后，如果这个 Bean 的 Spring 配置中配置了 destroy-method 属性，会自动调用其配置的销毁方法。
       -  bean 标签有两个重要的属性（init-method 和 destroy-method）。用它们你可以自己定制初始化和注销方法。它们也有相应的注解（@PostConstruct 和@PreDestroy）。
   - 多例 bean
     - 直接返回 Bean 给用户，剩下的生命周期由用户控制

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring IOC 原理（讲的一坨大便，不推荐看）
2. [一文读懂 Spring Bean 的生命周期 - CSDN](https://blog.csdn.net/riemann_/article/details/118500805)
3. [Bean的生命周期（不要背了记思想）- 腾讯云](https://cloud.tencent.com/developer/article/2184201)

## 循环依赖

三层缓存

- 加入 singletonFactories 三级缓存的前提是**执行了构造器**，所以构造器的循环依赖无法解决。
- Spring是**不支持**基于**多例Bean**，也就是原型模式下的Bean的setter方法的循环依赖。
- Spring默认是**不支持基于代理对象的setter方法的循环依赖**
- @DependsOn注解主要用于指定Bean的实例化顺序，Spring默认是**不支持基于@DependsOn注解的循环依赖**。

三层缓存：

- singletonObjects
- earlySingletonObjects
- singletonFactories

```java
public class DefaultSingletonBeanRegistry extends SimpleAliasRegistry implements SingletonBeanRegistry {
	...
	// 从上至下 分表代表这“三级缓存”
	private final Map<String, Object> singletonObjects = new ConcurrentHashMap<>(256); //一级缓存，存放完全实例化且属性赋值完成的 Bean ，可以直接使用
	private final Map<String, Object> earlySingletonObjects = new HashMap<>(16); // 二级缓存，存放早期 Bean 的引用，尚未装配属性的 Bean
	private final Map<String, ObjectFactory<?>> singletonFactories = new HashMap<>(16); // 三级缓存，存放实例化完成的 Bean 工厂
    ...
	
	/** Names of beans that are currently in creation. */
	// 这个缓存也十分重要：它表示bean创建过程中都会在里面呆着~
	// 它在Bean开始创建时放值，创建完成时会将其移出~
	private final Set<String> singletonsCurrentlyInCreation = Collections.newSetFromMap(new ConcurrentHashMap<>(16));

	/** Names of beans that have already been created at least once. */
	// 当这个Bean被创建完成后，会标记为这个 注意：这里是set集合 不会重复
	// 至少被创建了一次的  都会放进这里~~~~
	private final Set<String> alreadyCreated = Collections.newSetFromMap(new ConcurrentHashMap<>(256));
    ...
	@Override
	@Nullable
	public Object getSingleton(String beanName) {
		return getSingleton(beanName, true);
	}
	@Nullable
	protected Object getSingleton(String beanName, boolean allowEarlyReference) {
		//1.先从一级缓存中获取，获取到直接返回
		Object singletonObject = this.singletonObjects.get(beanName);
		//2.如果获取不到或对象正在创建，就到二级缓存中去获取，获取到直接返回
		if (singletonObject == null && isSingletonCurrentlyInCreation(beanName)) {
			synchronized (this.singletonObjects) {
				singletonObject = this.earlySingletonObjects.get(beanName);
				//3.如果仍获取不到，且允许 singletonFactories(allowEarlyCurrentlyInCreation()）通过 getObject()获取。
				//就到三级缓存中用 getObject() 获取。
				//获取到就从 singletonFactories中移出，且放进 earlySingletonObjects。
				//（即从三级缓存移动到二级缓存）
				if (singletonObject == null && allowEarlyReference) {
					ObjectFactory<?> singletonFactory = this.singletonFactories.get(beanName);
					if (singletonFactory != null) {
						singletonObject = singletonFactory.getObject();
						this.earlySingletonObjects.put(beanName, singletonObject);
						this.singletonFactories.remove(beanName);
					}
				}
			}
		}
		return singletonObject;
	}
	...
	public boolean isSingletonCurrentlyInCreation(String beanName) {
		return this.singletonsCurrentlyInCreation.contains(beanName);
	}
	protected boolean isActuallyInCreation(String beanName) {
		return isSingletonCurrentlyInCreation(beanName);
	}
	...
}
```

getBean() -> doGetBean() -> createBean() -> doCreateBean() -> 返回 Bean

创建 Bean 的三个核心方法

- createBeanInstance()：例化，即调用对象的构造方法实例化对象
- populateBean()：填充属性，主要对 bean 的依赖属性注入（@Autowired）
- initializeBean()：回到一些如initMethod，InitalizingBean等方法



为什么是三层不是两层：考虑 AOP 代理对象

1. 假设去掉三级缓存：在实例化阶段就得执行后置处理器，判断有 AnnotationAwareAspectJAutoProxyCreator 并创建代理对象。
2. 假设去掉二级缓存：如果去掉了二级缓存，则需要直接在 `singletonFactory.getObject()` 阶段初始化完毕，并放到一级缓存中。那有这么一种场景，B 和 C 都依赖了 A。而多次调用 `singletonFactory.getObject()` 返回的代理对象是不同的，就会导致 B 和 C 依赖了不同的 A。



**参考文档**

1. [彻底搞懂Spring之三级缓存解决循环依赖问题 - 知乎](https://zhuanlan.zhihu.com/p/610322151)
2. [万字长文带你彻底吃透Spring循环依赖，堪称全网最全（文末福利）- 腾讯云](https://cloud.tencent.com/developer/article/2367323)

## AOP 应用场景

1. Authentication 权限
2. Caching 缓存
3. Context passing 内容传递
4. Error handling 错误处理
5. Lazy loading 懒加载
6. Debugging 调试
7. logging, tracing, profiling and monitoring 记录跟踪 优化 校准
8. Performance optimization 性能优化
9. Persistence 持久化
10. Resource pooling 资源池
11. Synchronization 同步
12. Transactions 事务

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring AOP 原理

## AOP 核心概念

1. 切面（aspect）：类是对物体特征的抽象，切面就是对横切关注点的抽象
2. 横切关注点：对哪些方法进行拦截，拦截后怎么处理，这些关注点称之为横切关注点。
3. 连接点（joinpoint）：被拦截到的点，因为 Spring 只支持方法类型的连接点，所以在 Spring 中连接点指的就是被拦截到的方法，实际上连接点还可以是字段或者构造器。
4. 切入点（pointcut）：对连接点进行拦截的定义，即匹配连接点的断言。通知和一个切入点表达式关联，并在满足这个切入点的连接点上运行（例如，当执行某个特定名称的方法时），切入点表达式如何和连接点匹配是 AOP 的核心：Spring 缺省使用 AspectJ 切入点语法
5. 通知（advice）：所谓通知指的就是指拦截到连接点之后要执行的代码，通知分为前置、后置、异常、最终、环绕通知五类。许多 AOP 框架（包括 Spring）都是以拦截器做通知模型，并维护一个以连接点为中心的拦截器链
   1. 前置通知（Before advice）：在某连接点之前执行的通知，但这个通知不能阻止连接点之前的执行流程（除非它抛出一个异常）
   2. 后置通知（After returning advice）：在某连接点正常完成后执行的通知：例如，一个方法没有抛出任何异常，正常返回
   3. 异常通知（After throwing advice）：在方法抛出异常退出时执行的通知
   4. 最终通知（After (finally) advice）：当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）
   5. 环绕通知（Around advice）：包围一个连接点的通知，如方法调用。这是最强大的一种通知类型。环绕通知可以在方法调用前后完成自定义的行为。它也会选择是否继续执行连接点或直接返回它自己的返回值或抛出异常来结束执行。
6. 目标对象（Target Object）：代理的目标对象，即被一个或多个切面所通知的对象。也被称作被通知（adviced）对象。既然 Spring AOP 是通过运行时代理实现的，这个对象永远是一个被代理（proxied）对象
7. AOP 代理（AOP Proxy）：AOP 框架创建的对象，用来实现切面契约（例如通知方法执行等等）。在 Spring 中，AOP 代理可以是 JDK 动态代理或者 CGLIB 代理。
8. 织入（weave）：将切面应用到目标对象并导致代理对象创建的过程。把切面连接到其他的应用程序类型或者对象上，并创建一个被通知的对象。这些可以在编译时（例如使用 AspectJ 编译器），类加载时和运行时完成。Spring 和其他纯 Java AOP 框架一样，在运行时完成织入。
9. 引入（introduction）：在不修改代码的前提下，引入可以在运行期为类动态地添加一些方法或字段。用来给一个类型声明额外的方法或属性（也被称为连接类型声明（inter-type declaration））。Spring 允许引入新的接口（以及一个对应的实现）到任何被代理的对象。例如，你可以使用引入来使一个 bean 实现 JsModified 接口，以便简化缓存机制。

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring AOP 原理

## AOP 两种代理方式

- Spring 提供了两种方式来生成代理对象: JDKProxy 和 Cglib
  - 具体使用哪种方式生成由 AopProxyFactory 根据 AdvisedSupport 对象的配置来决定。
  - 默认的策略是如果目标类是接口，则使用 JDK 动态代理技术，否则使用 Cglib 来生成代理。
- **JDK**动态接口代理
  - JDK 动态代理主要涉及到 java.lang.reflect 包中的两个类：Proxy 和 InvocationHandler
  - InvocationHandler是一个接口，通过实现该接口定义横切逻辑，并通过反射机制调用目标类的代码，动态将横切逻辑和业务逻辑编制在一起。
  - Proxy 利用 InvocationHandler 动态创建一个符合某一接口的实例，生成目标类的代理对象。
- **CGLib** 动态代理
  - CGLib 全称为 Code Generation Library，是一个强大的高性能，高质量的代码生成类库，可以在运行期扩展 Java 类与实现 Java 接口，CGLib 封装了 asm，可以再运行期动态生成新的 class。
  - 和 JDK 动态代理相比较：JDK 创建代理有一个限制，就是只能为接口创建代理实例，而对于没有通过接口定义业务方法的类，则可以通过 CGLib 创建动态代理。

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring AOP 原理

## 事务隔离级别

- Isolation 属性一共支持五种事务设置，具体介绍如下：
  - DEFAULT 使用数据库设置的隔离级别 ( 默认 ) ，由 DBA 默认的设置来决定隔离级别 .
  - READ_UNCOMMITTED 会出现脏读、不可重复读、幻读 ( 隔离级别最低，并发性能高 )
  - READ_COMMITTED 会出现不可重复读、幻读问题（锁定正在读取的行）
  - REPEATABLE_READ 会出幻读（锁定所读取的所有行）
  - SERIALIZABLE 保证所有的情况不会发生（锁表）

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十五：数据库中事务的隔离级别有哪些？各自有什么特点？

## 事务传播行为

- **事务传播行为种类**
  - Spring在TransactionDefinition接口中规定了7种类型的事务传播行为，它们规定了事务方法和事务方法发生嵌套调用时事务如何进行传播：
  - **PROPAGATION_REQUIRED**
    - 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。
  - **PROPAGATION_SUPPORTS**
    - 支持当前事务，如果当前没有事务，就以非事务方式执行。
  - **PROPAGATION_MANDATORY**
    - 使用当前的事务，如果当前没有事务，就抛出异常。
  - **PROPAGATION_REQUIRES_NEW**
    - 新建事务，如果当前存在事务，把当前事务挂起。
  - **PROPAGATION_NOT_SUPPORTED**
    - 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。
  - **PROPAGATION_NEVER**
    - 以非事务方式执行，如果当前存在事务，则抛出异常。
  - **PROPAGATION_NESTED**
    - 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十五：数据库中事务的隔离级别有哪些？各自有什么特点？

## 事务的源码实现

- 解析

  - 之前在解析自定义标签时提到，`AOP` 和 `TX` 都使用了自定义标签，**按照我们上一篇** `**AOP**` **的学习，再来一遍解析自定义标签的套路：事务自定义标签。**

    - 定位到 `TxNamespaceHandler` 类的初始化方法：

      ```java
      @Override
      public void init() {
          registerBeanDefinitionParser("advice", new TxAdviceBeanDefinitionParser());
          // 使用 AnnotationDrivenBeanDefinitionParser 解析器，解析 annotation-driven 标签
          registerBeanDefinitionParser("annotation-driven", new AnnotationDrivenBeanDefinitionParser());
          registerBeanDefinitionParser("jta-transaction-manager", new JtaTransactionManagerBeanDefinitionParser());
      }
      ```

    - 根据上面的方法，`Spring` 在初始化时候，如果遇到诸如 `<tx:annotation-driven>` 开头的配置后，将会使用 `AnnotationDrivenBeanDefinitionParser` 解析器的 `parse` 方法进行解析。

  - `Spring` 中的事务默认是以 `AOP` 为基础，如果需要使用 `AspectJ` 的方式进行事务切入，需要在 `mode` 属性中配置:

    ```xml
    <tx:annotation-driven mode="aspectj"/>
    ```

    - 本篇笔记主要围绕着默认实现方式，动态 `AOP` 来学习，如果对于 `AspectJ` 实现感兴趣请查阅更多资料~

  - **注册 InfrastructureAdvisorAutoProxyCreator**

    - 与 `AOP` 一样，在解析时，会创建一个自动创建代理器，在事务 `TX` 模块中，使用的是 `InfrastructureAdvisorAutoProxyCreator`。
    - 首先来看，在默认配置情况下，`AopAutoProxyConfigurer.configureAutoProxyCreator(element, parserContext)` 做了什么操作：
      - **在这里注册了代理类和三个** **`bean`，这三个关键** **`bean`** **支撑了整个事务功能，为了待会更好的理解这三者的关联关系，我们先来回顾下** **`AOP`** **的核心概念：**
        - **Pointcut** 定义一个切点，可以在这个被拦截的方法前后进行切面逻辑。
        - **Advice** 用来定义拦截行为，在这里实现增强的逻辑，它是一个祖先接口 `org.aopalliance.aop.Advice`。还有其它继承接口，例如 `MethodBeforeAdvice` ，特定指方法执行前的增强。
        - **Advisor** 用来封装切面的所有信息，主要是上面两个，它用来充当 `Advice` 和 `Pointcut`的适配器。
      - 回顾完 `AOP` 的概念后，继续来看下这三个关键 `bean`:
        - **TransactionInterceptor**: 实现了 `Advice` 接口，在这里定义了拦截行为。
        - **AnnotationTransactionAttributeSource**：封装了目标方法是否被拦截的逻辑，虽然没有实现 `Pointcut` 接口，但是在后面目标方法判断的时候，实际上还是委托给了 `AnnotationTransactionAttributeSource.getTransactionAttributeSource`，通过适配器模式，返回了 `Pointcut` 切点信息。
        - **TransactionAttributeSourceAdvisor**: 实现了 `Advisor` 接口，包装了上面两个信息。
      - **这三个** **`bean`** **组成的结构与** **`AOP`** **切面环绕实现的结构一致，所以先学习** **`AOP`** **的实现，对事务的了解会有所帮助**
    - 接着看我们的自动创建代理器是如何创建的：
      - `AopNamespaceUtils.registerAutoProxyCreatorIfNecessary(parserContext, element)`
        - **在这一步中，注册了一个** **`beanName`** **是`org.springframework.aop.config.internalAutoProxyCreator` 的** **`bean`：`InfrastructureAdsivorAutoProxyCreator`**
        - **可以看到，它实现了** **`InstantiationAwareBeanPostProcessor`** **这个接口，也就是说在`Spring` 容器中，所有 `bean` 实例化时，`Spring`都会保证调用其`postProcessAfterInitialization`** **方法。**
        - 与上一篇介绍的 `AOP` 代理器一样，在实例化 `bean` 的时候，调用了代理器父类 `AbstractAutoProxyCreator` 的 `postProcessAfterInitialization` 方法
        - 其中关于 `wrapIfNecessory` 方法，在上一篇 `AOP` 中已经详细讲过，这里讲下这个方法做了什么工作：
          1. **找出指定 `bean` 对应的增强器**
          2. **根据找出的增强器创建代理**

  - **匹配标签 match**

    - **在匹配** **`match`** **操作中，区别的是** **`AOP`** **识别的是** **`@Before`** **、`@After`，而我们的事务** **`TX` 识别的是** **`@Transactional`** **标签。**
    - 判断是否是事务方法的入口方法在这：`org.springframework.transaction.interceptor.TransactionAttributeSourcePointcut#matches`

- 运行

  - **事务增强器 TransactionInterceptor**
    - `TransactionInterceptor` 支撑着整个事务功能的架构。跟之前 `AOP` 的 `JDK` 动态代理 分析的一样，`TransactionInterceptor` 拦截器继承于 `MethodInterceptor`，所以我们要从它的关键方法 `invoke()` 看起
  - **事务管理器**
    - 通过该方法，确定要用于给定事务的特定事务管理器 `TransactionAspectSupport#determineTransactionManager`
    - 由于最开始我们在 `XML` 文件中配置过 `transactionManager` 属性，所以该方法在我们例子中将会返回类型是 `DataSourceTransactionManager` 的事务管理器
      - 它实现了 `InitializingBean` 接口，不过只是在 `afterPropertiesSet()` 方法中，简单校验 `dataSource` 是否为空，不细说这个类。
  - **事务开启**
    - `TransactionAspectSupport#createTransactionIfNecessary`
    - 在创建事务方法中，主要完成以下三件事：
      1. **使用 `DelegatingTransactionAttribute` 包装 `txAttr` 实例**
      2. **获取事务：`tm.getTransaction(txAttr)`**
         - 创建对应的事务实例，我们使用的是 `DataSourceTransactionManager` 中的 `doGetTransaction` 方法，创建基于 `JDBC` 的事务实例。
         - **其中在同一个线程中，判断是否有重复的事务，是在`TransactionSynchronizationManager.getResource(obtainDataSource())` 中完成的**
         - **结论：`resources`** **是一个** **`ThreadLocal`** **线程私有对象，每个线程独立存储，所以判断是否存在事务，判断的依据是当前线程、当前数据源(DataSource)中是否存在活跃的事务 - `map.get(actualKey)`。**
      3. **构建事务信息：`prepareTransactionInfo(tm, txAttr, joinpointIdentification, status)`**
    - 核心方法在第二点和第三点，分别摘取核心进行熟悉。
      - 获取 TransactionStatus
      - 获取事务
      - 处理已存在的事务
      - 事务挂起
      - 事务创建

- 小结

  - 在声明式的事务处理中，主要有以下几个处理步骤：
    1. **获取事务的属性**：`tas.getTransactionAttribute(method, targetClass)`
    2. **加载配置中配置的 `TransactionManager`**：`determineTransactionManager(txAttr);`
    3. **不同的事务处理方式使用不同的逻辑**：关于声明式事务和编程式事务，可以查看这篇文章-[Spring编程式和声明式事务实例讲解](https://link.segmentfault.com/?enc=b1jFJN5YhOSu8mhj3wqLtg%3D%3D.8mY0Jeas3Yt6c76rG6jfE3FI4poAtMvGGTbpxLWpDAzKxCGYoRl856A8IizIR0j9q9Q69vgOW7Duo8prLoQ59B38cLnGECwNIh1pGNPls4WIP2sHc%2BSZvczR24aBWTcL)
    4. **在目标方法执行前获取事务并收集事务信**息：`createTransactionIfNecessary(tm, txAttr, joinpointIdentification)`
    5. **执行目标方法**：`invocation.proceed()`
    6. **出现异常，尝试异常处理**：`completeTransactionAfterThrowing(txInfo, ex);`
    7. **提交事务前的事务信息消除**：`cleanupTransactionInfo(txInfo)`
    8. **提交事务**：`commitTransactionAfterReturning(txInfo)`

参考文档：

1. [万字长文，带你从源码认识Spring事务原理，让Spring事务不再是面试噩梦 - 思否](https://segmentfault.com/a/1190000022754620)

## 编程式事务和声明式事务

- Sping 中事务的操作用两种：

  1. 编程式事务（手写代码操作事务）

     - MySQL 中使用事务

       - MySQL 中事务有 3 个重要的操作：开启事务、提交事务、回滚事务，它们对应的操作命令如下

         ```mysql
         -- 开启事务
         start transaction;
         
         -- 业务执⾏
         -- 提交事务
         commit;
         -- 回滚事务
         rollback;
         ```

     - Spring 中编程式事务的实现

       - Spring 中手动操作事务和 MySQL操作事务类似，也是有 3 个重要操作

         1. 开启事务（获取事务）
         2. 提交事务
         3. 回滚事务

       - Spring Boot 内置了两个对象，DataSourceTransactionManager （事务管理器）用来获取事务（开启事务）、提交或回滚事务的，而 TransactionDefinition 是事务的属性，在获取事务的时候需要将 TransactionDefinition 传递进去从而获得一个事务 TransactionStatus 对象

       - ```java
         @RestController
         public class UserController {
             @Autowired
             private UserService userService;
         
             @Autowired
             private DataSourceTransactionManager transactionManager;
         
             @Autowired
             private TransactionDefinition transactionDefinition;
         
             // 在此方法中使用编程式的事务
             @RequestMapping("/add")
             public int add(UserInfo userInfo) {
                 // 非空效验【验证用户名和密码不为空】
                 if(userInfo==null || !StringUtils.hasLength(userInfo.getUsername())
                         || !StringUtils.hasLength(userInfo.getPassword())) {
                     return 0;
                 }
                 // 开启事务（获取事务）
                 TransactionStatus transactionStatus = transactionManager.getTransaction(transactionDefinition);
                 int result = userService.add(userInfo);
                 System.out.println("add 受影响的行数：" + result);
                 // 提交事务
                 transactionManager.commit(transactionStatus);
         //        // 回滚事务
         //        transactionManager.rollback(transactionStatus);
                 return result;
             }
         }
         ```

  2. 声明式事务（利用注解自动开启和提交事务）

     - **声明式事务的实现 @Transactional**

       - 声明式事务的实现，只需要**在方法上添加 @Transactional 注解**就可以实现，无序手动开启事务和提交事务，进入方法时自动开启事务，方法执行完全会自动提交事务，如果中途发生了没有处理的异常会自动回滚事务

     - @Transactional 可以用来修饰方法或类：

       - 修饰方法时，**只能应用到 public 方法上，否则不生效**
       - 修饰类时，说明该注解对该类中所有的 public 方法都生效

     - @Transactional 参数设置

       - | 参数                   | 作用                                                         |
         | ---------------------- | ------------------------------------------------------------ |
         | value                  | 当你配置多个事务管理器时，可以使用该属性指定选择用哪个事务管理器 |
         | transactionManager     | 同上                                                         |
         | propagation            | 事务的传播行为，默认值为 Propagation.REQUIRED事务的传播行为，默认值为 Propagation.REQUIRED |
         | isolation              | 事务的隔离级别，默认值为 Isolation.DEFAULT                   |
         | timeout                | 事务的超时时间，默认值为-1，如果超过该时间限制但事务还没完成，则自动回滚事务 |
         | readOnly               | 指定事务是否为只读事务，默认值为 false，为了忽略那些不需要事务的方法，比如读取数据可以设置 read-only 为 true |
         | rolibackFor            | 用于指定能够触发事务回滚的异常类型，可以指定多个异常类型     |
         | rolibackForClassName   | 同上                                                         |
         | noRolibackFor          | 抛出指定的异常类型，不回滚事务，也可以指定多个异常类型       |
         | noRollbackForClassName | 同上                                                         |

     - @Transactional 异常情况

       - @Transactional 在异常被捕获的情况下，不会进⾏事务⾃动回滚
         - **解决方法1：将异常重新抛出去**
         - **解决方法2：使用代码的方式手动回滚当前事务**
           - 手动回滚事务，在⽅法中使⽤ TransactionAspectSupport.currentTransactionStatus() 可以得到当前的事务，然后设置回滚方法 setRollbackOnly 就可以实现回滚了

     - @Transactional 工作原理

       - @Transactional 是基于 AOP 实现的，AOP 又是使用动态代理来实现的。如果目标对象实现了接口，默认情况下采用 JDK 的动态代理，如果目标对象没有实现接口，会使用 CGLIB 动态代理
       - @Transactional 在开始执行业务之前，通过代理先开启事务，在执行成功之后再提交事务，如果中途遇到异常，则回滚事务

参考文档：

1. [Spring 事务（编程式事务、声明式事务@Transactional、事务隔离级别、事务传播机制）- CSDN](https://blog.csdn.net/m0_58761900/article/details/129032525)

## 多事务管理器下 GC 后自动匹配报错问题

- 简单来说，**解决方法**就是：在多事务管理器（transactionManager）情况下，能使用注解就使用注解 (`<tx:annotation-driven transaction-manager="transactionManager"/>` 和 `@Transactional`），避免使用 `<tx:advice />` 这个配置项。

- 错误现象

  - 由于项目中配置了三个事务管理器(即多个)，导致项目运行一段时间后，配置的事务切面在使用过程中出现以下报错
    - `org.springframework.beans.factory.NoUniqueBeanDefinitionException: No qualifying bean of type [org.springframework.transaction.PlatformTransactionManager] is defined: expected single matching bean but found 3: transactionManager1,transactionManager2,transactionManager`
  - PlatformTransactionManager(事务管理器)按类型匹配匹配到了三个，导致报错。

- 日志堆栈对应的关键Spring源码

  - TransactionAspectSupport.invokeWithinTransaction:

    ```java
    public abstract class TransactionAspectSupport implements BeanFactoryAware, InitializingBean {
        ...
            
    	/**
    	 * General delegate for around-advice-based subclasses, delegating to several other template
    	 * methods on this class. Able to handle {@link CallbackPreferringPlatformTransactionManager}
    	 * as well as regular {@link PlatformTransactionManager} implementations.
    	 * @param method the Method being invoked
    	 * @param targetClass the target class that we're invoking the method on
    	 * @param invocation the callback to use for proceeding with the target invocation
    	 * @return the return value of the method, if any
    	 * @throws Throwable propagated from the target invocation
    	 */
    	protected Object invokeWithinTransaction(Method method, Class<?> targetClass, final InvocationCallback invocation)
    			throws Throwable {
    
    		// If the transaction attribute is null, the method is non-transactional.
            // 这一步在后面的解决方法中会提到，可以注意下getTransactionAttribute方法
    		final TransactionAttribute txAttr = getTransactionAttributeSource().getTransactionAttribute(method, targetClass);
            // 报错的堆栈对应在这：
    		final PlatformTransactionManager tm = determineTransactionManager(txAttr);
    		final String joinpointIdentification = methodIdentification(method, targetClass);
            ...
        }
        
        ...
            
        /**
    	 * Determine the specific transaction manager to use for the given transaction.
    	 */
    	protected PlatformTransactionManager determineTransactionManager(TransactionAttribute txAttr) {
    		// Do not attempt to lookup tx manager if no tx attributes are set
    		if (txAttr == null || this.beanFactory == null) {
    			return getTransactionManager();
    		}
            // 优先根据获得的TransactionAttribute中的Qualifier判断TransactionManager（事务管理器）
    		String qualifier = txAttr.getQualifier();
    		if (StringUtils.hasText(qualifier)) {
    			return determineQualifiedTransactionManager(qualifier);
    		}
            // 否则，根据TransactionAspectSupport中的transactionManagerBeanName属性决定事务管理器
    		else if (StringUtils.hasText(this.transactionManagerBeanName)) {
    			return determineQualifiedTransactionManager(this.transactionManagerBeanName);
    		}
    		else {
                // 还是没有的话，就拿默认的事务管理器
    			PlatformTransactionManager defaultTransactionManager = getTransactionManager();
    			if (defaultTransactionManager == null) {
                    // 默认的事务管理器也为空的话，就按类型（PlatformTransactionManager）去自动匹配了——也就是这里报了错
    				defaultTransactionManager = this.beanFactory.getBean(PlatformTransactionManager.class);
    				this.transactionManagerCache.putIfAbsent(
    						DEFAULT_TRANSACTION_MANAGER_KEY, defaultTransactionManager);
    			}
    			return defaultTransactionManager;
    		}
    	}
        
        /**
    	 * Return the default transaction manager, or {@code null} if unknown.
    	 */
    	public PlatformTransactionManager getTransactionManager() {
    		return this.transactionManagerCache.get(DEFAULT_TRANSACTION_MANAGER_KEY);
    	}
    }
    ```

- 造成错误的出处

  - 使用了类似下面的配置：

    ```xml
    <beans>
        <!--省略其它配置-->
        <tx:advice id="1" transaction-manager="transactionManagerTest">
            <tx:attributes>
                <tx:method name="*" rollback-for="Exception"/>
            </tx:attributes>
        </tx:advice>
    
        <aop:config>
            <aop:pointcut id="interceptorPointCuts"
                          expression="execution(* org.nomadic.test.service.WordTestService.*(..))"/>
            <aop:advisor advice-ref="2"
                         pointcut-ref="interceptorPointCuts"/>
        </aop:config>
    </beans>
    ```

  - 看上去好似我们在tx:advice上配置了transaction-manager，已经按名字指定了事务管理器，但是为什么就是会在运行一段时间后失效，然后不按名字匹配而是去使用类型匹配bean呢？错误的具体原因就埋藏于此处对应的Spring源码中。

- 错误详细原因

  - 在 `tx:advice` 配置中，无法指定 `tx:attribute` 的 `qualifier` ，导致创建 `TransactionInterceptor` 时，对其 `Properties` 没有设置 `qualifier` ，而是直接将引用的类（transaction-manager）写入了其父类 `TransactionAspectSupport` 的属性 `transactionManagerCache` 中。
    - 设置transaction-manager的方法：`TransactionAspectSupport.setTransactionManager(transactionManager)`
    - 存储transactionManager的数据结构：transactionManagerCache：`ConcurrentMap<Object, PlatformTransactionManager> transactionManager = new ConcurrentReferenceHashMap<>(4);`
    - transactionManagerCache具体对应的类型就是ConcurrentReferenceHashMap：`this(initialCapacity, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL, DEFAULT_REFERENCE_TYPE);`
      - (DEFAULT_REFERENCE_TYPE = ReferenceType.SOFT)
    - **而从上述源码可以看出， `transactionManagerCache` 中的值默认都是 `SoftReference` ( 只有 `SOFT` 和 `WEAK` 可选，默认 `SOFT` )，导致当内存不足时，此缓存中的事务配置会被GC回收掉**。
  - 因此在执行事务的时候，会重新获取，由于没有 `qualifier` ，又没有类名（配置的时候只配置了具体的实例对象），所以 Spring 会通过类型获取，即获取 `PlatformTransactionManager` 的实现，而此时，多事务的情况下，就会有多个 `PlatformTransactionManager` 的 `Bean` 即会出现形如下面的异常`org.springframework.beans.factory.NoUniqueBeanDefinitionException: No qualifying bean of type [org.springframework.transaction.PlatformTransactionManager] is defined: expected single matching bean but found 2: transactionManager,transactionManagerTest`。

- 解决方案

  - 方案一：

    - 使用配置bean的方式配置一个 `TransactionInterceptor` ，不使用注解 `tx:advice` 。

    - ```xml
      <beans>
          <!--省略其它配置-->
          <tx:advice id="1" transaction-manager="transactionManagerTest">
              <tx:attributes>
                  <tx:method name="*" rollback-for="Exception"/>
              </tx:attributes>
          </tx:advice>
      
          <!--使用2这个bean替换1这个bean就好了。毕竟注解只是为了配置方便，但是功能有缺陷-->
          <bean id="2" class="org.springframework.transaction.interceptor.TransactionInterceptor">
              <property name="transactionAttributeSource">
                  <bean class="org.springframework.transaction.interceptor.NameMatchTransactionAttributeSource">
                      <property name="nameMap">
                          <map>
                              <entry key="*">
                                  <bean class="org.springframework.transaction.interceptor.RuleBasedTransactionAttribute">
                                      <!--指定事务-->
                                      <property name="qualifier" value="transactionManagerTest"/>
                                      <property name="rollbackRules">
                                          <list >
                                              <bean class="org.springframework.transaction.interceptor.RollbackRuleAttribute">
                                                  <constructor-arg type="java.lang.Class" value="java.lang.Exception"/>
                                              </bean>
                                          </list>
                                      </property>
                                  </bean>
                              </entry>
                          </map>
                      </property>
                  </bean>
              </property>
          </bean>
      </beans>
      ```

  - 方案二

    - 使用全注解的方式，原因如下。

      1. spring 在解析注解 `@Transactional` 的时候，会将 `value` 的值写入到 `qualifier` 中，和上面 `2` 的配置一样。如果不写，则是空的（会使用默认事务）。所以建议 `@Transactional` 一定要带上 `value` 属性，指定具体的事务管理器。
         （具体解析@Transactional中间value属性的源码，在SpringTransactionAnnotationParser中可以找到parseTransactionAnnotation方法）

         - 具体调用链可以参考：

         - ```
           TransactionInterceptor #invoke
           -->TransactionAspectSupport #invokeWithinTransaction （前面日志报错堆栈对应源码中也出现了）
           -->AbstractFallbackTransactionAttributeSource #getTransactionAttribute
           -->AbstractFallbackTransactionAttributeSource #computeTransactionAttribute
           -->AnnotationTransactionAttributeSource #findTransactionAttribute(Method)
           -->AnnotationTransactionAttributeSource #determineTransactionAttribute
           -->SpringTransactionAnnotationParser #parseTransactionAnnotation(AnnotatedElement)
           -->SpringTransactionAnnotationParser #parseTransactionAnnotation(AnnotationAttributes)
           ```

      2. `<tx:annotation-driven transaction-manager="transactionManager"/>` 会将属性 `transaction-manager` 的值设置到 `TransactionInterceptor` 的父类 `TransactionAspectSupport` 的 `transactionManagerBeanName` 属性中。如果不指定 `transaction-manager` ，设置的也是 `transactionManager` 。（意味着这个默认的设置在gc后也是可能会丢失的）

      3. 事务执行的时候，在 `TransactionAspectSupport#determineTransactionManager()` 中，先检查 `qualifier` ，再检查 `transactionManagerBeanName` ，如果这两个都没有值，则获取配置的 `TransactionManager bean` 对象。

      4. 如上面原因说的那样，再结合以上三点，由于这个 `TransactionManager bean` 对象是放在 `SoftReference` 中的，随时可能会丢失。所以通过 `tx:advice` 配置的事务是严重不靠谱的，它会丢失`bean`，在单事务管理下没有问题，但是在多事务管理下，就会出现 `org.springframework.beans.factory.NoUniqueBeanDefinitionException: No qualifying bean of type [org.springframework.transaction.PlatformTransactionManager] is defined: expected single matching bean but found 2: transactionManager,transactionManagerTest` 这样的异常。所以使用全注解的方式，可以保证事务执行的正确性，另外，可以避免 `tx:advice` 在多事务下出现的异常。

      5. `<tx:annotation-driven transaction-manager="transactionManager"/>` 在 xml 中可以配置多个，但是只有第一个会生效，后面的不会创建新的 `TransactionInterceptor` 实例。

- 总结

  - 这个问题总算是解决了，不再出现事务错误的问题了。
  - 谨记一点，能使用注解就使用注解 `<tx:annotation-driven transaction-manager="transactionManager"/>` 和 `@Transactional`，避免使用 `<tx:advice />` 这个配置项。

参考文档：

1. 我自己语雀里面的：项目相关内部知识库 - 《经验案例-广告运营平台pms-多事务管理器下配置事务后匹配报错问题》

## 设计模式

1. **简单工厂(非23种设计模式中的一种)**

   - 实现方式
     - **BeanFactory**。Spring中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的标识来获得Bean对象，但是否是在传入参数后创建还是传入参数前创建这个要根据具体情况来定。
   - 实质
     - 由一个工厂类根据传入的参数，动态决定应该创建哪一个产品类。
   - 实现原理
     - bean 容器的启动阶段
       - 读取bean的xml配置文件,将bean元素分别转换成一个BeanDefinition对象。
       - 然后通过BeanDefinitionRegistry将这些bean注册到beanFactory中，保存在它的一个ConcurrentHashMap中。
       - 将BeanDefinition注册到了beanFactory之后，在这里Spring为我们提供了一个扩展的切口，允许我们通过实现接口BeanFactoryPostProcessor 在此处来插入我们定义的代码。典型的例子就是：PropertyPlaceholderConfigurer，我们一般在配置数据库的dataSource时使用到的占位符的值，就是它注入进去的。
     - 容器中 bean 的实例化阶段
       - 实例化阶段主要是通过反射或者CGLIB对bean进行实例化，在这个阶段Spring又给我们暴露了很多的扩展点：
         - **各种的Aware接口** ，比如 BeanFactoryAware，对于实现了这些Aware接口的bean，在实例化bean时Spring会帮我们注入对应的BeanFactory的实例。
         - **BeanPostProcessor接口** ，实现了BeanPostProcessor接口的bean，在实例化bean时Spring会帮我们调用接口中的方法。
         - **InitializingBean接口** ，实现了InitializingBean接口的bean，在实例化bean时Spring会帮我们调用接口中的方法。
         - **DisposableBean接口** ，实现了BeanPostProcessor接口的bean，在该bean死亡时Spring会帮我们调用接口中的方法。
   - 设计意义
     - **松耦合。** 可以将原来硬编码的依赖，通过Spring这个beanFactory这个工厂来注入依赖，也就是说原来只有依赖方和被依赖方，现在我们引入了第三方——spring这个beanFactory，由它来解决bean之间的依赖问题，达到了松耦合的效果.
     - **bean的额外处理。** 通过Spring接口的暴露，在实例化bean的阶段我们可以进行一些额外的处理，这些额外的处理只需要让bean实现对应的接口即可，那么spring就会在bean的生命周期调用我们实现的接口来处理该bean。[非常重要]

2. **工厂方法**

   - 实现方式

     - **FactoryBean 接口**

   - 实现原理

     - 实现了FactoryBean接口的bean是一类叫做factory的bean。其特点是，spring会在使用getBean()调用获得该bean时，会自动调用该bean的getObject()方法，所以返回的不是factory这个bean，而是这个bean.getOjbect()方法的返回值。

   - 例子

     - 典型的例子有spring与mybatis的结合。

     - ````xml
       <bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean">
       	<property name="dataSource" ref="dataSource"/>
           <property name="configLocation" value="classpath:config/mybatis-config.xml"/>
           <property name="mapperLocations" value="classpath:config/mappers/master/**/*.xml"/>
       </bean>
       ````

     - 我们看上面该bean，因为实现了FactoryBean接口，所以返回的不是 SqlSessionFactoryBean 的实例，而是它的 SqlSessionFactoryBean.getObject() 的返回值。

3. **单例模式**

   - Spring依赖注入Bean实例默认是单例的。

   - Spring的依赖注入（包括lazy-init方式）都是发生在AbstractBeanFactory的getBean里。getBean的doGetBean方法调用getSingleton进行bean的创建。

   - 分析getSingleton()方法

     ```java
     public Object getSingleton(String beanName){
         //参数true设置标识允许早期依赖
         return getSingleton(beanName,true);
     }
     protected Object getSingleton(String beanName, boolean allowEarlyReference) {
         //检查缓存中是否存在实例
         Object singletonObject = this.singletonObjects.get(beanName);
         if (singletonObject == null && isSingletonCurrentlyInCreation(beanName)) {
             //如果为空，则锁定全局变量并进行处理。
             synchronized (this.singletonObjects) {
                 //如果此bean正在加载，则不处理
                 singletonObject = this.earlySingletonObjects.get(beanName);
                 if (singletonObject == null && allowEarlyReference) {
                     //当某些方法需要提前初始化的时候则会调用addSingleFactory 方法将对应的ObjectFactory初始化策略存储在singletonFactories
                     ObjectFactory<?> singletonFactory = this.singletonFactories.get(beanName);
                     if (singletonFactory != null) {
                         //调用预先设定的getObject方法
                         singletonObject = singletonFactory.getObject();
                         //记录在缓存中，earlysingletonObjects和singletonFactories互斥
                         this.earlySingletonObjects.put(beanName, singletonObject);
                         this.singletonFactories.remove(beanName);
                     }
                 }
             }
         }
         return (singletonObject != NULL_OBJECT ? singletonObject : null);
     }
     ```

   - spring依赖注入时，使用了 双重判断加锁 的单例模式

   - **总结**

     - **单例模式定义：** 保证一个类仅有一个实例，并提供一个访问它的全局访问点。
     - **spring对单例的实现：** spring中的单例模式完成了后半句话，即提供了全局的访问点BeanFactory。但没有从构造器级别去控制单例，这是因为spring管理的是任意的java对象。

4. **适配器模式**

   - 实现模式
     - SpringMVC中的适配器 **HandlerAdatper**。
   - 实现原理
     - HandlerAdatper根据Handler规则执行不同的Handler。
   - 实现过程
     - DispatcherServlet根据HandlerMapping返回的handler，向HandlerAdatper发起请求，处理Handler。
     - HandlerAdapter根据规则找到对应的Handler并让其执行，执行完毕后Handler会向HandlerAdapter 返回一个 ModelAndView，最后由 HandlerAdapter 向 DispatchServelet返回一个ModelAndView。
   - 实现意义
     - HandlerAdatper使得Handler的扩展变得容易，只需要增加一个新的Handler和一个对应的HandlerAdapter即可。
     - 因此Spring定义了一个适配接口，使得每一种Controller有一种对应的适配器实现类，让适配器代替controller执行相应的方法。这样在扩展Controller时，只需要增加一个适配器类就完成了SpringMVC的扩展了。

5. **装饰器模式**

   - 实现方式
     - Spring中用到的包装器模式在类名上有两种表现：一种是类名中含有Wrapper，另一种是类名中含有Decorator。
   - 实质
     - 动态地给一个对象添加一些额外的职责。
     - 就增加功能来说，Decorator模式相比生成子类更为灵活。

6. **代理模式**

   - 实现方式
     - AOP底层，就是动态代理模式的实现。
   - 动态代理
     - 在内存中构建的，不需要手动编写代理类
   - 静态代理
     - 需要手工编写代理类，代理类引用被代理对象。
   - 实现原理
     - 切面在应用运行的时刻被织入。一般情况下，在织入切面时，AOP容器会为目标对象创建动态的创建一个代理对象。SpringAOP就是以这种方式织入切面的。
     - 织入：把切面应用到目标对象并创建新的代理对象的过程。

7. **观察者模式**

   - 实现方式
     - spring的事件驱动模型使用的是 观察者模式 ，Spring中Observer模式常用的地方是listener的实现。
   - 具体实现
     - 事件机制的实现需要三个部分,事件源,事件,事件监听器
     - **ApplicationEvent**抽象类[事件]
       - 继承自 jdk 的 EventObject,所有的事件都需要继承 ApplicationEvent,并且通过构造器参数source得到事件源.
       - 该类的实现类ApplicationContextEvent 表示 ApplicaitonContext的容器事件.
     - **ApplicationListener**接口[事件监听器]
       - 继承自jdk的EventListener,所有的监听器都要实现这个接口。
       - 这个接口只有一个onApplicationEvent()方法,该方法接受一个ApplicationEvent或其子类对象作为参数,在方法体中,可以通过不同对Event类的判断来进行相应的处理。
       - 当事件触发时所有的监听器都会收到消息。
     - **ApplicationContext**接口[事件源]
       - ApplicationContext是spring中的全局容器，翻译过来是”应用上下文”。
       - 实现了ApplicationEventPublisher接口。
   - 职责
     - 负责读取bean的配置文档,管理bean的加载,维护bean之间的依赖关系,可以说是负责bean的整个生命周期,再通俗一点就是我们平时所说的IOC容器。
     - ApplicationEventMulticaster抽象类[事件源中publishEvent方法需要调用其方法getApplicationEventMulticaster]
     - 属于事件广播器,它的作用是把Applicationcontext发布的Event广播给所有的监听器.

8. **策略模式**

   - 实现方式
     - Spring框架的资源访问**Resource接口**。该接口提供了更强的资源访问能力，Spring 框架本身大量使用了 Resource 接口来访问底层资源。
   - Resource 接口介绍
     - source 接口是具体资源访问策略的抽象，也是所有资源访问类所实现的接口。
     - Resource 接口主要提供了如下几个方法:
       - **getInputStream()：** 定位并打开资源，返回资源对应的输入流。每次调用都返回新的输入流。调用者必须负责关闭输入流。
       - **exists()：** 返回 Resource 所指向的资源是否存在。
       - **isOpen()：** 返回资源文件是否打开，如果资源文件不能多次读取，每次读取结束应该显式关闭，以防止资源泄漏。
       - **getDescription()：** 返回资源的描述信息，通常用于资源处理出错时输出该信息，通常是全限定文件名或实际 URL。
       - **getFile：** 返回资源对应的 File 对象。
       - **getURL：** 返回资源对应的 URL 对象。
     - 最后两个方法通常无须使用，仅在通过简单方式访问无法实现时，Resource 提供传统的资源访问的功能。
     - Resource 接口本身没有提供访问任何底层资源的实现逻辑，**针对不同的底层资源，Spring 将会提供不同的 Resource 实现类，不同的实现类负责不同的资源访问逻辑。**
     - Spring 为 Resource 接口提供了如下实现类：
       - **UrlResource：** 访问网络资源的实现类。
       - **ClassPathResource：** 访问类加载路径里资源的实现类。
       - **FileSystemResource：** 访问文件系统里资源的实现类。
       - **ServletContextResource：** 访问相对于 ServletContext 路径里的资源的实现类.
       - **InputStreamResource：** 访问输入流资源的实现类。
       - **ByteArrayResource：** 访问字节数组资源的实现类。
     - 这些 Resource 实现类，针对不同的的底层资源，提供了相应的资源访问逻辑，并提供便捷的包装，以利于客户端程序的资源访问。

9. **模板方法模式**

   - 具体实现
     - JDBC的抽象和对Hibernate的集成，都采用了一种理念或者处理方式，那就是模板方法模式与相应的Callback接口相结合。
     - 采用模板方法模式是为了以一种统一而集中的方式来处理资源的获取和释放，以JdbcTempalte为例

参考文档：

1. [Spring 中经典的 9 种设计模式，打死也要记住啊！- 知乎](https://zhuanlan.zhihu.com/p/114244039)

# Spring Boot

## 常用注解

- 组件注册
  - **@Configuration** 这是一个配置类，替代以前的配置文件。配置类本身也是容器中的组件
  - **@SpringBootConfiguration** 和前者没区别，为了可读性。
  - **@Import** 给容器中放指定的类型的组件，组件的名字默认是全类名。
  - **@Bean** 替代以前的 Bean 标签。组件在容器中的名字默认是方法名，可以直接修改注解的值
  - **@Scope** 单例、多例
  - 步骤：
    - @Configuration 编写一个配置类
    - 在配置类中，自定义方法给容器中注册，配合 @Bean
    - 或者**使用 @Import 导入第三方组件**
- 条件注解
  - **@ConditionalOnXxx** 如果注解指定的条件成立，则触发指定行为
    - 放在类级别，如果注解判断生效，则整个配置类才生效
    - 放在方法级别，单独对这个方法进行注解判断
    - **@ConditionalOnClass**：如果类路径中存在这个类，则触发指定行为
    - **@ConditionalOnMissingClass**：如果类路径中不存在这个类，则触发指定行为
    - **@ConditionalOnBean**：如果类路径中存在这个 Bean（组件），则触发指定行为
    - **@ConditionalOnMissingBean**：如果类路径中不存在这个 Bean（组件），则触发指定行为
- 属性绑定
  - **@ConfigurationProperties**：声明组件的属性和配置文件哪些前缀开始项进行绑定
  - **@EnableConfigurationProperties**：快速注册注解，可以导入第三方写好的组件进行属性绑定
  - 将容器任意组件（Bean）的属性值和配置文件的配置项的值进行绑定

    1. 给容器中注册组件（@Component、@Bean）
    2. 使用 @ConfigurationProperties 声明组件和配置文件的哪些配置项进行绑定

参考文档：

1. [《尚硅谷SpringBoot零基础教程》笔记.md](../视频笔记/《尚硅谷SpringBoot零基础教程》笔记.md) P8 ~ 10

## 自动配置

- 流程：

  1. 导入 `starter-web`：导入了 web 开发场景

     1. 场景启动器导入了相关场景的所有依赖：`starter-json`、`starter-tomcat`、`springmvc`
     2. 每个场景启动器都引入了一个 `spring-boot-starter`，核心场景启动器。
     3. 核心场景启动器引入了 `spring-boot-autoconfigure` 包。
     4. `spring-boot-autoconfigure` 里面囊括了所有场景的所有配置。
     5. 只要这个包下的所有类都能生效，那么相当于 SpringBoot 官方写好的整合功能就生效了。
     6. SpringBoot 默认却扫描不到 `spring-boot-autoconfigure` 下写好的所有配置类。（这些配置类给我们做了整合操作）默认只扫描主程序所在的包。

  2. 主程序：`@SpringBootApplication`

     1. **`@SpringBootApplication` 由三个注解组成 `@SpringBootConfiguration`、`@EnableAutoConfiguration`、`@ComponentScan`**

     2. SpringBoot 默认只能扫描自己主程序所在的包及其下面的子包，扫描不到 `spring-boot-autoconfigure` 包中官方写好的配置类

     3. `@EnableAutoConfiguration`：SpringBoot 开启自动配置的核心。

        1. **是由 `@Import(AutoConfigurationImportSelector.class)` 提供功能的**：批量给容器中导入组件
        2. SpringBoot 启动会默认加载 124 个配置类
        3. **这 142 个配置类来自于 `spring-boot-autoconfigure` 下 `META-INF/spring/org.springframework.boot.configure.AutoConfiguration.imports` 文件指定的**

        项目启动的时候利用 @Import 批量导入组件机制把 `autoconfigure` 包下的 142 个 `xxxAutoConfiguration` 类导入进来（自动配置类）

     4. 按需生效：

        并不是这 142 个自动配置类都能生效

        **每个自动配置类，都有条件注解 `@ConditionalOnXxx`，只有条件成立，才能生效**

  3. `xxxAutoConfiguration` 自动配置类

     1. 给容器中使用 @Bean 放一堆组件
     2. **每个自动配置类都可能有这个注解 `@EnableConfigurationProperties(ServerProperties.class)`，用来把配置文件中配的指定前缀的属性值封装到 `xxxProperties` 属性类中**
     3. 以 Tomcat 为例：把服务器的所有配置都是以 `server` 开头的。配置都封装到属性类中
     4. 给容器中放的所有组件的一些核心参数，都来自于 `xxxProperties`。`xxxProperties` 都是和配置文件绑定。

     只需要改配置文件的值，核心组件的底层参数都能修改

  4. 写业务，全程无需关系各种整合（底层这些整合好了，而且也生效了）

- 核心流程：

  1. 导入 `starter`，就会导入 `autoconfigure` 包
  2. `autoconfigure` 包里面有一个文件 `META-INF/spring/org.springframework.boot.configure.AutoConfiguration.imports`, 里面指定了所有启动要加载的自动配置类
  3. @EnableAutoConfiguration 会自动地把上面文件里面写的所有**自动配置类都导入进来**。**xxxAutoConfiguration 是有条件注解进行按需加载**
  4. `xxxAutoConfiguration` 给容器中导入一堆组件，组件都是从 `xxxProperties` 中提取属性值
  5. `xxxProperties` 又是和**配置文件**进行了绑定

- 效果：导入 `starter`、修改配置文件，就能修改底层行为。

参考文档：

1. [《尚硅谷SpringBoot零基础教程》笔记.md](../视频笔记/《尚硅谷SpringBoot零基础教程》笔记.md) P11

## 自动配置与 SPI

- 应用关注的三大核心：场景、配置、组件
- 自动配置流程
  1. 导入 `starter`
  2. 依赖导入 `autoconfigure`
  3. 寻找类路径下 `META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports` 文件
  4. 加载所有自动配置类 `xxxAutoConfiguration`
     1. 给容器中配置功能组件
     2. 组件参数绑定到属性类中，`xxxProperties`
     3. 属性类和配置文件前缀项绑定
     4. `@Conditional` 派生的条件注解进行判断是否组件生效
  5. 效果：
     1. 修改配置文件，修改底层参数
     2. 所有场景自动配置好直接使用
     3. 可以注入 SpringBoot 配置好的组件随时使用
- SPI 机制
  - ChatGPT 3.5 回答：

    - Java 中的 SPI（Service Provider Interface）是一种软件设计模式，用于在应用程序中动态地发现和加载组件。SPI 的思想是，定义一个接口或抽象类，然后通过在 classpath 中定义实现该接口的类来实现对组件的动态发现和加载。
    - SPI 的主要目的是解决在应用程序中使用可插拔组件的问题。例如，一个应用程序可能需要使用不同的日志框架或数据库连接池，但是这些组件的选择可能取决于运行时的条件。通过使用 SPI，应用程序可以在运行时发现并加载适当的组件，而无需在代码中硬编码这些组件的实现类。
    - 在 Java 中，SPI 的实现方式是通过在 META-INF/services 目录下创建一个以服务接口全限定名为名字的文件，文件中包含实现该服务接口的类的全限定名。当应用程序启动时，Java 的 SPI 机制会自动扫描 classpath 中的这些文件，并根据文件中指定的类名来加载实现类。
    - 通过使用 SPI，应用程序可以实现更灵活、可扩展的架构，同时也可以避免硬编码依赖关系和增加代码的可维护性。
- 功能开关
  - 自动配置：全部都配置好，什么都不用管。自动批量导入
    - 项目一启动，SPI 文件中指定的所有都加载
  - `@EnableXxx`：手动控制哪些功能的开启；手动导入
    - 开启 xxx 功能
    - 都是利用 @Import 把此功能要用的组件导入进去

参考文档：

1. [《尚硅谷SpringBoot零基础教程》笔记.md](../视频笔记/《尚硅谷SpringBoot零基础教程》笔记.md) P62

## @SpringBootApplication 源码解析

- 源码上的注解

  - ```java
    @SpringBootConfiguration
    @EnableAutoConfiguration
    @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class)})
    ```

- **@SpringBootConfiguration**

  - 就是 @Configuration，容器中的组件、配置类， Spring IOC 启动就会加载创建这个类对象

- **@EnableAutoConfiguration**：开启自动配置

  - 上面有 @AutoConfigurationPackage 和 @Import(AutoConfigurationImportSelector.class)

    - **@AutoConfigurationPackage**：扫描主程序包；加载自己的组件

      - 上面有 `@Import(AutoConfigurationPackages.Registrar.class)`，利用它想要给容器中导入组件。
      - 把主程序所在的包的所有组件导入进来。
      - 为什么 SpringBoot 默认只扫描主程序所在的包及其子包

    - **@Import(AutoConfigurationImportSelector.class)**：加载所有自动配置类；加载 starter 导入的组件

      - ```java
        protected List<String> getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttribute attribute) {
        	List<String> configurations = ImportCandidates.load(AutoConfiguration.class, getBeanClassLoader()).getCandidates();
            // ...
        }
        ```

      - 扫描 SPI 文件：`META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports`

- **@ComponentScan**

  - 组件扫描：排除一些组件
  - 排除前面已经扫描进来的配置类和自动配置类

参考文档：

1. [《尚硅谷SpringBoot零基础教程》笔记.md](../视频笔记/《尚硅谷SpringBoot零基础教程》笔记.md) P63

## properties 和 yaml 配置文件的使用：复杂对象表示

- properties

  - ```properties
    person.name=张三
    person.age=18
    person.birthDay=2010/10/12 12:12:12
    person.like=true
    person.child.name=李四
    person.child.age=12
    person.child.birthDay=2018/10/12
    person.child.text[0]=abc
    person.child.text[1]=def
    person.dogs[0].name=小黑
    person.dogs[0].age=3
    person.dogs[1].name=小白
    person.dogs[1].age=2
    person.cats.c1.name=小蓝
    person.cats.c1.age=3
    person.cats.c2.name=小灰
    person.cats.c2.age=2
    ```

- yaml

  - ```yaml
    person:
    	name: 张三
    	age: 18
    	birth-day: 2010/10/12 12:12:12
    	like: true
    	child:
    		name: 李四
    		age: 20
    		birth-day: 2018/10/12
    		text: ["abc", "def"]
    	dogs:
    		- name: 小黑
    		  age: 3
    		- name: 小白
    		  age: 2
    	cats:
    		c1:
    			name: 小蓝
    			age: 3
    		c2: {name: 小灰, age: 2} # 对象也可以用 {} 表示
    ```

  - birthDay 推荐写为 birth-day

  - 文本

    - 单引号不会转义【\n 则为普通字符串显示】
    - 双引号会转义【\n 会显示为换行符】

  - 大文本

    - `|` 开头，大文本写在下层，保留文本格式，换行符正确显示
    - `>` 开头，大文本写在下层，没有缩进则折叠换行符，有缩进就保留原格式

  - 多文档合并

    - 使用 `---` 可以把多个 yaml 文档合并在一个文档中。每个文档区依然认为内容独立

参考文档：

1. [《尚硅谷SpringBoot零基础教程》笔记.md](../视频笔记/《尚硅谷SpringBoot零基础教程》笔记.md) P14 ~ 16

## 自定义 starter

- 过程

  - 创建自定义 starter 项目，引入 `spring-boot-starter` 基础依赖
  - 编写模块功能，引入模块所有需要的依赖。编写 `xxxAutoConfiguration` 自动配置类
  - 编写配置文件 `META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports` 指定启动需要加载的自动配置
  - 其他项目引入即可使用

- **1、业务代码**

  - 配置文件自动补全，需要导入以下依赖，重启项目：

    ```xml
    <dependency>
    	<groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-configuration-processor</artifactId>
        <optional>true</optional>
    </dependency>
    ```

- **2、基本抽取**

  - 创建 starter 项目，把公共代码需要的所有依赖导入
  - 把公共代码复制进来
  - **自己写一个 `XxxAutoConfiguration`，给容器中导入这个场景需要的所有组件（`@Import`）**
    - 为什么这些组件默认不会扫描进去？
    - starter 所在的包和引入它的项目的主程序所在的包不是父子层级
  - **别人引用这个 `starter`，直接导入这个 `XxxAutoConfiguration`，就能把这个场景的组件导入进来**
  - 功能生效
  - 测试编写配置文件

- **3、使用 Enable 机制**

  - ```java
    @Retention(RetentionPolicy.RUNTIME)
    @Target(ElementType.TYPE)
    @Documented
    @Import(RobotAutoConfiguration.class)
    public @interface EnableRobot {
        
    }
    ```

    别人引入 `starter` 需要使用 `@EnableRobot` 开启功能

- **4、完全自动配置**

  - 依赖 SpringBoot 的 SPI 机制
  - **`META-INF/spring/org.springframework.boot.configure.AutoConfiguration.imports` 文件中编写好我们自动配置类的全类名即可**
  - 项目启动，自动加载我们的自动配置类

参考文档：

1. [《尚硅谷SpringBoot零基础教程》笔记.md](../视频笔记/《尚硅谷SpringBoot零基础教程》笔记.md) P65

# Spring MVC

## 工作原理

- 图解

  - ```mermaid
    graph TB
    u[用户访问浏览器] --1 request请求url--> ds[前端控制器 DispatcherServlet 接收用户请求和响应]
    ds --2 请求查找 handler--> hm[处理器映射器 HandlerMapping]
    hm --3 返回一个执行链 HandlerExecutionChain--> ds
    ds --4 请求适配器执行 Handler--> ha[处理器适配器 HandlerAdapter 去执行 Handler]
    ha --5 执行--> h[Handler 处理器: 平常叫做 controller]
    h --6 返回 ModelAndView--> ha
    ha --7 返回 ModelAndView--> ds
    ds --8 请求进行视图解析--> vr[视图解析器 View Resolver]
    vr --9 返回 View--> ds
    ds --10 视图渲染: 将模型数据填充到 request 域中--> v[View 视图: jsp freemarker excel pdf]
    ds --11 response 响应--> u
    ```

- 流程说明（重要）

  1. 客户端（浏览器）发送请求，直接请求到 `DispatchServlet`
  2. `DispatcherServlet` 根据请求信息调用 `HandlerMapping`，解析请求对应的 `Handler`。
  3. 解析到对应的 `Handler`（也就是我们平常说的 `Controller` 控制器）后，开始由 `HandlerAdapter` 适配器处理。
  4. `HandlerAdapter` 会根据 `Handler` 来调用真正的处理器开处理请求，并处理相应的业务逻辑。
  5. 处理器处理完业务后，会返回一个 `ModelAndView` 对象，`Model` 是返回的数据对象，`View` 是个逻辑上的 View。
  6. `ViewResolver` 会根据逻辑 `View` 查找实际的 `View`。
  7. `DispatcherServlet` 把返回的 `Model` 传给 `View`（视图渲染）。
  8. 把 `View` 返回给请求者（浏览器）

参考文档：

1. 《v3.0-JavaGuide面试突击版.pdf》5.1.6 Spring MVC - SpringMVC 工作原理了解吗？

# MyBatis

## 分页、分页插件原理

- Mybatis 使用 **RowBounds 对象**进行分页，**它是针对 ResultSet 结果集执行的内存分页，而非物理分页**
  - 可以在 sql 内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。
- 分页插件的基本原理是使用 Mybatis 提供的插件接口，实现自定义插件，**在插件的拦截方法内拦截待执行的 sql，然后重写 sql**，根据 dialect 方言，添加对应的物理分页语句和物理分页参数。
  - 举例： `select _ from student`，拦截 sql 后重写为： `select t._ from （select \* from student）t limit 0，10`
  - **MyBatis 插件运行原理**
    - Mybatis 仅可以编写**针对 `ParameterHandler`、`ResultSetHandler`、`StatementHandler`、`Executor` 这 4 种接口**的插件，Mybatis 使用 **JDK 的动态代理**，为需要拦截的接口生成代理对象以实现接口方法拦截功能
    - 每当执行这 4 种接口对象的方法时，就会进入拦截方法，具体就是 `InvocationHandler` 的 `invoke()` 方法，当然，只会拦截那些你指定需要拦截的方法。
    - 实现 Mybatis 的 Interceptor 接口并复写 `intercept()` 方法，然后再给插件编写注解，指定要拦截哪个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。

参考文档：

1. 《v3.0-JavaGuide面试突击版.pdf》5.2.4 Mybatis 是如何分页的？分页插件的原理是什么？、5.2.5 简述 Mybatis 的插件运行原理，以及如何编写一个插件

## 一级缓存（SqlSession 级别）、二级缓存（mapper 级别）原理

- 一级缓存（SqlSession 级别）
  - **MyBatis 默认开启一级缓存**
  - 第一次发出一个查询 sql，sql 查询结果写入 SqlSession 的一级缓存中，缓存使用的数据结构是一个 map。
    - key：MapperID+offset+limit+Sql+所有的入参
    - value：用户信息
  - 同一个 SqlSession 再次发出相同的 sql，就从缓存中取出数据。如果两次中间出现 commit 操作（修改、添加、删除），本 SqlSession 中的一级缓存区域全部清空，下次再去缓存中查询不到所以要从数据库查询，从数据库查询到再写入缓存。
- 二级缓存原理（mapper 级别）
  - **MyBatis 默认不开启二级缓存**，需要在MyBtais全局配置文件中进行setting配置开启二级缓存。
  - **二级缓存的作用域是 SqlSessionFactory 级别，整个应用程序只有一个。**
  - 二级缓存的范围是 mapper 级别（mapper 同一个命名空间），mapper 以命名空间为单位创建缓存数据结构，结构是 map。
    - key：MapperID+offset+limit+Sql+所有的入参
  - mybatis 的二级缓存是通过 CacheExecutor 实现的。
    - CacheExecutor 其实是 Executor 的代理对象。所有的查询操作，在 CacheExecutor 中都会先匹配缓存中是否存在，不存在则查询数据库。
  - 具体使用需要配置：
    1. Mybatis 全局配置中启用二级缓存配置
    2. 在对应的 Mapper.xml 中配置 cache 节点
    3. 在对应的 select 查询节点中添加 useCache=true

参考文档：

1. 《Java岗面试核心MCA版.pdf》Mybatis 的一级缓存原理（sqlsession级别）
2. [MyBatis缓存看这一篇就够了（一级缓存+二级缓存+缓存失效+缓存配置+工作模式+测试）- CSDN](https://blog.csdn.net/Lotus_dong/article/details/116334317)

## 解析和运行原理

- MyBatis 与数据库会话步骤
  1. 创建SqlSessionFactory
  2. 通过 SqlSessionFactory 创建 SqlSession
  3. 通过sqlsession执行数据库操作
  4. 调用session.commit()提交事务
  5. 调用session.close()关闭会话
- 工作原理
  1. **读取 MyBatis 配置文件**：**mybatis-config.xml** 为 MyBatis 的全局配置文件，配置了 MyBatis 的运行环境等信息，例如数据库连接信息。
  2. **加载映射文件**。映射文件即 SQL 映射文件，该文件中配置了操作数据库的 SQL 语句， 需要在 MyBatis 配置文件 mybatis-config.xml 中加载。mybatis-config.xml 文件可以加 载多个映射文件，每个文件对应数据库中的一张表。
  3. **构造会话工厂**：通过 MyBatis 的环境等配置信息构建会话工厂 **SqlSessionFactory**。
  4. **创建会话对象**：由会话工厂创建 **SqlSession** 对象，该对象中包含了执行 SQL 语句的所有方法。
  5. **Executor 执行器**：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。
  6. **MappedStatement 对象**：在 Executor 接口的执行方法中有一个 MappedStatement 类型的参数，该参数是对映射信息的封装，用于存储要映射的 SQL 语句的 id、参数等信息。
  7. **输入参数映射**：输入参数类型可以是 Map、List 等集合类型，也可以是基本数据类型和 POJO 类型。输入参数映射过程类似于 JDBC 对 preparedStatement 对象设置参数的过 程。
  8. **输出结果映射**：输出结果类型可以是 Map、 List 等集合类型，也可以是基本数据类型 和 POJO 类型。输出结果映射过程类似于 JDBC 对结果集的解析过程。
- 功能架构
  - 我们把 Mybatis 的功能架构分为三层
    - **API接口层**：提供给外部使用的接口API，开发人员通过这些本地API来操纵数据库。接口层 一接收到调用请求就会调用数据处理层来完成具体的数据处理。
    - **数据处理层**：负责具体的SQL查找、SQL解析、SQL执行和执行结果映射处理等。它主要的 目的是根据调用的请求完成一次数据库操作。
    - **基础支撑层**：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这 些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的 支撑。
  - MyBatis的框架架构设计是怎么样的
    - MyBatis的初始化，会从mybatis-config.xml配置文件，解析构造成 Configuration这个类
      1. 加载配置：配置来源于两个地方，一处是配置文件，一处是Java代码的注解，将SQL的 配置信息加载成为一个个MappedStatement对象（包括了传入参数映射配置、执行的SQL 语句、结果映射配置），存储在内存中。
      2. SQL解析：当API接口层接收到调用请求时，会接收到传入SQL的ID和传入对象（可以是 Map、JavaBean或者基本数据类型），Mybatis 会根据SQL的ID找到对应的 MappedStatement，然后根据传入参数对象对MappedStatement进行解析，解析后可以 得到最终要执行的 SQL语句和参数。
      3. SQL执行：将最终得到的SQL和参数拿到数据库进行执行，得到操作数据库的结果。
      4. 结果映射：将操作数据库的结果按照映射的配置进行转换，可以转换成HashMap、 JavaBean或者基本数据类型，并将最终结果返回。

参考文档：

1. 《Java岗面试核心MCA版.pdf》MyBatis 的解析和运行原理

# MySQL

## 实现无数据插入，有数据则更新

- 向MySQL中插入数据，存在就更新，不存在则插入。本质上数据表中还是**需要存在唯一键，也就是唯一索引的**。往往在面试中，面试官都会默许存在这些前置条件。

- 这里，有两种方法可以实现这个效果。

  - 一种方法是结合 INSERT 语句和 **ON DUPLICATE KEY UPDATE 语句**实现

    - 如果指定了ON DUPLICATE KEY UPDATE，并且插入行后会导致在一个UNIQUE索引或PRIMARY KEY中出现重复值，则执行UPDATE。

    - ```mysql
      INSERT INTO table (a,b,c) VALUES (1,2,3) ON DUPLICATE KEY UPDATE c=c+1;
      ```

    - 如果行作为新记录被插入，则受影响行的值为1；如果原有的记录被更新，则受影响行的值为2。

  - 另一种方法是通过 REPLACE 语句实现。

    - 使用 REPLACE 的最大好处就是可以将 DELETE 和 INSERT 合二为一，形成一个原子操作。

    - 这样就可以不必考虑在同时使用 DELETE 和 INSERT 时添加事务等复杂操作了。在使用 REPLACE 时，表中必须有唯一索引，而且这个索引所在的字段不能允许空值，否则 REPLACE 就和 INSERT 完全一样的。

    - 语法和 INSERT 非常的相似，如下面的REPLACE语句是插入或更新一条记录

    - ```mysql
      REPLACE INTO users (id,name,age) VALUES(1, 'binghe', 18);
      ```

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十四：MySQL 如何实现无数据插入，有数据更新？

## B Tree

- B 树的英文是 Balance Tree，也就是多路平衡查找树。简写为 B-Tree（注意横杠表示这两个单词连起来的意思，不是减号）。它的高度远小于平衡二叉树的高度。
- B 树作为多路平衡查找树，它的每一个节点最多可以包括 M 个子节点，M 称为 B 树的阶。每个磁盘块中包括了关键字和子节点的指针。如果一个磁盘块中包括了 x 个关键字，那么指针数就是 x + 1。
- 一个 M 阶的 B 树（M > 2）有以下的特性：
  1. 根节点的儿子数的范围是 [2, M]
  2. **每个中间节点包含 k - 1 个关键字和 k 个孩子**，孩子的数量 = 关键字的数量 + 1，k 的取值范围为 [ceil(M/2), M]。
  3. 叶子节点包括 k-1 个关键字（叶子节点没有孩子），k 的取值范围为 [ceil(M/2), M]。
  4. 假设中间节点的关键字为: Key[1], Key[2], ..., Key[k-1], 且关键字按照升序排序，即 Key[i] < Key[i + 1]。此时 k-1 个关键字相当于划分了 k 个范围，也就是对应着 k 个指针，即为：P[1], P[2], ..., P[k]，其中 P[1] 指向关键字小于 Key[1] 的子树，P[i] 指向关键字属于(Key[i-1], Key[i]) 的子树，P[k] 指向关键字大于 Key[k-1] 的子树。
  5. **所有叶子节点位于同一层**。

- 小结：
  1. B 树在插入和删除时如果导致树不平衡，就通过自动调节节点位置来保持树的自平衡
  2. 关键字集合分布在整个树中，即叶子节点和非叶子节点都存放数据。搜索有可能在非叶子节点结束。
  3. 其搜索性能等价于在关键字全集内做一次二分查找

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P120

## B+ Tree

- B+ 树也是一种多路搜索树，基于 B 树做出了改进，主流的 DBMS 都支持 B+ 树的索引方式，比如 MySQL。相比于 B-Tree，B+Tree 适合文件索引系统。

- **B+ 树和 B 树的差异**在于以下几点：

  1. **有 k 个孩子的节点就有 k 个关键字**。也就是孩子数量 = 关键字数，而 B 树中，孩子数量 = 关键字数 + 1。
  2. 非叶子节点的关键字也会同时存在于子节点中，并且是在子节点中所有关键字的最大（或最小）。
  3. 非叶子节点仅用于索引，不保存数据记录，**跟记录有关的信息都放在叶子节点中**。而 B 树中，非叶子节点既保存索引，也保存数据记录。
  4. 所有关键字都在叶子节点出现，**叶子节点构成一个有序链表**，而且叶子节点本身按照关键字的大小从小到大顺序链接。

- B+ 树和 B 树有个根本的差异在于，B+ 树中间节点并不直接存储数据。这样的好处都有什么呢？

  - 首先，**B+ 树查询效率更稳定**，因为 B+ 树每次只有访问到叶子节点才能找到对应的数据，而在 B 树中，非叶子节点也会存储数据，这样就会造成查询效率不稳定的情况，有时候访问到了非叶子节点就可以找到关键字，而有时需要访问到叶子节点才能找到关键字。
  - 其次，**B+ 树的查询效率更高**。这是因为通常 B+ 树比 B 树更矮胖（阶数更大，深度更低），查询所需要的磁盘 I/O 也会更少。同样的磁盘页大小，B+ 树可以存储更多的节点关键字。
  - 不仅是对单个关键字的查询上，**在查询范围上，B+ 树的效率也比 B 树高**。这是因为所有关键字都出现在 B+ 树的叶子节点中，叶子节点之间会有指针，数据又是递增的，这使得我们范围查找可以通过指针连接查找。而在 B 树中则需要通过中序遍历才能完成查询范围的查找，效率要低很多。

- **思考题：为了减少 IO，索引树会一次性加载吗？**

  1. 数据库索引是存储在磁盘上的，如果数据量很大，必然导致索引的大小也会很大，超过几个 G
  2. 当我们利用索引查询的时候，是不可能将全部几个 G 的索引都加载进内存的，我们能做的只能是：逐一加载每个磁盘页，因为磁盘页对应着索引树的节点。

- **思考题：B+ 树的存储能力如何？为何说一般查找行记录，最多只需 1~3 次磁盘 IO**

  - InnoDB 存储引擎中页的大小为 16KB，一般表的主键类型为 INT（占用 4 个字节）或 BIGINT（占用 8 个字节），指针类型也一般为 4 或 8 个字节，也就是说一个页（B+Tree 中的一个节点）中大概存储 16KB/(8B+8B)=1K 个键值（因为是估值。为方便计算，这里 K 取值为 10^3。也就是说一个深度为 3 的 B+Tree 索引可以维护 10^3 * 10^3 * 10^3 = 10 亿条记录。这里假定一个数据页也存储 10^3 条行记录数据了）
  - 实际情况中每个节点可能不能填充满，因此在数据库中，B+Tree 的高度一般都在 2~4 层。MySQL 的 InnoDB 存储引擎在设计时是将根节点常驻内存的，也就是说查找某个键值的行记录时最多只需要 1 ~ 3 次磁盘 I/O 操作。

- **思考题：为什么说 B+ 树比 B树更适合实际应用中操作系统的文件索引和数据库索引**

  1. B+ 树的磁盘读写代价更低
     - B+ 树的内部节点并没有指向关键字具体信息的指针。因此其内部节点相对于 B 树更小。如果把同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说 IO 读写次数也就降低了。

  2. B+ 树的查询效率更加稳定

     - 由于非终结点并不是最终指向文件内容的节点，而只是叶子节点中关键字的索引。所以任何关键字的查找必须走一条从根节点到叶子节点的路。所有关键字查询的路径长度相同，导致每个数据的查询效率相当。

- **思考题：Hash 索引和 B+ 树索引的区别**

  我们之前讲到过 B+ 树索引的结构，Hash 索引结构和 B+ 树的不同，因此在索引使用上也会有差别。

  1. Hash 索引不能进行范围查询
  2. Hash 索引不支持联合索引的最左侧原则（即联合索引的部分索引无法使用），而 B+ 树可以。
  3. Hash 索引不支持 ORDER BY 排序
  4. InnoDB 不支持哈希索引

- **思考题：Hash 索引与 B+ 树索引是在建索引的时候手动指定吗？**

  - 针对 InnoDB 和 MyISAM 存储引擎，都会默认采用 B+ 树索引，无法使用 Hash 索引。InnoDB 提供的自适应 Hash 是不需要手动指定的。如果是 Memory/Heap 和 NDB 存储引擎，是可以进行选择 Hash 索引的。

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P120

## 索引类型

- 索引按照**物理实现**方式，索引可以分为 2 种：**聚簇（聚集）**和**非聚簇（非聚集）**索引。
  - 我们也把非聚簇索引称为**二级索引**或者**辅助索引**。
  - 以多个列的大小为排序规则建立的 B+ 树称为**联合索引**，本质上也是一个二级索引。
- 按照**功能逻辑**区分，MySQL目前主要有以下索引类型：
  - **主键索引**
    - 数据列不允许重复，不允许为 NULL，一个表只能有一个主键。
    - `ALTER TABLE table_name ADD PRIMARY KEY (column);`
  - **普通索引**
    - MySQL中基本索引类型，没有什么限制，允许在定义索引的列中插入重复值和NULL值。一个表允许多个列创建普通索引。
    - `ALTER TABLE table_name ADD INDEX index_name (column);`
  - **唯一索引**
    - 索引列中的值必须是唯一的，但是允许NULL值。建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。一个表允许多个列创建唯一索引。
    - `ALTER TABLE table_name ADD UNIQUE (column);`
  - **全文索引**
    - 主要是为了快速检索大文本数据中的关键字的信息。字段长度比较大时，如果创建普通索引，在进行like模糊查询时效率比较低，这时可以创建全文索引，基于倒排索引，类似于搜索引擎。MyISAM存储引擎支持全文索引，InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。
    - `ALTER TABLE table_name ADD FULLTEXT (column);`
  - **前缀索引**
    - 在文本类型如BLOB、TEXT或者很长的VARCHAR列上创建索引时，可以使用前缀索引，数据量相比普通索引更小，可以指定索引列的长度，但是数值类型不能指定。
    - `ALTER TABLE table_name ADD KEY(column_name(prefix_length));`
  - **组合索引**
    - 指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀原则。
    - 主键索引、普通索引、唯一索引等都可以使用多个字段形成组合索引。例如，`ALTER TABLE table_name ADD INDEX index_name ( column1, column2, column3 );`
  - **空间索引**
    - MySQL在5.7之后的版本支持了空间索引，而且支持OpenGIS几何数据模型。MySQL在空间索引这方面遵循OpenGIS几何数据模型规则。

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P118
2. [MySQL索引的概念以及七种索引类型介绍 - CSDN](https://blog.csdn.net/weixin_43767015/article/details/119109385)

## 索引失效

- **最佳左前缀法则**
  - 在 MySQL 建立联合索引时会遵守最佳左前缀匹配原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。
  - 如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。
  - 对于多列索引，**过滤条件要使用索引必须按照索引建立时的顺序，依次满足，一旦跳过某个字段，索引后面的字段都无法被使用**。如果查询条件中没有使用这些字段中第 1 个字段时，多列（或联合）索引不会被使用。
- **主键**插入顺序
  - 对于一个使用 `InnoDB` 存储引擎的表来说，在我们没有显式的创建索引时，表中的数据实际上都是存储在聚簇索引的叶子节点的。
  - 而记录又是存储在数据页中的，数据页和记录又是按照记录主键值从小到大的顺序进行排序，所以如果我们插入的记录的主键值是依次增大的话，那我们每插满一个数据页就换到下一个数据页继续插，而如果我们插入的主键值忽大忽小的话，就比较麻烦了。
  - 所以我们建议：让主键具有 `AUTO_INCREMENT`，让存储引擎自己为表生成主键，而不是我们手动插入。
    - 数据页已经满了，再插进来怎么办呢？我们需要把当前页面分裂成两个页面，把本页中的一些记录移动到新创建的这个页中。**页面分裂**和**记录移位**意味着什么？意味着：性能损耗！所以如果我们想尽量避免这样无谓的性能损耗，最好让插入的记录的主键值依次递增，这样就不会发生这样的性能损耗了。
- **计算、函数、类型转换（自动或手动）**导致索引失效
- **类型转换**导致索引失效
- **范围条件**右边的列索引失效
  - 范围右边的列不能使用。比如：（<）（<=）（>）（>=）和 between 等
  - 将范围查询条件放置语句最后
- **不等于**（!= 或者 <>）索引失效
- is null 可以使用索引，**is not null** 无法使用索引
- **like 以通配符 % 开头**索引失效
- **OR 前后存在非索引的列**，索引失效
  - 在 WHERE 子句中，如果在 OR 前的条件列进行了索引，而在 OR 后的条件列没有进行索引，那么索引会失效。也就是说，**OR 前后的两个条件中的列都是索引时，查询中才使用索引**。
  - 前后都是索引列时可以使用索引合并：index_merge
- 数据库和表的字符集统一使用 utf8mb4
  - 统一使用 utf8mb4（5.5.3 版本以上支持）兼容性更好，统一字符集可以避免由于字符集转换产生的乱码。**不同的字符集进行比较前需要进行转换会造成索引失效**。

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P141 ~ 142

## 覆盖索引

- 什么是覆盖索引？
  - **理解方式一：**索引是高效找到行的一个方法，但是一般数据库也能使用索引找到一个列的数据，因此它不必读取整个行。毕竟索引叶子节点存储了它们索引的数据；当能通过读取索引就可以得到想要的数据，那就不需要读取行了。**一个索引包含了满足查询结果的数据就叫做覆盖索引**。
  - **理解方式二**：非聚簇符合索引的一种形式，它包括在查询里的 SELECT、JOIN 和 WHERE 子句用到的所有列（即建索引的字段正好是覆盖查询条件中所涉及的字段）。
  - 简单说就是，`索引列+主键`包含 SELECT 到 FROM 之间查询的列。
- 覆盖索引的利弊
  - **好处：**
    - **1、避免 InnoDB 表进行索引的二次查询（回表）**
      - InnoDB 是以聚集索引的顺序来存储的，对于 InnoDB 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据，在查找到相应的键值后，还需通过主键进行二次查询才能获取我们真实所需要的数据。
      - 在覆盖索引中，二级索引的键值中可以获取所要的数据，避免了对主键的二次查询，减少了 IO 操作，提升了查询效率。
    - **2、可以把随机 IO 变成顺序 IO 加快查询效率**
      - 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此引用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。
    - **由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。**
  - **弊端：**
    - 索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这是业务 DBA，或者称为业务数据架构师的工作。

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P147

## 索引下推

- Index Condition Pushdown（ICP）是 MySQL 5.6 中新特性，是一种在存储引擎层使用索引过滤数据的优化方式。

  - 如果没有 ICP，存储引擎会遍历索引以定位基表中的行，并将它们返回给 MySQL 服务器，由 MySQL 服务器评估 `WHERE` 后面的条件是否保留行。
  - 启用 ICP 后，如果部分 `WHERE` 条件可以仅使用索引中的列进行筛选，则 MySQL 服务器会把这部分 `WHERE` 条件放到存储引擎筛选。然后存储引擎通过使用索引条目来筛选数据，并且只有在满足这一条件时才从表中读取行。
    - 好处：ICP 可以减少存储引擎必须访问基表的次数和 MySQL 服务器必须访问存储引擎的次数。
    - 但是，ICP 的加速效果取决于存储引擎内通过 ICP 筛选掉的数据的比例。

- ICP 的开启/关闭

  - 默认情况下启用索引条件下推。可以通过设置系统变量 `optimizer_switch` 控制：`index_condition_pushdown`

    ```mysql
    # 打开索引下推
    SET optimizer_switch = 'index_condition_pushdown=on';
    # 关闭索引下推
    SET optimizer_switch = 'index_condition_pushdown=off';
    ```

  - 当使用索引条件下推时，`EXPLAIN` 语句输出结果中 `Extra` 列内容显示为 `Using index condition`。

- ICP 使用案例

  - 建表

  - 插入数据

  - 为该表定义联合索引 zip_last_first(zipcode, lastname, firstname)。如果我们知道了一个人的邮编，但是不确定这个人的姓氏，我们可以进行如下检索：

    ```mysql
    SELECT * FROM people
    WHERE zipcode='000001'
    AND lastname LIKE '%张%'
    AND address LIKE '%北京市%';
    ```

    执行查看 SQL 的查询计划，`Extra` 中显示了 `Using index condition`，这表示使用了索引下推。另外， Using where 表示条件中包含需要过滤的非索引列的数据，即 address LIKE '%北京市%' 这个条件并不是索引列，需要在服务端过滤掉。

    如果不想出现 Using where，把 address LIKE '%北京市%' 去掉即可

    这个表中存在两个索引，分别是：

    - 主键索引
    - 二级索引 zip_last_first

    下面我们关闭 ICP 查看执行计划

    ```mysql
    SET optimizer_switch = 'index_condition_pushdown=off';
    ```

    查看执行计划，已经没有了 Using index condition，表示没有使用 ICP

    ```mysql
    EXPLAIN SELECT * FROM people
    WHERE zipcode='000001'
    AND lastname LIKE '%张%'
    AND address LIKE '%北京市%';
    ```

- ICP 的使用条件

  1. 如果表访问的类型为 range、ref、eq_ref 和 ref_or_null 可以使用 ICP
  2. ICP 可以用于 `InnoDB` 和 `MyISAM` 表，包括分区表 `InnoDB` 和 `MyISAM` 表
  3. 对于 `InnoDB` 表，ICP 仅用于二级索引。ICP 的目标是减少全行读取次数，从而减少 I/O 操作。
  4. 当 SQL 使用覆盖索引时，不支持 ICP。因为这种情况下使用 ICP 不会减少 I/O。
  5. 相关子查询的条件不能使用 ICP

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P148

## ACID

- **原子性（atomicity）**

  原子性是指事务是一个不可分割的工作单位，要么全部提交，要么全部失败回滚。即要么转账成功，要么转账失败，是不存在中间的状态。如果无法保证原子性会怎么样？就会出现数据不一致的情形，A 账户减去 100 元，而 B 账户增加 100 元操作失败，系统将无故丢失 100 元。

- **一致性（consistency）**

  根据定义，一致性是指事务执行前后，数据从一个**合法性状态**变换到另外一个**合法性状态**。这种状态是**语义上**的而不是语法上的，跟具体的业务有关。

  那么什么是合法的数据状态呢？满足**预定的约束**的状态就叫做合法的状态。通俗一点，这状态是由你自己来定义的（比如满足现实世界中的约束）。满足这个状态，数据就是一致的，不满足这个状态，数据就是不一致的！如果事务中某个操作失败了，系统就会自动撤销当前正在执行的事务，返回到事务操作之前的状态。

- **隔离性（isolation）**

  事务的隔离性是指一个事务的执行**不能被其他事务干扰**，即一个事务内部的操作及使用的数据对**并发**的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。

- **持久性（durability）**

  持久性是指一个事务一旦被提交，它对数据库中数据的改变就是**永久性的**，接下来的其他操作和数据库故障不应该对其有任何影响。

  持久性是通过**事务日志**来保证的。日志包括了**重做日志**和**回滚日志**。当我们通过事务对数据进行修改的时候，首先会将数据库的变化信息记录到重做日志中，然后再对数据库中对应的行进行修改。这样做的好处是，即使数据库系统崩溃，数据库重启后也能找到没有更新到数据库系统中的重做日志，重新执行，从而使事务具有持久性。

> 总结
>
> ACID 是事务的四大特性，在这四个特性中，原子性是基础，隔离性是手段，一致性是约束条件，而持久性是我们的目的。
>
> 数据库事务，其实就是数据库设计者为了方便起见，把需要保证**原子性**、**隔离性**、**一致性**和**持久性**的一个或多个数据库操作称为一个事务

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P161

## 事务隔离级别

- 数据库提供了四种事务隔离级别, 不同的隔离级别采用不同的锁类开来实现。
  - 在四种隔离级别中, Serializable 的级别最高, Read Uncommited级别最低。
  - 大多数数据库的默认隔离级别为: Read Commited,如 Sql Server , Oracle。
  - 少数数据库默认的隔离级别为 Repeatable Read, 如 MySQL InnoDB 存储引擎 。
  - 即使是最低的级别,也不会出现 第一类 丢失 更新问题。
- 事务并发问题
  - 1.脏读(事务没提交，提前读取)
    - 脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。
  - 2.不可重复读(两次读的不一致)
    - 指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。
    - 例如，一个编辑人员两次读取同一文档，但在两次读取之间，作者重写了该文档。当编辑人员第二次读取文档时，文档已更改。原始读取不可重复。如果只有在作者全部完成编写后编辑人员才可以读取文档，则可以避免该问题。
  - 3.幻读
    - 指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好像发生了幻觉一样。
    - 例如，一个编辑人员更改作者提交的文档，但当生产部门将其更改内容合并到该文档的主复本时，发现作者已将未编辑的新材料添加到该文档中。如果在编辑人员和生产部门完成对原始文档的处理之前，任何人都不能将新材料添加到文档中，则可以避免该问题。
  - 4.第一类更新丢失(回滚丢失)
    - 当2个事务更新相同的数据源，如果第一个事务被提交，而另外一个事务却被撤销，那么会连同第一个事务所做的更新也被撤销。也就是说第一个事务做的更新丢失了。
  - 5.第二类更新丢失(覆盖丢失)
    - 第二类更新丢失实在实际应用中经常遇到的并发问题，他和不可重复读本质上是同一类并发问题，通常他被看做不可重复读的特例：当2个或这个多个事务查询同样的记录然后各自基于最初的查询结果更新该行时，会造成第二类丢失更新。因为每个事务都不知道不知道其他事务的存在，最后一个事务对记录做的修改将覆盖其他事务对该记录做的已提交的更新。
- **不可重复读和幻读的区别**
  - 不可重复读的重点是修改：同样的条件，你读取过的数据，再次读取出来发现值不一样了 。
  - 幻读的重点在于新增或者删除：同样的条件, 第1次和第 2 次读出来的记录数不一样。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十五：数据库中事务的隔离级别有哪些？各自有什么特点？

## 慢查询

- 默认情况下，MySQL 数据库没有开启慢查询日志，需要我们手动来设置这个参数。

  - 如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。

- 开启慢查询日志步骤

  - 开启 slow_query_log

    在使用前，我们需要先看下慢查询是否已经开启，使用下面这条命令即可：

    ```mysql
    show variables like '%slow_query_log%';
    ```

    我们能看到 `slow_query_log=OFF`，我们可以把慢查询日志打开，注意设置变量值的时候需要使用 global，否则会报错：

    ```mysql
    set global slow_query_log='ON';
    ```

    然后我们再来查看下慢查询日志是否开启，以及慢查询日志文件的位置（slow_query_log_file）：

    你能看到这时慢查询分析已经开启，同时文件保存在 `/var/lib/mysql/atguigu02-slow.log` 文件中。

  - 修改 long_query_time 阈值

    接下来我们来看下慢查询的时间阈值设置，使用如下命令：

    ```mysql
    show variables like '%long_query_time%';
    ```

    这里如果我们想把时间缩短，比如设置为 1 秒，可以这样设置：

    ```mysql
    #测试发现：设置 global 的方式对当前 session 的 long_query_time 失效，对新连接的客户端有效，所以可以一并执行下述语句
    set global long_query_time = 1;
    show global variables like '%long_query_time%';
    
    set long_query_time = 1;
    show variables like '%long_query_time%';
    ```

    **补充：配置文件中一并设置参数**

    如下的方式相较于前面的命令行方式，可以看作是永久设置的方式。

    修改 `my.cnf` 文件，[mysqld]下增加或修改参数 `long_query_time`、`slow_query_log` 和 `slow_query_log_file` 后，然后重启 MySQL 服务器。

    ```properties
    [mysqld]
    slow_query_log=ON # 开启慢查询日志的开关
    slow_query_log_file=/var/lib/mysql/atguigu-slow.log #慢查询日志的目录和文件名信息
    long_query_time=3 #设置慢查询的阈值为 3 秒，超出此设定值的 SQL 即被记录到慢查询日志
    log_output=FILE
    ```

    如果不指定存储路径，慢查询日志将默认存储到 MySQL 数据库的数据文件夹下。如果不指定文件名，默认文件名为 hostname-slow.log

- 查看慢查询数目

  - 查询当前系统中有多少条慢查询记录：

    ```mysql
    SHOW GLOBAL STATUS LIKE `%Slow_queries%`;
    ```

- 慢查询日志分析工具：mysqldumpslow

  - 在生产环境中，如果要手工分析日志，查找、分析 SQL，显然是个体力活，MySQL 提供了日志分析工具 `mysqldumpslow`。

  - 查看 mysqldumpslow 的帮助信息

    ```mysql
    mysqldumpslow --help
    ```

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P135

## EXPLAIN 执行计划

- 版本情况

  - MySQL 5.6.3 以前只能 `EXPLAIN SELECT`；MySQL 5.6.3 以后就可以 `EXPLAIN SELECT, UPDATE, DELETE`
  - 在 5.7 以前的版本中，想要显示 `partitions` 需要使用 `explain partitions` 命令；想要显示 `filtered` 需要使用 `explain extended` 命令。在 5.7 版本后，默认 explain 直接显示 partitions 和 filtered 中的信息。

- EXPLAIN 语句输出的各个列的作用如下：

  | 列名          | 描述                                                      |
  | ------------- | --------------------------------------------------------- |
  | id            | 在一个大的查询语句中每个 SELECT 关键字都对应一个唯一的 id |
  | select_type   | SELECT 关键字对应的那个查询的类型                         |
  | table         | 表名                                                      |
  | partitions    | 匹配的分区信息                                            |
  | type          | 针对单表的访问方法                                        |
  | possible_keys | 可能用到的索引                                            |
  | key           | 实际上使用的索引                                          |
  | key_len       | 实际使用到的索引长度                                      |
  | ref           | 当使用索引列等值查询时，与索引列进行等值匹配的对象信息    |
  | rows          | 预估的需要读取的记录条数                                  |
  | filtered      | 某个表经过搜索条件过滤后剩余记录条数的百分比              |
  | Extra         | 一些额外信息                                              |

- 各列作用

  - table

    - MySQL 规定 EXPLAIN 语句输出的每条记录都对应着某个单表的访问方法，该条记录的 table 列代表着该表的表名（有时不是真实的表名字，可能是简称）。

  - id

    - **查询语句中每出现一个 `SELECT` 关键字，MySQL 就会为它分配一个唯一的 `id` 值**。

    - 这个 `id` 值就是 `EXPLAIN` 语句的第一个列

    - 这里需要大家记住的是，**在连接查询的执行计划中，每个表都会对应一条记录，这些记录的 id 列的值是相同的**，出现在前边的表表示驱动表，出现在后边的表表示被驱动表。

    - 对于包含子查询的查询语句来说，就可能涉及多个 `SELECT` 关键字，所以在**包含子查询的查询语句的执行计划中，每个 `SELECT` 关键字都会对应一个唯一的 `id` 值**

    - 但是这里大家需要特别注意，**查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询。**所以如果我们想知道查询优化器对某个包含子查询的语句是否进行了重写，直接查看执行计划就好了

    - UNION 子句、UNION ALL 的情况：

      - 对于包含 `UNION` 子句的查询语句来说，每个 `SELECT` 关键字对应一个 `id` 值也是没错的，不过还是有点儿特别的东西，比方说下边这个查询：

        ```mysql
        EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2;
        ```

        这个语句的执行计划的第三条记录是什么？为何 `id` 值是 `NULL`，而且 table 列也很奇怪？`UNION`！它会把多个查询的结果集并起来并对结果集中的记录进行去重，怎么去重呢？MySQL 使用的是内部的临时表。正如上边的查询计划中所示，UNION 子句是为了把 id 为 1 的查询和 id 为 2 的查询的结果集合并起来并去重，所以在内部创建了一个名为 `<union1, 2>` 的临时表（就是执行计划第三条记录的 table 列的名称），id 为 `NULL` 表明这个临时表是为了合并两个查询的结果集而创建的。

      - 跟 UNION 对比起来，`UNION ALL` 就不需要为最终的结果集进行去重，它只是单纯的把多个查询的结果集中的记录合并成一个并返回给用户，所以也就不需要使用临时表。所以在包含 `UNION ALL` 子句的查询的执行计划中，就没有那个 id 为 NULL 的记录

    - 小结：

      - id 如果相同，可以认为是一组，从上往下顺序执行
      - 在所有组中，id 值越大，优先级越高，越先执行
      - 关注点：id 号每个号码，表示一趟独立的查询，一个 sql 的查询趟数越少越好

  - select_type

    - | 名称                 | 描述                                                         |
      | -------------------- | ------------------------------------------------------------ |
      | SIMPLE               | Simple SELECT (not using UNION or subqueries)<br />查询语句中不包含 `UNION` 或者子查询的查询都算作是 `SIMPLE` 类型<br />当然，连接查询也算是 `SIMPLE` 类型 |
      | PRIMARY              | Outermost SELECT<br />对于包含 `UNION`、`UNION ALL` 或者子查询的大查询组成的，其中最左边的那个查询的 `select_type` 值就是 `PRIMARY` |
      | UNION                | Second or later SELECT statement in a UNION<br />对于包含 `UNION` 或者 `UNION ALL` 的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的 `select_type` 值就是 `UNION` |
      | UNION RESULT         | Result of a UNION<br />`MySQL` 选择使用临时表来完成 `UNION` 查询的去重工作，针对该临时表的查询的 `select_type` 就是 `UNION RESULT` |
      | SUBQUERY             | First SELECT in subquery<br />如果包含子查询的查询语句不能够转为对应的 `semi-join` 的形式，并且该子查询是不相关子查询，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个 `SELECT` 关键字代表的那个查询的 `select_type` 就是 `SUBQUERY` |
      | DEPENDENT SUBQUERY   | First SELECT in subquery, dependent on outer query           |
      | DEPENDENT UNION      | Second or later SELECT statement in a UNION, dependent on outer query |
      | DERIVED              | Derived table                                                |
      | MATERIALIZED         | Materialized subquery<br />当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询对应的 `select_type` 属性就是 `MATERIALIZED` |
      | UNCACHEABLE SUBQUERY | A subquery for which the result cannot be cached and must be re-evaluated for each row of the outer query |
      | UNCACHEABLE UNION    | The second or later select in a UNION that belongs to an uncacheable subquery (see UNCACHEABLE SUBQUERY) |

  - partitions

    - 代表分区表中的命中情况，非分区表，该项为 `NULL`。

  - type

    - 执行计划的一条记录就代表着 MySQL 对某个表的执行查询时的访问方法，又称“访问类型”，其中的 `type` 列就表明了这个访问方法是啥，是较为重要的一个指标。

    - 完整的访问方法如下：`system`，`count`，`eq_ref`，`ref`，`fulltext`，`ref_or_null`，`index_merge`，`unique_subquery`，`index_subquery`，`range`，`index`，`ALL`。

    - 详细解释

      - system

        - 当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如 MyISAM、Memory，那么对该表的访问方法就是 `system`。
          - 测试：可以把表改成使用 InnoDB 存储引擎，试试看执行计划的 `type` 列是什么？ALL

      - const

        - 当我们根据**主键**或者**唯一**二级索引列与常数进行等值匹配时，对单表的访问方法就是 `const`

      - eq_ref

        - 在**连接查询**时，如果**被驱动表**是通过**主键**或者**唯一**二级索引列等值匹配的方式进行访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较）则对该被驱动表的访问方法就是 `eq_ref`

      - ref

        - 当通过**普通的二级索引**列与常量进行等值匹配时来查询某个表，那么对该表的访问方法就可能是 `ref`

      - fulltext

        - 全文索引

      - ref_or_null

        - 当对**普通二级索引**进行等值匹配查询，该索引列的值也可以是 `NULL` 值时，那么对该表的访问方法就可能是 `ref_or_null`

      - index_merge

        - 一般情况下对于某个表的查询只能使用到一个索引，但单表访问方法时在某些场景下可以使用 **`Intersection`、`Union`、`Sort-Union`** 这三种索引合并的方式来执行查询。

        - 例子：

          ```mysql
          EXPLAIN SELECT * FROM s1 WHERE key1 = 'a' OR key3 = 'a';
          ```

      - unique_subquery

        - 类似于两表连接中的被驱动表的 `eq_ref` 访问方法，`unique_subquery` 是针对在一些**包含 `IN` 子查询**的查询语句中，如果查询优化器决定将 `IN` 子查询转换为 `EXISTS` 子查询，而且子查询可以使用到**主键**进行**等值匹配**的话，那么该子查询执行计划的 `type` 列的值就是 `unique_subquery`

      - index_subquery

        - `index_subquery` 与 `unique_subquery` 类似，只不过访问子查询中的表时使用的是**普通的索引**

      - range

        - 如果**使用索引**获取某些**特定范围**的记录，那么就可能使用到 `range` 访问方法

      - index

        - 当我们可以使用索引覆盖，但**需要扫描全部的索引记录**时，该表的访问方法就是 `index`

      - ALL

        - 最熟悉的全表扫描

    - **小结：**

      - 结果值从最好到最坏依次是：

        **system > const > eq_ref > ref** > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > **range > index > ALL**

        其中比较重要的几个提取出来（见粗体）。

      - SQL 性能优化的目标：至少要达到 range 级别，要求是 ref 级别，最好是 const 级别。（阿里巴巴开发手册要求）

  - possible_keys

    - `possible_keys` 列表示在某个查询语句中，对某个表执行单表查询时可能用到的索引有哪些。一般查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用。

  - keys

    - `key` 列表示实际用到的索引有哪些，如果为 NULL，则没有使用索引。

  - key_len

    - 实际使用到的索引长度（即：字节数）
    - 帮你检查是否充分的利用上了索引，值越大越好，主要针对于联合索引，有一定的参考意义。

  - ref

    - 显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量。哪些列或常量被用于查找索引列上的值。
    - 当使用索引列等值匹配的条件去执行查询时，也就是在访问方法是 `const`、`eq_ref`、`ref`、`ref_or_null`、`unique_subquery`、`index_subquery` 其中之一时，`ref` 列展示的就是与索引列作等值匹配的结构是什么，比如只是一个常数或者是某个列。

  - rows

    - 预估的需要读取的记录条数
    - 值越小越好

  - filtered

    - 某个表经过搜索条件过滤后剩余记录条数的**百分比**

  - extra

    - 顾名思义，`Extra` 列是用来说明一些额外信息的，包含不适合在其他列中显示但十分重要的额外信息。

    - `No tables used`

      - 当查询语句的没有 `FROM` 子句时将会提示该额外信息

    - `Impossible WHERE`

      - 查询语句的 `WHERE` 子句永远为 `FALSE` 时将会提示该额外信息

    - `Using where`

      - 不用读取表中所有信息，仅通过索引就可以获取所需数据，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示 mysql 服务器将在存储引擎检索行后再进行过滤。表明使用了 where 过滤。
      - 当我们使用全表扫描来执行对某个表的查询，并且该语句的 `WHERE` 子句中有针对该表的搜索条件时，在 `Extra` 列中会提示上述额外信息。

    - `No matching min/max row`

      - 当查询列表处有 `MIN` 或者 `MAX` 聚集函数，但是并没有符合 `WHERE` 子句中的搜索条件的记录时，将会提示该额外信息

    - `Using index`

      - 当我们的查询列表以及搜索条件中只包含属于某个索引的列，也就是在**可以使用索引覆盖**的情况下，在 `Extra` 列将会提示该额外信息。

    - `Using index condition`

      - 如果在查询语句的执行过程中将要使用索引条件下推这个特性，在 Extra 列中将会显示 `Using index condition`

      - **索引条件下推**

        - 有些搜索条件中虽然出现了索引列，但却不能使用到索引

        - 案例：

          - ```mysql
            SELECT * FROM s1 WHERE key1 > 'z' AND key1 LIKE '%a';
            ```

          - 先根据 `key1 > 'z'` 这个条件，定位到二级索引 `idx_key1` 中对应的二级索引记录，先不着急回表，而是先检测一下该记录是否满足 `key1 LIKE '%a'` 这个条件，如果这个条件不满足，则该二级索引记录压根儿就没必要回表。

        - 回表操作其实是一个随机 IO，比较耗时，所以上述修改虽然只改进了一点点，但是可以省去好多回表操作的成本。

    - `Using join buffer (Block Nested Loop)`

      - 在连接查询执行过程中，当被驱动表不能有效的利用索引加快访问速度，MySQL 一般会为其分配一块名叫 `join buffer` 的内存块来加快查询速度，也就是我们所讲的基于块的嵌套循环算法

    - `Not exists`

      - 当我们使用左（外）连接时，如果 `WHERE` 子句中包含要求被驱动表的某个列等于 `NULL` 值的搜索条件，而且那个列又是不允许存储 `NULL` 值的，那么在该表的执行计划的 Extra 列就会提示 `Not exists` 额外信息

    - `Using intersect(...)`、`Using union(...)` 和 `Using sort_union(...)`

      - 如果执行计划的 `Extra` 列出现了 `Using intersect(...)` 提示，说明准备使用 `Intersect` 索引合并的方式执行查询，括号中的 ... 表示需要进行索引合并的索引名称；
      - 如果出现了 `Using union(...)` 提示，说明准备使用 `Union` 索引合并的方式执行查询；
      - 出现了 `Using sort_union(...)` 提示，说明准备使用 `Sort-Union` 索引合并的方式执行查询。

    - `Zero limit`

      - 当我们的 `LIMIT` 子句参数为 0 时，表示压根不打算从表中读出任何记录，将会提示该额外信息

    - `Using filesort`

      - 但是很多情况下排序操作无法使用到索引，只能在内存中（记录较少的时候）或者磁盘中（记录较多的时候）进行排序，MySQL 把这种在内存中或者磁盘上进行排序的方式统称为文件排序（英文名：`filesort`）。
      - 如果某个查询需要使用文件排序的方式执行查询，就会在执行计划的 `Extra` 列中显示 `Using filesort` 提示

    - `Using temporary`

      - 在许多查询的执行过程中，MySQL 可能会借助临时表来完成一些功能，比如去重、排序之类的，比如我们在执行许多包含 `DISTINCT`、`GROUP BY`、`UNION` 等子句的查询过程中，如果不能有效利用索引来完成查询，MySQL 很有可能寻求通过建立内部的临时表来执行查询。
      - 如果查询中使用到了内部的临时表，在执行计划的 `Extra` 列将显示 `Using temporary` 提示

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P136 ~ 138

## 锁的种类

- 对数据的操作类型划分
  - 读锁/共享锁
    - S 锁 `SELECT ... LOCK IN SHARE MODE;`
    - X 锁 `SELECT ... FOR UPDATE;`
  - 写锁/排他锁
    - `DELETE`
    - `UPDATE`
- 锁粒度角度划分
  - 表级锁
    - 表级别的 S 锁、X 锁
    - 意向锁
    - 自增锁
    - MDL 锁
  - 行级锁
    - Record Locks
    - Gap Locks
    - Next-Key Locks
    - 插入意向锁
  - 页级锁
- 对待锁的态度划分
  - 悲观锁
  - 乐观锁
- 加锁方式
  - 隐式锁
  - 显式锁
- 其他
  - 全局锁
  - 死锁

参考文档：

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P174

## 表锁

- **1、表级别的 S 锁、X 锁**

  - MyISAM 在执行查询语句（SELECT）前，会给涉及的所有表加读锁，在执行增删改操作前，会给涉及的表加写锁。InnoDB 存储引擎是不会为这个表添加表级别的**读锁**或者**写锁**的。
    - 一般情况下，不会使用 InnoDB 存储引擎提供的表级别的 **S 锁**和 **X 锁**。只会在一些特殊情况下，比方说**崩溃恢复**过程中用到。比如，在系统变量 `autocommit=0, innodb_table_locks = 1` 时，**手动**获取 InnoDB 存储引擎提供的表 t 的 **S 锁**或者 **X 锁**可以这么写：
      - `LOCK TABLES t READ`：InnoDB 存储引擎会对表 `t` 加表级别的 **S 锁**
      - `LOCK TABLES t WRITE`：InnoDB 存储引擎会对表 `t` 加表级别的 **X 锁**
    - 不过尽量避免在使用 InnoDB 存储引擎的表上使用 `LOCK TABLES` 这样的手动锁表语句，它们并不会提供什么额外的保护，只是会降低并发能力而已。InnoDB 的厉害之处还是实现了更细粒度的**行锁**，关于 InnoDB 表级别的 **S 锁**和 **X 锁**大家了解一下就可以了。

- **2、意向锁（Intention lock）**

  - InnoDB 支持**多粒度锁（multiple granularity locking）**, 它允许**行级锁**与**表级锁**共存，而**意向锁**就是其中的一种**表锁**。

    1. 意向锁的存在是为了协调行锁和表锁的关系，支持多粒度（表锁与行锁）的锁并存。
    2. 意向锁是一种**不与行级锁冲突表级锁**，这一点非常重要。
    3. 表明“某个事务正在某些行持有了锁或该事务准备去持有锁”

  - 意向锁分为两种：

    - **意向共享锁**（intention shared lock，IS）：事务有意向对表中的某些行加**共享锁**（S 锁）

      ```mysql
      -- 事务要获取某些行的 S 锁，必须先获得表的 IS 锁。
      SELECT column FROM table ... LOCK IN SHARE MODE;
      ```

    - **意向排他锁**（intention exclusive lock，IX）：事务有意向对表中的某些行加**排他锁**（X 锁）

      ```mysql
      -- 事务要获取某些行的 X 锁，必须先获得表的 IX 锁
      SELECT column FROM table ... FOR UPDATE;
      ```

  - 首先，我们需要知道意向锁之间的兼容互斥性，如下所示。

    |                  | 意向共享锁（IS） | 意向排他锁（IX） |
    | ---------------- | ---------------- | ---------------- |
    | 意向共享锁（IS） | 兼容             | 兼容             |
    | 意向排他锁（IX） | 兼容             | 兼容             |

    即意向锁之间是互相兼容的，虽然意向锁和自家兄弟互相兼容，但是它会与普通的排他/共享锁互斥。

    |              | 意向共享锁（IS） | 意向排他锁（IX） |
    | ------------ | ---------------- | ---------------- |
    | 共享锁（IS） | 兼容             | 互斥             |
    | 排他锁（IX） | 互斥             | 互斥             |

    注意这里的排他/共享锁指的都是表锁，**意向锁不会与行级的共享/排他锁互斥**。

  - 并发性

    - 意向锁不会与行级的共享/排他锁互斥！正因为如此，意向锁并不会影响到多个事务对不同数据行加排他锁的并发性。（不然我们直接用普通的表锁就行了）

- **3、自增锁（AUTO-INC 锁）**

  - 在使用 MySQL 过程中，我们可以为表的某个列添加 `AUTO_INCREMENT` 属性；由于这个表的 id 字段声明了 AUTO_INCREMENT，意味着在书写插入语句时不需要为其赋值

  - 所有插入数据的方式总共分为三类，分别是“`Simple inserts`”，“`Bulk inserts`” 和 “`Mixed-mode inserts`”。

    1. **“Simple inserts”（简单插入）**

       可以**预先确定要插入和行数**（当语句被初始处理时）的语句。包括没有嵌套子查询的单行和多行 `INSERT ... VALUES()` 和 `REPLACE` 语句。比如我们上面举的例子就属于该类插入，已经确定要插入的行数。

    2. **“Bulk inserts”（批量插入）**

       **事先不知道要插入的行数**（和所需自动递增的数量）的语句。比如 `INSERT ... SELECT`, `REPLACE ... SELECT` 和 `LOAD DATA` 语句，但不包括纯 INSERT。InnoDB 在每处理一行，为 AUTO_INCREMENT 列分配一个新值。

    3. **“Mixed-mode inserts”（混合模式插入）**

       这些是“Simple inserts”语句但是指定部分新行的自动递增值。例如 `INSERT INTO teacher (id, name) VALUES (1, 'a'), (NULL, 'b'), (5, 'c'), (NULL, 'd');` 只是指定了部分 id 的值。另一种类型的“混合模式插入”是 `INSERT ... ON DUPLICATE KEY UPDATE`。

  - 对于上面数据插入的案例，MySQL 中采用了**自增锁**的方式来实现，**AUTO-INC 锁是当向使用 AUTO_INCREMENT 列的表中插入数据时需要获取的一种特殊的表级锁**，在执行插入语句时就在表级别加一个 AUTO-INC 锁，然后为每条待插入记录的 AUTO_INCREMENT 修饰的列分配递增的值，在该语句执行结束后，再把 AUTO-INC 锁释放掉。

    - **一个事务在持有 AUTO-INC 锁的过程中，其他事务的插入语句都要被阻塞**，可以保证一个语句中分配的递增值是连续的。

    - 也正因为此，其并发性显然并不高，**当我们向一个有 AUTO_INCREMENT 关键字的主键插入值的时候，每条语句都要对这个表锁进行竞争**，这样的并发潜力其实是很低下的，所以 innodb 通过 `innodb_autoinc_lock_mode` 的不同取值来提供不同的锁定机制，来显著提高 SQL 语句的可伸缩性和性能。

      - **（1）innodb_autoinc_lock_mode = 0（“传统”锁定模式）**

        在此锁定模式下，所有类型的 insert 语句都会获得一个特殊的表级 AUTO-INC 锁，用于插入具有 AUTO_INCREMENT 列的表。

      - **（2）innodb_autoinc_lock_mode = 1（“连续”锁定模式）**

        在 MySQL 8.0 之前，连续锁定模式是**默认**的。

        在这个模式下，“bulk inserts”仍然使用 AUTO-INC 表级锁，并保持到语句结束。

        对于“Simple inserts”（要插入的行数事先已知），则通过在 `mutex（轻量锁）`的控制下获得所需数量的自动递增值来避免表级 AUTO-INC 锁，它只在分配过程的持续时间内保持，而不是直到语句完成。不使用表级 AUTO-INC 锁，除非 AUTO-INC 锁由另一个事务保持。如果另一个事务保持 AUTO-INC 锁，则“Simple inserts”等待 AUTO-INC 锁，如同它是一个“bulk inserts”

      - **（3）innodb_autoinc_lock_mode = 2（“交错”锁定模式）**

        从 MySQL 8.0 开始，交错锁模式是**默认**设置。

        在这种锁定模式下，所有类 INSERT 语句都不会使用表级 AUTO-INC 锁，并且可以同时执行多个语句。这是最快和最可扩展的锁定模式，但是当使用基于语句的复制或恢复方案时，**从二进制日志重播 SQL 语句时，这是不安全的**。

- **4、元数据锁（MDL 锁）**

  - MySQL 5.5 引入了 meta data lock，简称 MDL 锁，属于表锁范畴。
  - MDL 的作用是，保证读写的正确性。
    - 比如，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个**表结构做变更**，增加了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。
  - 因此，**当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。**
  - **不需要显式使用**，在访问一个表的时候会被自动加上。

参考文档：

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P175 ~ 176

## 行锁

- 行锁（Row Lock）也称为记录锁，顾名思义，就是锁住某一行（某条记录 row）。需要的注意的是，MySQL 服务器层并没有实现行锁机制，**行级锁只在存储引擎层实现**。
- InnoDB 与 MyISAM 的最大不同有两点：一是支持事务（TRANSACTION）；二是采用了行级锁。
- 分类
  1. 记录锁（Record Locks）
     - 记录锁也就是仅仅把一条记录锁上，官方的类型名称为：`LOCK_REC_NOT_GAP`。比如我们把 id 值为 8 的那条记录加一个记录锁，仅仅是锁住了 id 值为 8 的记录，对周围的数据没有影响。
     - 记录锁是有 S 锁和 X 锁之分的，称之为 **S 型记录锁**和 **X 型记录锁**
  2. 间隙锁（Gap Locks）
     - `MySQL` 在 `REPEATABLE READ` 隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用 `MVCC` 方案解决，也可以采用**加锁**方案解决。（分别对应**快照读**（一致性读）和**当前读**（锁定读）两种情况下的处理方式）
       - 但是在使用加锁方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些**幻影记录**加上**记录锁**。
       - InnoDB 提出了一种称之为 **Gap Locks** 的锁，官方的类型名称为：`LOCK_GAP`，我们可以简称为 **gap 锁**。
     - **gap 锁的提出仅仅是为了防止插入幻影记录而提出的。**虽然有**共享 gap 锁**和**独占 gap 锁**这样的说法，但是它们起到的作用是相同的。而且如果对一条记录加了 gap 锁（不论是共享 gap 锁还是独占 gap 锁），并不是限制其他事务对这条记录加记录锁或者继续加 gap 锁。
  3. 临键锁（Next-Key Locks）
     - 有时候我们既想**锁住某条记录**，又想**阻止**其他事务在该记录前边的**间隙插入新纪录**，所以 InnoDB 就提出了一种称之为 `Next-Key Locks` 的锁，官方的类型名称为：`LOCK_ORDINARY`，我们也可以简称为 **next-key 锁**。
     - Next-Key Locks 是在存储引擎 `innodb`、事务级别在**可重复读**的情况下使用的数据库锁，innodb 默认的锁就是 Next-Key locks。
     - **next-key 锁**的本质就是一个**记录锁**和一个 **gap 锁**的合体，它既能保护该条记录，又能阻止别的事务将新的记录插入被保护记录前边的**间隙**。
  4. 插入意向锁（Insert Intention Locks）
     - 我们说一个事务在**插入**一条记录时需要判断一下插入位置是不是被别的事务加了 **gap 锁**（**next-key 锁**也包含 **gap 锁**），如果有的话，插入操作需要等待，直到拥有 **gap 锁**的那个事务提交。
     - 但是 **InnoDB 规定事务在等待的时候也需要在内存中生成一个锁结构**，表明有事务想在某个**间隙**中**插入**新纪录，但是现在在等待。
     - InnoDB 就把这种类型的锁命名为 `Insert Intention Locks`，官方的类型名称为：`LOCK_INSERT_INTENTION`，我们称为**插入意向锁**。
     - 插入意向锁是一种 **Gap 锁**，不是意向锁，在 insert 操作时产生。
     - 总结来说，插入意向锁的特性可以分成两部分：

       1. 插入意向锁是一种**特殊的间隙锁**——间隙锁可以锁定开区间内的部分记录。
       2. 插入意向锁之间**互不排斥**，所以即使多个事务在同一区间插入多条记录，只要记录本身（主键、唯一索引）不冲突，那么事务之间就不会出现冲突等待。
     - 注意，虽然插入意向锁中含有意向锁三个字，但是它并不属于意向锁而属于间隙锁，因为意向锁是表锁而插入意向锁是**行锁**。

参考文档：

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P177 ~ 178

## 页锁

- 页锁就是在**页的粒度**上进行锁定，锁定的数据资源比行锁要多，因为一个页中可以有多个行记录。
  - 当我们使用页锁的时候，会出现数据浪费的现象，但这样的浪费最多也就是一个页上的数据行。
- **页锁的开销介于表锁和行锁之间，会出现死锁。锁定粒度介于表锁和行锁之间，并发度一般。**
- 每个层级的锁数量是有限制的，因为锁会占用内存空间，**锁空间的大小是有限的**。当某个层级的锁数量超过了这个层级的阈值时，就会进行**锁升级**。
  - 锁升级就是用更大粒度的锁替代多个更小粒度的锁，比如 InnoDB 中行锁升级为表锁，这样做的好处是占用的锁空间降低了，但同时数据的并发度也下降了。
- 目前只有 **BDB 引擎**支持页面锁，应用场景较少。
  - （也就是 InnoDB 没有页锁）

参考文档：

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P179

## 隐式锁

- 一个事务在执行 `INSERT` 操作时，如果即将插入的**间隙**已经被其他事务加了 **gap 锁**，那么本次 `INSERT` 操作会阻塞，并且当前事务会在该间隙上加一个**插入意向锁**，否则一般情况下 `INSERT` 操作是不加锁的。

- 那如果一个事务首先插入了一条记录（此时并没有在内存生产与该记录关联的锁结构），然后另一个事务：

  - 立即使用 `SELECT ... LOCK IN SHARE MODE` 语句读取这条记录，也就是要获取这条记录的 **S 锁**，或者使用 `SELECT ... FOR UPDATE` 语句读取这条记录，也就是要获取这条记录的 **X 锁**，怎么办？

    如果允许这种情况的发生，那么可能产生**脏读**问题。

  - 立即修改这条记录，也就是要获取这条记录的 **X 锁**，怎么办？

  - 如果允许这种情况的发生，那么可能产生**脏写**问题。

- 这时候我们前边提到过的**事务 id** 又要起作用了。我们把聚簇索引和二级索引中的记录分开看一下：

  - **情景一**：对于聚簇索引记录来说，有一个 `trx_id` 隐藏列，该隐藏列记录着最后改动该记录的**事务 id**。那么如果在当前事务中新插入一条聚簇索引记录后，该记录的 `trx_id` 隐藏列代表的就是当前事务的**事务 id**，如果其他事务此时想对该记录添加 **S 锁**或者 **X 锁**时，首先会看一下该记录的 `trx_id` 隐藏列代表的事务是否是当前的活跃事务，如果是的话，那么就帮助当前事务创建一个 **X 锁**（也就是为当前事务创建一个锁结构，`is_waiting` 属性是 `false`），然后自己进入等待状态（也就是为自己也创建一个锁结构，`is_waiting` 属性是 `true`）。
  - **情景二**：对于二级索引记录来说，本身并没有 `trx_id` 隐藏列，但是在二级索引页面的 `Page Header` 部分有一个 `PAGE_MAX_TRX_ID` 属性，该属性代表对该页面做改动的最大的**事务 id**，如果 `PAGE_MAX_TRX_ID` 属性值小于当前最小的活跃**事务 id**，那么说明对该页面做修改的事务都已经提交了，否则就需要在页面中定位到对应的二级索引记录，然后回表找到它对应的聚簇索引记录，然后再重复**情景一**的做法。
  - 即 ：一个事务对新插入的记录可以不显式的加锁（生成一个锁结构），但是由于**事务 id**的存在，相当于加了一个**隐式锁**。别的事务在对这条记录加 **S 锁**或者 **X 锁**时，由于**隐式锁**的存在，会先帮助当前事务生成一个锁结构，然后自己再生成一个锁结构后进入等待状态。隐式锁是一种**延迟加锁**的机制，从而来减少加锁的数量。
  - 隐式锁在实际内存对象中并不含有这个锁信息，只有当产生锁等待时，隐式锁转化为显式锁。

- 隐式锁的逻辑过程如下：

  1. InnoDB 的每条记录中都一个隐含的 trx_id 字段，这个字段存在于聚簇索引的 B+Tree 中。
  2. 在操作一条记录前，首先根据记录中的 trx_id 检查该事务是否是活动的事务（未提交或回滚）。如果是活动的事务，首先将**隐式锁**转换为**显式锁**（就是为该事务添加一个锁）。
  3. 检查是否有锁冲突，如果有冲突，创建锁，并设置为 waiting 状态。如果没有冲突不加锁，跳到 5.
  4. 等待加锁成功，被唤醒，或者超时
  5. 写数据，并将自己的 trx_id 写入 trx_id 字段。

参考文档：

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P180

## 全局锁

全局锁就是对**整个数据库实例**加锁。当你需要让整个库处于**只读状态**的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。全局锁的典型使用**场景**是：做**全库逻辑备份**。

全局锁的命令：

```mysql
Flush tables with read lock
```

参考文档：

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P181

## 防范 SQL 注入

1. JDBC 的 PreparedStatement

   - 采用预编译语句集，它内置了处理SQL注入的能力

2. 使用 MyBatis 的 #

   - \# 将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。

   - $ **将传入的数据直接显示生成在sql中。**

   - MyBatis是如何做到SQL预编译的呢？

     其实在框架底层，是JDBC中的PreparedStatement类在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的SQL语句。

     - 简单说，#{}是经过预编译的，是安全的；
     - ${}是未经过预编译的，仅仅是取变量的值，是非安全的，存在SQL注入。

## WAL 机制 / 日志刷盘

- **WAL，全称是 Write-Ahead Logging， 预写日志系统。**
  - 指的是 MySQL 的写操作并不是立刻更新到磁盘上，而是先记录在日志上，然后在合适的时间再更新到磁盘上。
    - 这样的好处是错开高峰期。
- 日志主要分为 undo log、redo log、binlog。
  - 这三种在之前的博客已经详细说过了，作用分别是：
    - undo log "完成MVCC从而实现 MySQL 的隔离级别 "
    - redo log "降低随机写的性能消耗（转成顺序写），同时防止写操作因为宕机而丢失 "
    - binlog " 写操作的备份，保证主从一致 "。
- 关于这三种日志的内容讲的比较分散且具体的执行过程没有提到，所以这里来总结一下这三种日志。
  - **undo log**
    - undo log 主要用于实现 MVCC，从而实现 MySQL 的 **”读已提交“、”可重复读“** 隔离级别。
    - 在每个行记录后面有两个隐藏列，**"trx_id"、"roll_pointer"**，分别表示上一次修改的事务id，以及 "上一次修改之前保存在 undo log中的记录位置 "。
      - 在对一行记录进行修改或删除操作前，会先将该记录拷贝一份到 undo log 中，然后再进行修改，并将修改事务 id，拷贝的记录在 undo log 中的位置写入 "trx_id"、"roll_pointer"。
    - 而 MVCC 最核心的就是 **版本链** 和通过版本链生成的 **Read View**。
      1. **版本链**：通过 "roll_pointer" 字段指向的上一次修改的值，使每行记录变化前后形成了一条版本链。
      2. **Read View**：Read View 表示可见视图，用于限制当前事务查询数据的，通过与版本链的配合可以实现对数据的 “快照读” 。
         - Read View 内部主要有四个部分组成
           - 第一个是创建当前 Read View 的事务 id **creator_trx_id**
           - 第二个是创建 Read View 时还未提交的事务 id 集合 **trx_ids**
           - 第三个是未提交事务 id 集合中的最大值 **up_limit_id**
           - 第四个是未提交事务 id 集合中的最小值 **low_limit_id**。
    - 当执行查询操作时会先找磁盘上的数据，然后根据 Read View 里的各个值进行判断，
      1. 如果该数据的 trx_id 等于 creator_trx_id，那么就说明这条数据是创建 Read View的事务修改的，那么就直接返回；
      2. 如果 trx_id 大于等于 up_limit_id，说明是新事务修改的，那么会根据 roll_pointer 找到上一个版本的数据重新比较；
      3. 如果 trx_id 小于 low_limit_id，那么说明是之前的事务修改的数据，那么就直接返回；
      4. 如果 trx_id 是在 low_limit_id 与 up_limit_id 中间，那么需要去 trx_ids 中对各个元素逐个判断，如果存在相同值的元素，就根据 roll_pointer 找到上一个版本的数据，然后再重复判断；如果不存在就说明该数据是创建当前 Read View 时就已经修改好的了，可以返回。
    - 而读已提交和可重复读之所以不同就是它们 Read View 生成机制不同
      - **读已提交是每次 select 都会重新生成一次**，
      - 而**可重复读是一次事务只会创建一次且在第一次查询时创建 Read View**。事务启动命令begin/start transaction不会创建Read View，但是通过 start transaction with consistent snapshot 开启事务就会在开始时就创建一次 Read View。
  - **redo log**
    - redo log 是搭配缓冲池、change buffer 使用的
      - **缓冲池**的作用是缓存磁盘上的数据页，减少磁盘的IO；
      - **change buffer** 的作用是将写操作先存在内存中，等到下次需要读取这些操作涉及到的数据页时，就把数据页加载到缓冲池中，然后在缓冲池中更新；
    - redo log 的作用就是持久化记录的写操作，防止在写操作更新到磁盘前发生断电丢失这些写操作，**直到该操作对应的脏页真正落盘（先读取数据页到缓冲池然后应用写操作到缓冲池，最后再将脏页落盘替换磁盘上的数据页），该操作才会从 redo log 中移除（覆盖）**。记录的是写操作对数据页的修改逻辑以及 change buffer的变化。
    - 三种状态：
      - 在将写操作写入 redo log 的过程中并不是直接就进行磁盘IO来完成的，而是分为三个步骤。
        1. 写入 **redo log buffer** 中，这部分是属于MySQL 的内存中，是**全局公用**的。
        2. 在事务编写完成后，就可以执行 write 操作，写到文件系统的 **page cache** 中，属于操作系统的内存，如果 MySQL 崩溃不会影响，但如果机器断电则会丢失数据。
        3. 执行 fsync（持久化）操作，将 page cache 中的数据正式写入**磁盘**上的 redo log 中，也就是图中的 hard disk。
    - redo log 的持久化
      1. 持久化策略通过参数 innodb_flush_log_at_trx_commit 控制。
         - 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ; MySQL 崩溃就会丢失。
         - 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘（**将 redo log buffer 中的操作全部进行持久化，可能会包含其他事务还未提交的记录**）；断电也不会丢失。
         - 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。MySQL 崩溃不会丢失，断电会丢失。
      2. InnoDB 后台还有一个线程会每隔一秒钟将 redo log buffer 中记录的操作执行 write 写到 page cache，然后再 fsync 到磁盘上。
    - 未提交的事务操作也可能会持久化，未提交事务操作的持久化触发场景如下：
      1. redo log buffer 被占用的空间达到 innodb_log_buffer_size（redo log buffer 大小参数）的一半时，后台会主动写盘，无论是否是已完成的事务操作都会执行。
      2. innodb_flush_log_at_trx_commit 设为 1 时，在每次事务提交时，都会将 redo log buffer 中的所有操作（包括未提交事务的操作）都进行持久化。
      3. 后台有线程每秒清空 redo log buffer 进行落盘。
  - **binlog**
    - binlog 也是保存写操作的，但是它主要是用于进行集群中保证主从一致以及执行异常操作后恢复数据的。
    - 三种格式
      1. Row（5.7默认）。
         - 记录操作语句对具体行的操作以及操作前的整行信息。
         - 缺点是占空间大。
         - 优点是**能保证数据安全，不会发生遗漏**。
         - 内容可以通过 " mysqlbinlog + 文件名 " 来查看，**一个事务的结尾会有 " Xid" 标记（作为三步提交时判断事务是否执行完成的判断标记）**
      2. Statement。
         - 记录修改的 sql。
         - 缺点是在 mysql 集群时可能会导致操作不一致从而使得数据不一致（比如在操作中加入了Now()函数，主从数据库操作的时间不同结果也不同）。
         - 优点是占空间小，执行快。
      3. MIxed
         - 会针对于操作的 sql 选择使用Row 还是 Statement。
         - 相比于 row 更省空间，但还是可能发生主从不一致的情况。
    - 三种状态
      - 和 redo log 类似，binlog 写到磁盘上的过程也分为三种状态：binlog cache（**每个线程各有一份**）、page chache、disk。
        - write：从binglog cache写到 page cache。
        - fsync：将数据持久化到磁盘。
    - binlog 的持久化
      - binlog 的持久化策略通过参数 sync_binlog 控制：
        - sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync；
        - sync_binlog=1 的时候，表示每次提交事务都会执行 fsync；
        - sync_binlog=N(N>1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。
- redo log 和 binlog 两者的联系
  - 状态
    - 两者都经历三种状态： MySQL 的 Cache、Page cache、磁盘。
    - 只不过 redo log 在 MySQL 的 Cache 是全局共用的，而 binlog 在 MySQL 中的 Cache 是线程私有的，每个线程都有一份。
    - 同时两者的 write 操作写入 Page Cache 都非常快（因为在内存中），而 fsync 到磁盘都比较慢（因为需要进行磁盘IO）。
  - Crash-Safe 能力
    - Crash-safe 能力，指的是在机器突然断电重启后，之前的数据不会丢失，能够恢复成断电前状态的能力。redo log 拥有 crash-safe 能力，而 binlog 没有。
      - 这是因为 **redo log 记录的是未更新到磁盘上的操作，在断电后只需要将记录的操作数据更新到缓冲池中就可以了。**
      - **而 binlog 记录的是所有请求过来的写操作，这个写操作在断电前有没有落盘并不知道**。也正因为如此所以采用 redo log 与 binlog 的 “ 三步提交 ” 来保证 binlog 也具有 crash-safe 能力。
  - " 三步提交 " 过程是 " 写 redo log（prepare）-----> 写 binlog --------> redo log (commit) "。**在断电重启后先检查 redo log 记录的事务操作是否为 commit 状态**
    1. 如果是 commit 状态说明没有数据丢失，判断下一个。
    2. 如果是 prepare 状态，检查 binlog 记录的对应事务操作（redo log 与 binlog 记录的事务操作有一个共同字段 XID，redo log 就是通过这个字段找到 binlog 中对应的事务的）是否完整（这点在前面 binlog 三种格式分析过，每种格式记录的事务结尾都有特定的标识），如果完整就将 redo log 设为 commit 状态，然后结束；不完整就回滚 redo log 的事务，结束。
  - 三步提交的参数配置
    - 上面说到 redo log 与 binlog 的 “ 三步提交 ” 可以使 binlog 也具有 crash-safe 能力，但是并不是绝对的，" 三步提交 " 还需要搭配合适的 redo log 与 binlog 的持久化策略才可以完全保证断电重启后操作数据不会丢失。
    - 如果想要数据库拥有 " crash-safe " 能力，那么就需要将 redo log 的持久化策略参数 innodb_flush_log_at_trx_commit 设为1，binlog 的持久化策略参数 sync_binlog 设为大于0。
      1. 首先 innodb_flush_log_at_trx_commit 如果设为 “ 非1 ”，那么断电后一定会丢失 redo log 记录的数据，而binlog 也就失去了 “ 参照物 ”，造成主从不一致。
      2. 而如果 sync_binlog 设为 0 时，在断电后会丢失所有数据；等于1 会丢失还未 fsync 完成的事务数据；大于1时会在断电后丢失上一次 fsync 到现在所有未完成 fsync 的事务数据。1 和 大于1 的区别就是 大于1 会更节省 CPU 资源，但是在断电后会丢失更多的事务操作，所以在一般情况下都使用 “ **双 1 配置** ”，也就是将 sync_binlog 和 innodb_flush_log_at_trx_commit 都设为 1， 这样搭配 “ 三步提交 ” 可以在最大程度上保证数据的完整性。最多也只会丢失一条事务操作，然后回滚就可以了。
    - 但是 “ 双 1 配置 ” 伴随着巨大的性能消耗，所以在某些场景下不适合使用 “ 双 1 配置 ”。
      1. 业务高峰期，系统执行缓慢；
      2. 备库延迟较高，需要让备库尽快赶上主库；
      3. 批量导入数据时。
    - **上面这些非双1场景一般设置：innodb_flush_logs_at_trx_commit=2、sync_binlog=1000。**
  - 组提交优化“三步提交”
    - 通过上面的分析知道 “ 双1配置 ” 可以更完整得具有 crash-safe 能力，但是这样配置会给系统带来更大的 IO 压力，因为这样配置就需要在每次事务提交时都进行一次 redo log 与 binlog 的磁盘 IO，带来的压力是非常大的，那么有没有什么方式来缓解呢？组提交就是用来减少 redo log、binlog 带来的磁盘 IO 压力的。
    - **实现方式**：日志逻辑序列号（log sequence number，LSN）表示redo log记载的写入点，也就是最新写入事务的开始点，其前面都是已写完的事务。因为 redo log 写入 “ redo log buffer 完成 “ 到 " write 到 page cache"、" 正式开始 fsync " 需要时间，在这个时间内可能伴随着多个事务的写入完成，那么就可以以第一个事务为准，在持久化时将操作记录完成的事务合并一起进行 fsync。
    - 因为 binlog 也拥有组提交，所以这样执行也可以提高 binlog 的 IO 消耗，但单条 redo log 的 fsync 执行的很快，为了进一步提高 binlog 组提交节省的资源，还可以通过参数 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 来延长 binlog 执行 fsync 的时间。 
  - 三步提交过程总结
    - 在 redo log 持久化参数 innodb_flush_log_at_trx_commit 设为 1 时，每次提交、每秒钟都会清空 redo log buffer 来执行三步提交，而在两个日志 fsync 持久化时还会分组来进行组提交，减少磁盘IO 次数。
    - 特点：
      1. 组提交是以组为单位按顺序进行写操作的，从 redo log prepare 状态开始到 redo log commit 状态同一时刻只会有一个组的事务在执行。
      2. 一个组的事务中的操作对某一行的操作一定是唯一的。因为如果两个事务对同一行记录进行操作，那么一定有一个事务会被行锁所阻塞，导致其不会跟另一个事务在同一个组内。
- 三个日志的比较（undo、redo、bin）
  1. undo log是用于事务的回滚、保证事务隔离级别读已提交、可重复读实现的。redo log是用于对暂不更新到磁盘上的操作进行记录，使得其可以延迟落盘，保证程序的效率。bin log是对数据操作进行备份恢复（并不能依靠 bin log 直接完成数据恢复）。
  2. undo log 与 redo log 是存储引擎层的日志，只能在 InnoDB 下使用；而bin log 是 Server 层的日志，可以在任何引擎下使用。
  3. redo log 大小有限，超过后会循环写；另外两个大小不会。
  4. undo log 记录的是行记录变化前的数据；redo log 记录的是 sql 的数据页修改逻辑以及 change buffer 的变更；bin log记录操作语句对具体行的操作以及操作前的整行信息（5.7默认）或者sql语句。
  5. 单独的 binlog 没有 crash-safe 能力，也就是在异常断电后，之前已经提交但未更新的事务操作到磁盘的操作会丢失，也就是主从复制的一致性无法保障，而 redo log 有 crash-safe 能力，通过与 redo log 的配合实现 "三步提交"，就可以让主从库的数据也能保证一致性。
  6. redo log 是物理日志，它记录的是数据页修改逻辑以及 change buffer 的变更，只能在当前存储引擎下使用，而 binlog 是逻辑日志，它记录的是操作语句涉及的每一行修改前后的值，在任何存储引擎下都可以使用。

参考文档：

1. [MySQL 中的WAL机制 - 博客园](https://www.cnblogs.com/mengxinJ/p/14211427.html)

## 主从复制

- **主从复制原理**
  - (1) Master 将数据改变记录到**二进制日志(binary log)**中，也就是配置文件 log-bin 指定的文件， 这些记录叫做**二进制日志事件(binary log events)**；
  - (2) Slave 通过 I/O 线程读取 Master 中的 binary log events 并写入到它的**中继日志(relay log)**；
  - (3) Slave 重做中继日志中的事件，把中继日志中的事件信息一条一条的在本地执行一次，完 成数据在本地的存储，从而实现将改变反映到它自己的数据(数据重放)。
- **注意事项**
  - (1)主从服务器操作系统版本和位数一致；
  - (2) Master 和 Slave 数据库的版本要一致；
  - (3) Master 和 Slave 数据库中的数据要一致；
  - (4) Master 开启二进制日志，Master 和 Slave 的 server_id 在局域网内必须唯一；
- **配置主从复制步骤**
  - **Master 数据库**
    - (1) 安装数据库；
    - (2) 修改数据库配置文件，指明 server_id，开启二进制日志(log-bin)；
    - (3) 启动数据库，查看当前是哪个日志，position 号是多少；
    - (4) 登录数据库，授权数据复制用户（IP 地址为从机 IP 地址，如果是双向主从，这里的 还需要授权本机的 IP 地址，此时自己的 IP 地址就是从 IP 地址)；
    - (5) 备份数据库（记得加锁和解锁）；
    - (6) 传送备份数据到 Slave 上；
    - (7) 启动数据库；
    - 以上步骤，为单向主从搭建成功，想搭建双向主从需要的步骤：
      - (1) 登录数据库，指定 Master 的地址、用户、密码等信息（此步仅双向主从时需要）；
      - (2) 开启同步，查看状态；
  - **Slave 上的配置**
    - (1) 安装数据库；
    - (2) 修改数据库配置文件，指明 server_id（如果是搭建双向主从的话，也要开启二进制 日志 log bin）；
    - (3) 启动数据库，还原备份；
    - (4) 查看当前是哪个日志，position 号是多少（单向主从此步不需要，双向主从需要）；
    - (5) 指定 Master 的地址、用户、密码等信息；
    - (6) 开启同步，查看状态。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景六：讲讲 MySQL 主从复制的原理和注意事项？

# Redis

## 数据结构

1. redis 字符串（String）
   - String（字符串）
   - string 是 redis 最基本的类型，一个 key 对应一个 value。
   - string 类型是**二进制安全的**，意思是 redis 的 string 可以包含任何数据，比如 jpg 图片或者序列化的对象。
   - string 类型是 Redis 最基本的数据类型，一个 redis 中字符串 value 最多可以是 512M
2. redis 列表（List）
   - Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的**头部（左边）或者尾部（右边）**
   - 它的底层实际是个**双端链表**，最多可以包含 2^32 - 1 个元素（4294967295，每个列表超过 40 亿个元素）
3. redis 哈希表（Hash）
   - Redis hash 是一个 string 类型的 field（字段）和 value（值）的映射表，hash 特别适合用于存储对象。
   - Redis 中每个 hash 可以存储 2^32 - 1 键值对（40 多亿）
4. redis 集合（Set）
   - Redis 的 Set 是 String 类型的**无序集合**。集合成员是唯一的，这就意味着集合中不能出现重复的数据，集合对象的编码可以是 intset 或者 hashtable。
   - Redis 中 Set 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。
   - 集合中最大的成员数为 2^32 - 1（4294967295，每个集合可存储 40 多亿个成员）
5. redis 有序集合（ZSet）
   - zset（sorted set，有序集合）
   - Redis zset 和 set 一样也是 string 类型元素的集合，且不允许重复的成员。
   - **不同的是每个元素都会关联一个 double 类型的分数，**redis 正是通过分数来为集合中的成员进行从小到大的排序。
   - **zset 的成员是唯一的，但分数（score）却可以重复。**
   - **zset 集合是通过哈希表实现的，所以添加、删除，查找的复杂度都是 O(1)。集合中最大的成员数为 2^32 - 1**
6. redis 地理空间（GEO）
   - Redis GEO 主要用于存储地理位置信息，并对存储的信息进行操作，包括：
     - 添加地理位置的坐标
     - 获取地理位置的坐标
     - 计算两个位置之间的距离
     - 根据用户给定的经纬度坐标来获取指定范围内的地理位置集合
7. redis 基数统计（HypeLogLog）
   - HyperLogLog 是用来做**基数统计**的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定且是很小的。
   - 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。
   - 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。
8. redis 位图（bitmap）
   - Bit arrays (or simply bitmaps，我们可以称之为 位图)
   - 一个字节（一个 byte）= 8 位
   - 上图由许许多多的小格子组成，每一个格子里面只能放 1 或者 0，用它来判断 Y/N 状态。说的专业点，每一个个小格子就是一个个 bit
   - 由 0 和 1 状态表现的二进制位的 bit 数组
9. redis 位域（bitfield）
   - 通过 bitfield 命令可以一次性操作多个**比特位域（指的是连续的多个比特位）**，它会执行一系列操作并返回一个响应数组，这个数组中的元素对应参数列表中的相应操作的执行结果。
   - 说白了就是通过 bitfield 命令我们可以一次性对多个比特位域进行操作。
10. redis 流（Stream）
    - Redis Stream 是 Redis 5.0 版本新增加的数据结构。
    - Redis Stream 主要用于消息队列（MQ，Message Queue），Redis 本身是有一个 Redis 发布订阅（pub/sub）来实现消息队列的功能，但它有个缺点就是消息无法持久化，如果出现网络断开、Redis 宕机等，消息就会被丢弃。
    - 简单来说发布订阅（pub/sub）可以分发消息，但无法记录历史消息。
    - 而 Redis Stream 提供了消息的持久化和主备复制功能，可以让任何客户端访问任何时刻的数据，并且能记住每一个客户端的访问位置，还能保证消息不丢失

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P11

## 底层数据结构

- redis 6 类型-物理编码-对应表

  - （1：04）

    | 类型         | 编码                      | 对象                                             |
    | ------------ | ------------------------- | ------------------------------------------------ |
    | REDIS_STRING | REDIS_ENCODING_INT        | 使用整数值实现的字符串对象                       |
    | REDIS_STRING | REDIS_ENCODING_EMBSTR     | 使用 embstr 编码的简单动态字符串实现的字符串对象 |
    | REDIS_STRING | REDIS_ENCODING_RAW        | 使用简单动态字符串实现的字符串对象               |
    | REDIS_LIST   | REDIS_ENCODING_ZIPLIST    | 使用压缩列表实现的列表对象                       |
    | REDIS_LIST   | REDIS_ENCODING_LINKEDLIST | 使用双端链表实现的列表对象                       |
    | REDIS_HASH   | REDIS_ENCODING_ZIPLIST    | 使用压缩列表实现的哈希对象                       |
    | REDIS_HASH   | REDIS_ENCODING_HT         | 使用字典实现的哈希对象                           |
    | REDIS_SET    | REDIS_ENCODING_INTSET     | 使用整数集合实现的集合对象                       |
    | REDIS_SET    | REDIS_ENCODING_HT         | 使用字典实现的集合对象                           |
    | REDIS_ZSET   | REDIS_ENCODING_ZIPLIST    | 使用压缩列表实现的有序集合对象                   |
    | REDIS_ZSET   | REDIS_ENCODING_SKIPLIST   | 使用跳跃表和字典实现的有序集合对象               |

- redis 6 数据类型对应的底层数据结构

  - （1：57）

    1. 字符串

       **int**：8 个字节的长整型

       **embstr**：小于等于 44 个字节的字符串

       **raw**：大于 44 个字节的字符串

       - embstr 与 raw 类型底层的数据结构其实都是 SDS（简单动态字符串，Redis 内部定义 sdshdr 一种结构）。

    2. 哈希

       **ziplist**（压缩列表）：当哈希类型元素小于 hash-max-ziplist-entries 配置（默认 512 个）、同时所有值都小于 hash-max-ziplist-value 配置（默认 64 字节）时，Redis 会使用 ziplist 作为哈希的内部实现，ziplist 使用更加紧凑的结构实现多个元素的连续存储。所以在节省内存方面比 hashtable 更加优秀

       **hashtable**（哈希表）：当哈希类型无法满足 ziplist 的条件时，Redis 会使用 hashtable 作为哈希的内部实现，因为此时 ziplist 的读写效率会下降，而 hashtable 的读写时间复杂度为 O(1)

    3. 列表

       **ziplist**（压缩列表）：当列表的元素个数小于 list-max-ziplist-entries 配置（默认 512 个），同时列表中每个元素的值都小于 list-max-ziplist-value 配置时（默认 64 字节），Redis 会选用 ziplist 来作为列表的内部实现来减少内存的使用。

       **linkedlist**（链表）：当列表类型无法满足 ziplist 的条件时，Redis 会使用 linkedlist 作为列表的内部实现。

       3.2 版本之后只有使用：**quicklist**：ziplist 和 linkedlist 的结合以 ziplist 为节点的链表

    4. 集合

       **intset**（整数集合）：当集合中的元素都是整数且元素个数小于 set-max-intset-entries 配置（默认 512 个）时，Redis 会用 intset 来作为集合的内部实现，从而减少内存的使用。

       **hashtable**（哈希表）：当集合类型无法满足 intset 的条件时，Redis 会使用 hashtable 作为集合的内部实现

    5. 有序集合

       **ziplist**（压缩列表）：当有序集合的元素个数小于 zset-max-ziplist-entries 配置（默认 128 个），同时每个元素的值都小于 zset-max-ziplist-value 配置（默认 64 字节）时，Redis 会用 ziplist 来作为有序集合的内部实现，ziplist 可以有效减少内存的使用

       **skiplist**（跳跃表）：当 ziplist 条件不满足时，有序集合会使用 skiplist 作为内部实现，因为此时 ziplist 的读写效率会下降

- redis 6 数据类型以及数据结构的关系（2：35）

- redis 7 数据类型以及数据结构的关系（2：52）

  - ziplist -> listpack（所有地方，包括 quicklist 内）

  - listpack 避免了 ziplist 的连锁更新的问题

- redis 数据类型以及数据结构的时间复杂度（3：24）

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P163

## 为什么快

1. Redis是基于内存操作，需要的时候需要我们手动持久化到硬盘中

   - Redis 是基于内存的数据库，不论读写操作都是在内存上完成的，完全吊打磁盘数据库的速度。Redis之所以可以使用单线程来处理，其中的一个原因是，内存操作对资源损耗较小，保证了处理的高效性。

2. Redis高效数据结构，对数据的操作也比较简单

3. Redis是单线程模型，从而避开了多线程中上下文频繁切换的操作

4. 使用多路I/O复用模型，非阻塞I/O

   - Redis 采用 I/O 多路复用技术，并发处理连接。采用了 epoll + 自己实现的简单的事件框架。epoll 中的读、写、关闭、连接都转化成了事件，然后利用 epoll 的多路复用特性，绝不在 IO 上浪费一点时间。

   - |        | 并发连接限制                                | 内存拷贝                     | 活跃连接感知                                |
     | :----- | :------------------------------------------ | :--------------------------- | ------------------------------------------- |
     | select | 受fd限制，32位机默认1024个/64位机默认2048个 | 把fd集合从用户态拷贝到内核态 | 只能感知有fd就绪，但无法定位，需要遍历+轮询 |
     | poll   | 采用链表存储fd无最大并发连接数限制          | 同select                     | 同select，需遍历+轮询                       |
     | epoll  | 没有最大并发连接的限制                      | 共享内存，无需内存拷贝       | 基于event callback方式，只感知活跃连接      |

5. 使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求

   - （备注:**Redis的虚拟内存机制**是啥？ 虚拟内存机制就是暂时把不经常访问的数据从 内存交换到磁盘中，从而腾出宝贵的空间用于需要访问呢的数据（热数据））。
   - 通过VM功能可以实现冷热数据分离，是热数据人在内存中，冷数据保存在磁盘中，这样就 避免因为内存不足而造成访问数据下降的问题。

6. 异步多线程进行 RDB（bgsave）、AOF 重写、惰性删除，避免阻塞主线程

参考文档：

1. [Redis高性能原理：Redis为什么这么快？- CSDN](https://blog.csdn.net/CSDN2497242041/article/details/120755188)

## 单线程 / 多线程

- Redis 3.x 单线程时代但性能依旧很快的主要原因

  - 基于内存操作：Redis 的所有数据都存在内存中，因此所有的运算都是内存级别的，所以他的性能比较高；
  - 数据结构简单：Redis 的数据结构是专门设计的，而这些简单的数据结构的查找和操作的时间大部分复杂度都是 O(1)，因此性能比较高；
  - 多路复用和非阻塞 I/O：Redis 使用 I/O 多路复用功能来监听多个 socket 连接客户端，这样就可以使用一个线程连接来处理多个请求，减少线程切换带来的开销，同时也避免了 I/O 阻塞操作
  - 避免上下文切换：因为是单线程模型，因此就避免了不必要的上下文切换和多线程竞争，这就省去了多线程切换带来的时间和性能上的消耗，而且单线程不会导致死锁问题的发生

- **Redis 4.0 之前**一直采用单线程的主要原因有以下三个

  1. 使用单线程模型是 Redis 的开发和维护更简单，因为单线程模型方便开发和调试
  2. 即使使用单线程模型也并发的处理多客户端的请求，主要使用的是 IO 多路复用和非阻塞 IO
  3. 对于 Redis 系统来说，**主要的性能瓶颈是内存或者网络带宽而并非 CPU**.

- Redis 是基于内存操作的，**因此他的瓶颈可能是机器的内存或者网络带宽而并非 CPU**，既然 CPU 不是瓶颈，那么自然就采用单线程的解决方案了，况且使用多线程比较麻烦。**但是在 Redis 4.0 中开始支持多线程了，例如后台删除、备份等功能**。

  - RDB bgsave
  - AOF 重写
  - 惰性删除
    - 大 key 问题
    - 在 Redis 4.0 就引入了多个线程来实现数据的异步惰性删除等功能，但是其处理读写请求的仍然只有一个线程，所以仍然算是狭义上的单线程。

- **在 Redis 6/7 中，非常受关注的第一个新特性就是多线程**。

  - 随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 IO 的处理上，也就是说，单个主线程处理网络请求的速度跟不上底层网络硬件的速度

  - 为了应对这个问题：**采用多个 IO 线程来处理网络请求，提高网络请求处理的并行度，Redis 6/7 就是采用的这种方法**。

  - 但是，Redis 的多 IO 线程只是用来处理网络请求的，**对于读写操作命令 Redis 仍然使用单线程来处理**

    - 这是因为，Redis 处理请求时，网络处理经常是瓶颈，通过多个 IO 线程并行处理网络操作，可以提升实例的整体处理性能。而继续使用单线程执行命令操作，就不用为了保证 Lua 脚本、事务的原子性，额外开发多线程**互斥加锁机制了（不管加锁操作处理）**，这样一来，Redis 线程模型实现就简单了。

  - 主线程和 IO 线程四个阶段

    - **阶段一：服务端和客户端建立 Socket 连接，并分配处理线程**

      首先，主线程负责接收建立连接请求。当有客户端请求和实例建立 Socket 连接时，主线程会创建和客户端的连接，并把 Socket 放入全局等待队列中。紧接着，主线程通过轮询方法把 Socket 连接分配给 IO 线程。

    - **阶段二：IO 线程读取并解析请求**

      主线程一旦把 Socket 分配给 IO 线程，就会进入阻塞状态，等待 IO 线程完成客户端请求读取和解析。因为有多个 IO 线程在并行处理。所以，这个过程很快就可以完成。

    - **阶段三：主线程执行请求操作**

      等到 IO 线程解析完请求，主线程还是会以单线程的方式执行这些命令操作。

    - **阶段四：IO 线程回写 Socket 和主线程清空全局队列**

      当主线程执行完请求操作后，会把需要返回的结果写入缓冲区，然后，主线程会阻塞等待 IO 线程，把这些结果回写到 Socket 中，并返回给客户端。和 IO 线程读取和解析请求一样，IO 线程回写 Socket 时，也是有多个线程在并发执行，所以回写 Socket 的速度也很快。等到 IO 线程回写 Socket 完毕，主线程会清空全局队列，等待客户端和后续请求。

- Redis 采用 **Reactor 模式**的网络模型，对于一个客户端请求，主线程负责一个完整的处理过程：

  读取 socket -> 解析请求 -> 执行操作 -> 写入 socket

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P101 ~ 102

## 持久化（RDB、AOF）

- RDB（Redis DataBase）

  - **默认开启**

  - 自动触发

    - Redis 6.0.16 以下在 Redis.conf 配置文件中的 SNAPSHOTTING 下配置 save 参数，来触发 Redis 的 RDB 持久化条件，比如 “save m n”：表示 m 秒内数据集存在 n 次修改时，自动触发 bgsave

      - save 900 1：每隔 900s（15min），如果有超过 1 个 key 发生了变化，就写一份新的 RDB 文件

        save 300 10：每隔 300s（5min），如果有超过 10 个 key 发生了变化，就写一份新的 RDB 文件

        save 60 10000：每隔 60s（1min），如果有超过 10000 个 key 发生了变化，就写一份新的 RDB 文件

    - Redis 7 版本，按照 redis.conf 里配置的 `save <seconds> <changes>`

  - 手动触发

    - save（阻塞） 和 bgsave（默认，fork 子线程来持久化）

  - 如何恢复

    - 将备份文件（dump.rdb）移动到 redis 安装目录并启动服务即可

  - 如何检查修复 dump.rdb 文件

    - `redis-check-rdb /myredis/dumpfiles/dump6379.rdb`

- AOF（Append Only File）

  - **默认不开启**

  - 三种写回策略

    - | 配置项   | 写回时机           | 优点                     | 缺点                             |
      | -------- | ------------------ | ------------------------ | -------------------------------- |
      | Always   | 同步写回           | 可靠性高，数据基本不丢失 | 每个写命令都要落盘，性能影响较大 |
      | Everysec | 每秒写回           | 性能适中                 | 宕机时丢失 1 秒内的数据          |
      | No       | 操作系统控制的写回 | 性能好                   | 宕机时丢失数据较多               |

  - 重写机制

    - AOF 文件重写并不是对原文件进行重新整理，而是直接读取服务器现有的键值对，然后用一条命令去代替之前记录这个键值对的多条命令，生成一个新的文件后去替换原来的 AOF 文件

    - 自动触发

      - auto-aof-rewrite-percentage 100

        auto-aof-rewrite-min-size 64mb

        注意，**同时满足，且的关系**才会触发

    - 手动触发

      - 客户端向服务器发送 bgrewriteaof 命令

  - 异常修复命令：`redis-check-aof --fix` 进行修复

- RDB-AOF 混合持久化

  - 设置 `aof-use-rdb-preamble` 的值为 yes
  - RDB 镜像做全量持久化，AOF 做增量持久化

- 同时关闭 RDB + AOF

  - `save ""`
    - 禁用 rdb
    - 禁用 rdb 持久化模式下，我们仍然可以使用命令 save、bgsave 生成 rdb 文件
  - `appendonly no`
    - 禁用 aof
    - 禁用 aof 持久化模式下，我们仍然可以使用命令 bgrewriteaof 生成 aof 文件

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P28 ~ 46

## keys * 的问题 / 引出 scan

- 大批量往 redis 里面插入 2000W 测试数据 key

  - Linux Bash 下面执行，插入 100W

    - ```bash
      # 生成 100W 条 redis 批量设置 kv 的语句（key=kn, value=vn）写入到 /tmp 目录下的 redisTest.txt 文件中
      for((i=1;i<=100*10000;i++)); do echo "set k$i v$i" >> /tmp/redisTest.txt ;done;
      ```

  - 通过 redis 提高的管道 --pipe 命令插入 100W 大批量数据

    - 结合自己机器的地址

      ```shell
      cat /tmp/redisTest.txt | /opt/redis-7.0.0/src/redis-cli -h 127.0.0.1 -p 6379 -a 111111 --pipe
      redis-cli -a 111111 dbsize
      ```

      100w 数据插入 redis 花费 5.8 秒左右

- 某快递巨头真实生产案例新闻

  - 新闻

    - 对 Redis 稍微有点使用经验的人都知道线上是不能执行 keys * 相关命令的，虽然其模糊匹配功能使用非常方便也很强大，在小数据量情况下使用没什么问题，数据量大会导致 Redis 锁住及 CPU 飙升，在生产环境建议禁用或者重命名！

  - keys * 你试试 100W 花费多少秒遍历查询

    - keys * （34.74s） flushdb (2.16s)

    - keys * 这个指令有致命的弊端，在实际环境中最好不要使用

      > 这个指令没有 offset、limit 参数，是要一次性吐出所有满足条件的 key，由于 redis 是单线程的，其所有操作都是原子的，而 keys 算法是遍历算法，复杂度是 O(n)，如果实例中有千万级以上的 key，这个指令就会导致 Redis 服务卡顿，所有读写 Redis 的其它的指令都会被延后甚至会超时报错，可能会引起缓存雪崩甚至数据库宕机

  - 生产上限制 keys */flushdb/flushall 等危险命令以防止误删误用？

    - 通过配置设置禁用这些命令，redis.conf 在 SECURITY 这一项中

      - ```
        rename-command keys ""
        rename-command flushdb ""
        rename-command flushall ""
        ```

- 不用 keys * 避免卡顿，那该用什么

  - scan 命令登场

    - 一句话，类似 mysql limit 的**但不完全相同**

  - Scan 命令用于迭代数据库中的数据库键

    - 语法

      - SCAN cursor [MATCH pattern] [COUNT count]

        基于游标的迭代器，需要基于上一次的游标延续之前的迭代过程

        以 0 作为游标开始一次新的迭代，直到命令返回游标 0 完成一次遍历

        **不保证每次执行都返回某个给定数量的元素，支持模糊查询**

        一次返回的数量不可控，只能是大概率符合 count 参数

    - 特点

      - redis Scan 命令基本语法如下：

        SCAN cursor [MATCH pattern] [COUNT count]

        - cursor - 游标
        - pattern - 匹配的模式
        - count - 指定从数据集里返回多少元素，默认值为 10

        SCAN 命令是一个基于游标的迭代器，每次被调用之后，都会向用户返回一个新的游标，**用户在下次迭代时需要使用这个新游标作为 SCAN 命令的游标参数**，以此来延续之前的迭代过程。

        SCAN 返回一个包含**两个元素的数组**，第一个元素是用于进行下一次迭代的新游标，第二个元素则是一个数组，这个数组中包含了所有被迭代的元素。**如果新游标返回零表示迭代已结束。**

        SCAN 的遍历顺序**非常特别，它不是从第一维数组的第零位一直遍历到末尾，而是采用了高位进位加法来遍历。之所以使用这样特殊的方式进行遍历，是考虑到字典的扩容和缩容时避免槽位的遍历重复和遗漏**

    - 使用

      - ```
        127.0.0.1:6379> keys *
        1) "db_number"
        2) "key1"
        3) "myKey"
        127.0.0.1:6379> scan 0 MATCH * COUNT 1
        1) "2"
        2) 1) "db_number"
        127.0.0.1:6379> scan 2 MATCH * COUNT 1
        1) "1"
        2) 1) "myKey"
        127.0.0.1:6379> scan 1 MATCH * COUNT 1
        1) "3"
        2) 1) "key1"
        127.0.0.1:6379> scan 3 MATCH * COUNT 1
        1) "0"
        2) (empty list or set)
        ```

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P105

## Big Key 问题

- 多大算 Big

  - 参考《阿里云 Redis 开发规范》

    - 【强制】：拒绝 bigkey（防止网卡流量、慢查询）

      **string 类型控制在 10KB 以内，hash、list、set、zset 元素个数不要超过 5000**.

      反例：一个包含 200 万个元素的 list

      **非字符串的 bigkey，不要使用 del 删除，使用 hscan、sscan、zscan 方式渐进式删除，同时要注意防止 bigkey 过期时间自动删除问题**（例如一个 200 万的 zset 设置 1 小时过期，会触发 del 操作，造成阻塞，而且该操作不会出现在慢查询中（lantency 可查））

  - string 和二级结构

    - string 是 value，最大 512MB 但是 >= 10KB 就是 bigkey
    - list、hash、set 和 zset，个数超过 5000 就是 bigkey
      - 疑问？？？
        - list
          - 一个列表最多可以包含 2^32-1 个元素（4294967295，每个列表超过 40 亿个元素）。
        - hash
          - Redis 中每个 hash 可以存储 2^32 - 1 键值对（40 多亿）
        - set
          - 集合中最大的成员数为 2^32 - 1（4294967295，每个集合可存储 40 多亿个成员）。
        - ...

- 哪些危害

  - 内存不均，集群迁移困难
  - 超时删除，大 key 删除作梗
  - 网络流量阻塞

- 如何产生

  - 社交类
    - 王心凌粉丝列表，典型案例粉丝逐步递增
  - 汇总统计
    - 某个报表，月日年经年累月的积累

- 如何发现

  - **redis-cli --bigkeys**

    - **好处，见最下面总结**

      给出**每种数据结构 Top 1 bigkey**，同时给出**每种数据类型的键值个数 + 平均大小**

      **不足**

      想查询大于 10kb 的所有 key，--bigkeys 参数就无能为力了，需要用到 memory usage 来计算每个键值的字节数

      redis-cli --bigkeys -a 111111

      redis-cli -h 127.0.0.1 -p 6379 -a 111111 -bigkeys

      每隔 100 条 scan 指令就会休眠 0.1s，ops 就不会剧烈抬升，但是扫描的时间会变长

      redis-cli -h 127.0.0.1 -p 7001 --bigkeys -i 0.1

  - **MEMORY USAGE 键**

    - 计算每个键值的字节数
    - 官网

- 如何删除

  - 参考《阿里云 Redis 开发规范》

    - 【强制】：拒绝 bigkey（防止网卡流量、慢查询）

      **string 类型控制在 10KB 以内，hash、list、set、zset 元素个数不要超过 5000.**

      反例：一个包含 200 万个元素的 list

      **非字符串的 bigkey，不要使用 del 删除，使用 hscan、sscan、zscan 方式渐进式删除，同时要注意防止 bigkey 过期时间自动删除问题**（例如一个 200 万的 zset 设置 1 小时过期，会触发 del 操作，造成阻塞，而且该操作不会出现在慢查询中（lantency 可查））

  - 官网

  - 普通命令

    - String

      - 一般用 del，如果过于庞大 unlink

    - hash

      - 使用 hscan 每次获取少量 field-value，再使用 hdel 删除每个 field

      - 命令

        - Redis HSCAN 命令会用于迭代哈希表中的键值对

          **语法**

          redis HSCAN 命令基本语法如下：

          HSCAN key cursor [MATCH pattern] [COUNT count]

          - cursor - 游标
          - pattern - 匹配的模式
          - count - 指定从数据集里返回多少元素，默认值为 10

          **可用版本**

          大于等于 2.8.0

          **返回值**

          返回的每个元素都是一个元组，每一个元组元素由一个字段（field）和值（value）组成

      - 阿里手册

        - 1、Hash 删除：hsan + hdel

          ```java
          public void delBigHash(String host, int port, String password, String bigHashKey) {
              Jedis jedis = new Jedis(host, port);
              if (password != null && !"".equals(password)) {
                  jedis.auth(password);
              }
              ScanParams scanParams = new ScanParams().count(100);
              String cursor = "0";
              do {
                  ScanResult<Entry<String, String>> scanResult = jedis.hscan(bigHashKey, cursor, scanParams);
                  List<Entry<String, String>> entryList = scanResult.getResult();
                  if (entryList != null && !entryList.isEmpty()) {
                      for (Entry<String, String> entry : entryList) {
                          jedis.hdel(bigHashKey, entry.getKey());
                      }
                  }
                  cursor = scanResult.getStringCursor();
              } while (!"0".equals(cursor));
              
              // 删除 bigkey
              jedis.del(bigHashKey);
          }
          ```

    - list

      - 使用 ltrim 渐进式逐步删除，直到全部删除完成

      - 命令

        - Redis Ltrim 对一个列表进行修剪（trim），就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除。

          下标 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。你也可以使用负数下标，以 -1 表示列表的最后一个元素，-2 表示列表的倒数第二个元素，以此类推。

          **语法**

          redis Ltrim 命令基本语法如下：

          LTRIM KEY_NAME START STOP

          **可用版本**

          大于等于 1.0.0

          **返回值**

          命令执行成功时，返回 ok

      - 阿里手册

        - 2、List 删除：ltrim

          ```java
          public void delBigList(String host, int port, String password, String bigListKey) {
              Jedis jedis = new Jedis(host, port);
              if (password != null && !"".equals(password)) {
                  jedis.auth(password);
              }
              long llen =  jedis.llen(bigListKey);
              int counter = 0;
              int left = 100;
              while (counter < llen) {
                  // 每次从左侧截掉 100 个
                  jedis.ltrim(bigListKey, left, llen);
                  counter += left;
              }
              // 最终删除 key
              jedis.del(bigListKey);
          }
          ```

    - set

      - 使用 sscan 每次获取部分元素，再使用 srem 命令删除每个元素

      - 命令

      - 阿里手册

        - 3、Set 删除：sscan + srem

          ```java
          public void delBigSet(String host, int port, String password, String bigSetKey) {
              Jedis jedis = new Jedis(host, port);
              if (password != null && !"".equals(password)) {
                  jedis.auth(password);
              }
              ScanParams scanParams = new ScanParams().count(100);
              String cursor = "0";
              do {
                  ScanResult<String> scanResult = jedis.sscan(bigSetKey, cursor, scanParams);
                  List<String> memberList = scanResult.getResult();
                  if (memberList != null && !memberList.isEmpty()) {
                      for (String member : memberList) {
                          jedis.srem(bigSetKey, member);
                      }
                  }
                  cursor = scanResult.getStringCursor();
              } while (!"0".equals(cursor));
              
              // 删除 bigkey
              jedis.del(bigHashKey);
          }
          ```

    - zset

      - 使用 zscan 每次获取部分元素，再使用 ZREMRANGEBYRANK 命令删除每个元素

      - 命令

      - 阿里手册

        - ```java
          public void delBigZset(String host, int port, String password, String bigZsetKey) {
              Jedis jedis = new Jedis(host, port);
              if (password != null && !"".equals(password)) {
                  jedis.auth(password);
              }
              ScanParams scanParams = new ScanParams().count(100);
              String cursor = "0";
              do {
                  ScanResult<Tuple> scanResult = jedis.zscan(bigZsetKey, cursor, scanParams);
                  List<Tuple> tupleList = scanResult.getResult();
                  if (tupleList != null && !tupleList.isEmpty()) {
                      for (Tuple tuple : tupleList) {
                          jedis.srem(bigZsetKey, tuple.getElement());
                      }
                  }
                  cursor = scanResult.getStringCursor();
              } while (!"0".equals(cursor));
              
              // 删除 bigkey
              jedis.del(bigHashKey);
          }
          ```

BigKey 生产调优

- redis.conf 配置文件 LAZY FREEING 相关说明

  - 阻塞和非阻塞删除命令

    - Redis 有两个原语来删除键。一种称为 **DEL，是对象的阻塞删除**。这意味着服务器停止处理新命令，以便以同步方式回收与对象关联的所有内存。如果删除的键与一个小对象相关联的所有内存。如果删除的键与一个小对象相关联，则执行 DEL 命令所需的时间非常短，可与大多数其它命令相媲美

      Redis 中的 O(1) 或 O(log_N) 命令。但是，如果键与包含数百万个元素的聚合值相关联，则服务器可能会阻塞很长时间（甚至几秒钟）才能完成操作。

      基于上述原因，Redis 还提供了非阻塞删除原语，例如 **UNLINK（非阻塞 DEL）**以及 **FLUSHALL 和 FLUSHDB 命令的 ASYNC 选项**，以便在后台回收内存。这些命令在恒定时间内执行。另一个线程将尽可能快地逐步释放后台中的对象。

      FLUSHALL 和 FLUSHDB 的 DEL、UNLINK 和 ASYNC 选项是用户控制的。这取决于应用程序的设计，以了解何时使用其中一个是个好主意。然而，作为其它操作的副作用，Redis 服务器有时不得不删除键或刷新整个数据库。具体而言，Redis 在以下场景中独立于用户调用删除对象

  - 优化配置

    - ```
      lazyfree-lazy-server-del no 改为 Yes
      replica-lazy-flush no 改为 Yes
      
      lazyfree-lazy-user-del no 改为 Yes
      ```

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P106

## 缓存双写一致性

- 反馈回来的面试题

  - 一图

    - 通用查询方法三部曲

      1 OK，直接从 redis 获得并返回

      2 next，redis 无，从 mysql 获得并返回

      3 完成第 2 步同时，讲 mysql 数据回写 redis，两边数据一致

      问题，上面业务逻辑你用 Java 代码如何写

  - 你只要用缓存，就可能会涉及到 redis 缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？

  - 双写一致性，你先动缓存 redis 还是数据库 mysql 哪一个？why？

  - **延时双删**你做过吗？会有哪些问题？

  - 有这么一种情况 ，微服务查询 redis 无 mysql 有，为保证数据双写一致性回写 redis 你需要注意什么？**双检加锁**策略你了解过吗？如何尽量避免缓存击穿？

  - redis 和 mysql 双写 100% 会出纰漏，做不到强一致性，你如何保证**最终一致性**？

  - ……

- 缓存双写一致性，谈谈你的理解

  - 如果 redis 中**有数据**

    - 需要和数据库中的值相同

  - 如果 redis 中**无数据**

    - 数据库中的值要是最新值，且准备回写 redis

  - 缓存按照操作来分，细分 2 种

    - **只读缓存**
    - 读写缓存
      - **同步直写策略**
        - 写数据库后也同步写 redis 缓存，缓存和数据库中的数据一致；
        - 对于读写缓存来说，要想保证缓存和数据库中的数据一致，就要采用同步直写策略
      - **异步缓写策略**
        - 正常业务运行中，mysql 数据变动了，但是可以在业务上容许出现一定时间后才作用于 redis，比如仓库、物流系统
        - 异常情况出现了，不得不将失败的动作重新修补，有可能需要借助 kafka 或者 RabbitMQ 等消息中间件，实现重试重写

  - 一图代码你如何写

    - 问题》》》？

    - 采用双检加锁策略

      - ```java
        public String get(String key) {
            String value = redis.get(key); // 查询缓存
            if (value != null) {
                // 缓存存在直接返回
                return value;
            } else {
                // 缓存不存在则对方法加锁
                // 假设请求量很大，缓存过期
                synchronized (TestFuture.class) {
                    value = redis.get(key); // 再查一遍 redis
                    if (value != null) {
                        // 查到数据直接返回
                        return value;
                    } else {
                        // 二次查询缓存也不存在，直接查 DB
                        value = dao.get(key);
                        // 数据缓存
                        redis.setnx(key, value, time);
                        // 返回
                        return value;
                    }
                }
            }
        }
        ```

    - Code

- 数据库和缓存一致性的几种更新策略

  - 目的

    - 总之，我们要达到最终一致性！

      - **给缓存设置过期时间，定期清理缓存并回写，是保证最终一致性的解决方案。**

        我们可以对存入缓存的数据设置过期时间，所有的**写操作以数据库为准**，对缓存操作只是尽最大努力即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存，达到一致性，**切记，要以 mysql 的数据库写入库为准**。

        上述方案和后续落地案例是调研后的主流 + 成熟的做法，但是考虑到各个公司业务系统的差距，**不是 100% 绝对正确，不保证绝对适配全部情况，**请同学们自行酌情选择打法，合适自己的最好。

  - 可以停机的情况

    - 挂牌报错，凌晨升级，温馨提示，服务降级
    - 单线程，这样重量级的数据操作最好不要多线程

  - 我们讨论 4 种更新策略

    - X 先更新数据库，再更新缓存

      - 异常问题 1

        1. 先更新 mysql 的某商品的库存，当前商品的库存是 100，更新为 99 个
        2. 先更新 mysql 修改为 99 成功，然后更新 redis
        3. **此时假设异常出现**，更新 redis 失败了，这导致 mysql 里面的库存是 99 而 redis 里面的还是 100.
        4. 上述发生，会让数据库里面和缓存 redis 里面数据不一致，**读到 redis 脏数据**

      - 异常问题 2

        - 【先更新数据库，再更新缓存】，A、B 两个线程发起调用

          【正常逻辑】

          1. A update mysql 100
          2. A update redis 100
          3. B update mysql 80
          4. B update redis 80

          ============

          【异常逻辑】多线程环境下，A、B 两个线程有快有慢，有前有后有并行

          1. A update mysql 100
          2. B update mysql 80
          3. B update redis 80
          4. A update redis 100

          ============

          最终结果，mysql 和 redis 数据不一致，mysql 80，redis 100

    - X 先更新缓存，再更新数据库

      - X 不太推荐

        - 业务上一般把 mysql 作为**底单数据库**，保证最后解释

      - 异常问题 2

        - 【先更新数据库，再更新缓存】，A、B 两个线程发起调用

          【正常逻辑】

          1. A update redis 100
          2. A update mysql 100
          3. B update redis 80
          4. B update mysql 80

          ============

          【异常逻辑】多线程环境下，A、B 两个线程有快有慢，有前有后有并行

          1. A update redis 100
          2. B update redis 80
          3. B update mysql 80
          4. A update mysql 100

          ============

          --- mysql 100，redis 80

    - X 先删除缓存，再更新数据库

      - 异常问题

        - 步骤分析 1，先删除缓存，再更新数据库

          - （10:40）

            阳哥自己这里写 20 秒，是自己乱写的，表示更新数据库可能失败，实际中不可能，哈哈~

            1 A 线程先成功删除了 redis 里面的数据，然后去更新 mysql，此时 mysql 正在更新中，还没有结束。（比如网络延时）**B 突然出现要来读取缓存数据**。

        - 步骤分析 2，先删除缓存，再更新数据库（13:25）

        - 步骤分析 3，先删除缓存，再更新数据库（14:03）

        - 上面 3 步骤串讲梳理（16:30）

      - 解决方案

        - 采用**延时双删策略**（20:39）

        - 双删方案面试题

          - 这个删除该休眠多久呢？

            - （22:14）

              **第一种方法：**加百毫秒即可

              **第二种方法：**新启动一个后台监控程序

          - 这种同步淘汰策略，吞吐量降低怎么办？

            - （23:38）

              ```java
              CompletableFuture.supplyAsync(() -> {
                  
              }).whenComplete((t, u) -> {
                  
              }).exceptionally(e -> {
                  
              }).get();
              ```

          - 后续看门狗 WatchDog 源码分析

    - **先更新数据库，再删除缓存**

      - 异常问题（27:59）

      - 业务指导思想

        - 微软云
        - 我们后面的阿里巴巴 Canal 也是类似的思想
          - 上述的订阅 binlog 程序在 mysql 中有现成的中间件叫 canal，可以完成订阅 binlog 日志的功能。

      - 解决方案

        - （33:43）

          暂存到消息队列中

          没成功删除，从消息队列中重新读取

          成功删除，从消息队列中去除

          重试超过一定次数，向业务层发送报错信息

      - 类似经典的分布式事务问题，只有一个权威答案

        - 最终一致性
          - 流量充值，先下发短信实际充值可能滞后 5 分钟，可以接受
          - 电商发货，短信下发但是物流明天见

  - 小总结

    - 如何选择方案？利弊如何（36:56）
    - 一图总结（39:11）


参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P107 ~ 109

## 缓存预热、雪崩、穿透、击穿

- 缓存预热（5:37）

  - @PostConstruct 初始化白名单数据

- 缓存雪崩

  - 发生
    - redis 主机挂了，Redis 全盘崩溃，偏硬件运维
    - redis 中有大量 key 同时过期大面积失效，偏软件开发
  - 预防 + 解决
    - redis 中 key 设置为永不过期 or 过期时间错开
    - redis 缓存集群实现高可用
      - 主从 + 哨兵
      - Redis Cluster
      - 开启 Redis 持久化机制 aof/rdb，尽快恢复缓存集群
    - 多缓存结合预防雪崩
      - ehcache 本地缓存 + redis 缓存
    - 服务降级
      - Hystrix 或者阿里 sentinel 限流 & 降级（10:03）
    - **人民币玩家**
      - 阿里云-云数据库 Redis 版

- 缓存穿透

  - 是什么
    - 请求去查询一条记录，先查 redis 无，后查 mysql 无，**都查询不到该条记录，**但是请求每次都会打到数据库上去，导致后台数据库压力暴增，这种现象我们称为缓存穿透，这个 redis 变成了一个摆设……
    - 简单说就是本来无一物，两库都没有。既不在 Redis 缓存库，也不在 mysql，数据库存在被多次暴击风险
  - 解决
    - 缓存穿透 | 恶意攻击 | 空对象缓存、bloomfilter 过滤器
    - 一图（16:11）
    - 方案1：空对象缓存或者缺省值
      - 一般 OK（17:48）
      - But
        - 黑客或者恶意攻击
          - 黑客会对你的系统进行攻击，拿一个不存在的 id 去查询数据，会产生大量的请求到数据库去查询。可能会导致你的数据库由于压力过大而宕掉
          - key 相同打你系统
            - 第一次打到 mysql，空对象缓存后第二次就返回 defaultNull 缺省值，避免 mysql 被攻击，不用再到数据库中去走一圈了
          - **key 不同打你系统**
            - 由于存在空对象缓存和缓存回写（看自己业务不限死），redis 中的无关紧要的 key 也会越写越多**（记得设置 redis 过期时间）**
    - 方案2：Google 布隆过滤器 Guava 解决缓存穿透

- 缓存击穿

  - 是什么
    - 大量的请求同时查询一个 key 时，此时这个 key 正好失效了，就会导致大量的请求都打到数据库上面去
    - **简单说就是热点 key 突然失效了，暴打 mysql**
    - 备注
      - 穿透和击穿，截然不同
  - 危害
    - 会造成某一时刻数据库请求量过大，压力剧增
    - 一般技术部门需要知道**热点 key 是那些个**？做到心里有数防止击穿
  - 解决
    - 缓存击穿 | 热点 key 失效 | 互斥更新、随机退避、差异失效时间
    - 热点 key 失效
      - 时间到了自然清除但还没被访问到
      - delete 掉的 key，刚巧又被访问
    - 方案1：差异失效时间，对于访问频繁的热点 key，干脆就不设置过期时间
    - **方案2：互斥更新，采用双检加锁策略**（8:20）

- | 缓存问题     | 产生原因               | 解决方案                               |
  | ------------ | ---------------------- | -------------------------------------- |
  | 缓存更新方式 | 数据变更、缓存时效性   | 同步更新、失效更新、异步更新、定时更新 |
  | 缓存不一致   | 同步更新失败、异步更新 | 增加重试、补偿任务、最终一致           |
  | 缓存穿透     | 恶意攻击               | 空对象缓存、bloomfilter 过滤器         |
  | 缓存击穿     | 热点 key 失效          | 互斥更新、随机退避、差异失效时间       |
  | 缓存雪崩     | 缓存挂掉               | 快速失败熔断、主从模式、集群模式       |

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P124 ~ 127

## 布隆过滤器

- 原理

  - BloomFilter 的算法是，首先分配一块内存空间做 bit 数组，数组的 bit 位初始值全部设为 0。
  - 加入元素时，采用 k 个相互独立的 Hash 函数计算，然后将元素 Hash 映射的 K 个位置全部设置为 1。
  - 检测 key 是否存在，仍然用这 k 个 Hash 函数计算出 k 个位置，如果位置全部为 1，则表明 key 存在，否则不存在。

- Redis 实现

  - bitmaps

    - 基本命令

      - | 命令                        | 作用                                                  | 时间复杂度 |
        | --------------------------- | ----------------------------------------------------- | ---------- |
        | setbit key offset val       | 给指定 key 的值的第 offset 赋值 val                   | O(1)       |
        | getbit key offset           | 获取指定 key 的第 offset 位                           | O(1)       |
        | bitcount key start end      | 返回指定 key 中 [start, end] 中为 1 的数量            | O(n)       |
        | bitop operation destkey key | 对不同的二进制存储数据进行位运算（AND、OR、NOT、XOR） | O(n)       |

  - Redisson

    - Redis 实现布隆过滤器的底层就是通过 bitmap 这种数据结构，至于如何实现，这里就不重复造轮子了，介绍业界比较好用的一个客户端工具——Redisson。

    - ```java
      package com.ys.rediscluster.bloomfilter.redisson;
      
      import org.redisson.Redisson;
      import org.redisson.api.RBloomFilter;
      import org.redisson.api.RedissonClient;
      import org.redisson.config.Config;
      
      public class RedissonBloomFilter {
      
          public static void main(String[] args) {
              Config config = new Config();
              config.useSingleServer().setAddress("redis://192.168.14.104:6379");
              config.useSingleServer().setPassword("123");
              //构造Redisson
              RedissonClient redisson = Redisson.create(config);
      
              RBloomFilter<String> bloomFilter = redisson.getBloomFilter("phoneList");
              //初始化布隆过滤器：预计元素为100000000L,误差率为3%
              bloomFilter.tryInit(100000000L,0.03);
              //将号码10086插入到布隆过滤器中
              bloomFilter.add("10086");
      
              //判断下面号码是否在布隆过滤器中
              System.out.println(bloomFilter.contains("123456"));//false
              System.out.println(bloomFilter.contains("10086"));//true
          }
      }
      ```

    - 这是单节点的Redis实现方式，如果数据量比较大，期望的误差率又很低，那单节点所提供的内存是无法满足的，这时候可以使用分布式布隆过滤器，同样也可以用 Redisson 来实现

- 非 Redis 实现

  - Guava

    - ```java
      package com.ys.rediscluster.bloomfilter;
      
      import com.google.common.base.Charsets;
      import com.google.common.hash.BloomFilter;
      import com.google.common.hash.Funnel;
      import com.google.common.hash.Funnels;
      
      public class GuavaBloomFilter {
          public static void main(String[] args) {
              BloomFilter<String> bloomFilter = BloomFilter.create(Funnels.stringFunnel(Charsets.UTF_8),100000,0.01);
      
              bloomFilter.put("10086");
      
              System.out.println(bloomFilter.mightContain("123456"));
              System.out.println(bloomFilter.mightContain("10086"));
          }
      }
      ```

参考文档：

1. [Redis详解（十三）------ Redis布隆过滤器 -  博客园](https://www.cnblogs.com/ysocean/p/12594982.html)

## 过期删除策略

- 三种不同的删除策略：
  1. **立即删除**。在设置键的过期时间时，创建一个回调事件，当过期时间达到时，由时间处理器自动执行键的删除操作。
     - 立即删除能保证内存中数据的最大新鲜度，因为它保证过期键值会在过期后马上被删除，其所占用的内存也会随之释放。但是立即删除对cpu是最不友好的。因为删除操作会占用cpu的时间，如果刚好碰上了cpu很忙的时候，比如正在做交集或排序等计算的时候，就会给cpu造成额外的压力。
     - 而且目前redis事件处理器对时间事件的处理方式–无序链表，查找一个key的时间复杂度为O(n),所以并不适合用来处理大量的时间事件。
  2. **惰性删除**。键过期了就过期了，不管。每次从dict字典中按key取值时，先检查此key是否已经过期，如果过期了就删除它，并返回nil，如果没过期，就返回键值。
     - 惰性删除是指，某个键值过期后，此键值不会马上被删除，而是等到下次被使用的时候，才会被检查到过期，此时才能得到删除。所以惰性删除的缺点很明显:浪费内存。dict字典和expires字典都要保存这个键值的信息。
     - 举个例子，对于一些按时间点来更新的数据，比如log日志，过期后在很长的一段时间内可能都得不到访问，这样在这段时间内就要拜拜浪费这么多内存来存log。这对于性能非常依赖于内存大小的redis来说，是比较致命的。
  3. **定时删除**。每隔一段时间，对expires字典进行检查，删除里面的过期键。
     可以看到，第二种为被动删除，第一种和第三种为主动删除，且第一种实时性更高。
     - 从上面分析来看，立即删除会短时间内占用大量cpu，惰性删除会在一段时间内浪费内存，所以定时删除是一个折中的办法。
     - 定时删除是：每隔一段时间执行一次删除操作，并通过限制删除操作执行的时长和频率，来减少删除操作对cpu的影响。另一方面定时删除也有效的减少了因惰性删除带来的内存浪费。
- Redis 使用的策略
  - redis使用的过期键值删除策略是：**惰性删除 + 定期删除**，两者配合使用。

参考文档

1. [Redis过期键的删除策略 - CSDN](https://blog.csdn.net/ThinkWon/article/details/101522970)

## 内存淘汰策略

- redis 缓存淘汰策略

  - redis 配置文件（0：14）
  - lru 和 lfu 算法的区别是什么（1：30）
  - 有哪些（redis 7 版本）
    1. **noeviction**：不会驱逐任何 key，表示即使内存达到上限也不进行置换，所有能引起内存增加的命令都会返回 error
    2. **allkeys-lru**：对所有 key 使用 LRU 算法进行删除，优先删除掉最近最不经常使用的 key，用以保存新数据
    3. **volatile-lru**：对所有设置了过期时间的 key 使用 LRU 算法进行删除
    4. **allkeys-random**：对所有 key 随机删除
    5. **volatile-random**：对所有设置了过期时间的 key 随机删除
    6. **volatile-ttl**：删除马上要过期的 key
    7. **allkeys-lfu**：对所有 key 使用 LFU 算法进行删除
    8. **volatile-lfu**：对所有设置了过期时间的 key 使用 LFU 算法进行删除
  - 上面总结
    - 2 * 4 = 8
    - 2 个维度
      - 过期键中筛选，volatile
      - 所有键中筛选，allkeys
    - 4 个方面
      - LRU
        - 值得一提的是，这里的lru和平常我们所熟知的lru还不完全一样，**Redis使用的是采样概率的思想**，省略了双向链表的内存消耗。
        - Redis 会在每一次处理命令的时候判断是否达到了最大限制，如果达到则使用对应的算法去删除涉及到的Key，这时，我们前面所维护过键的LRU值就会派上用场了。
      - LFU
      - random
      - ttl
    - 8 个选项
  - 你平时用哪一种（8:33）
  - 如何配置、修改
    - 直接使用 config 命令
    - 直接 redis.conf 配置文件

- redis 缓存淘汰策略配置性能建议
  - 避免存储 bigkey
  - 开启惰性淘汰，lazyfree-lazy-eviction=yes

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P144

## 分布式锁（SETNX、Lua、Redisson、RedLock）

一把靠谱的分布式锁应该有哪些特征：

- **「互斥性」**: 任意时刻，只有一个客户端能持有锁。
- **「锁超时释放」**：持有锁超时，可以释放，防止不必要的资源浪费，也可以防止死锁。
- **「可重入性」**:一个线程如果获取了锁之后,可以再次对其请求加锁。
- **「高性能和高可用」**：加锁和解锁需要开销尽可能低，同时也要保证高可用，避免分布式锁失效。
- **「安全性」**：锁只能被持有的客户端删除，不能被其他客户端删除

方案：

- 方案一：SETNX + EXPIRE
  - 缺点
    - 但是这个方案中，`setnx`和`expire`两个命令分开了，**「不是原子操作」**。如果执行完`setnx`加锁，正要执行`expire`设置过期时间时，进程crash或者要重启维护了，那么这个锁就“长生不老”了，**「别的线程永远获取不到锁啦」**。
- 方案二：SETNX + value值是（系统时间+过期时间）
  - 缺点
    - 过期时间是客户端自己生成的（System.currentTimeMillis()是当前系统的时间），必须要求分布式环境下，每个客户端的时间必须同步。
    - 如果锁过期的时候，并发多个客户端同时请求过来，都执行 jedis.getSet()，最终只能有一个客户端加锁成功，但是该客户端锁的过期时间，可能被别的客户端覆盖
    - 该锁没有保存持有者的唯一标识，可能被别的客户端释放/解锁。
- 方案三：使用Lua脚本(包含SETNX + EXPIRE两条指令)
- 方案四：SET的扩展命令（SET EX PX NX）
  - 缺点
    - 问题一：**「锁过期释放了，业务还没执行完」**。假设线程a获取锁成功，一直在执行临界区的代码。但是100s过去后，它还没执行完。但是，这时候锁已经过期了，此时线程b又请求过来。显然线程b就可以获得锁成功，也开始执行临界区的代码。那么问题就来了，临界区的业务代码都不是严格串行执行的啦。
    - 问题二：**「锁被别的线程误删」**。假设线程a执行完后，去释放锁。但是它不知道当前的锁可能是线程b持有的（线程a去释放锁时，有可能过期时间已经到了，此时线程b进来占有了锁）。那线程a就把线程b的锁释放掉了，但是线程b临界区业务代码可能都还没执行完呢。
- 方案五：SET EX PX NX  + 校验唯一随机值,再释放锁
  - 缺点：
    - 方案五还是可能存在**「锁过期释放，业务没执行完」**的问题。
- 方案六: 开源框架~Redisson
  - 只要线程一加锁成功，就会启动一个`watch dog`看门狗，它是一个后台线程，会每隔10秒检查一下，如果线程1还持有锁，那么就会不断的延长锁key的生存时间。
  - 使用Redisson解决了**「锁过期释放，业务没执行完」**问题。
- 方案七：多机实现的分布式锁Redlock
  - 搞多个Redis master部署，以保证它们不会同时宕掉。并且这些master节点是完全相互独立的，相互之间不存在数据同步。同时，需要确保在这多个master实例上，是与在Redis单实例，使用相同方法来获取和释放锁。

参考文档：

1. [Redis实现分布式锁的7种方案，及正确使用姿势！ - 博客园](https://www.cnblogs.com/wangyingshuo/p/14510524.html)

## 部署方式（单机、主从复制、哨兵、集群）

- 单机模式
  - 优点
    - 架构简单，部署方便；
    - 高性价比：缓存使用时无需备用节点（单实例可用性可以用supervisor或crontab保证），当然为了满足业务的高可用性，也可以牺牲一个备用节点，但同时刻只有一个实例对外提供服务；
    - 高性能。
  - 缺点
    - 不保证数据的可靠性；
    - 在缓存使用，进程重启后，数据丢失，即使有备用的节点解决高可用性，但是仍然不能解决缓存预热问题，因此不适用于数据可靠性要求高的业务；
    - 高性能受限于单核CPU的处理能力（Redis是单线程机制），CPU为主要瓶颈，所以适合操作命令简单，排序、计算较少的场景。也可以考虑用Memcached替代。
- 主从模式（复制）
  - 优点
    - 高可靠性：一方面，采用双机主备架构，能够在主库出现故障时自动进行主备切换，从库提升为主库提供服务，保证服务平稳运行；另一方面，开启数据持久化功能和配置合理的备份策略，能有效的解决数据误操作和数据异常丢失的问题；
    - 读写分离策略：从节点可以扩展主库节点的读能力，有效应对大并发量的读操作。
  - 缺点
    - 故障恢复复杂，如果没有RedisHA系统（需要开发），当主库节点出现故障时，需要手动将一个从节点晋升为主节点，同时需要通知业务方变更配置，并且需要让其它从库节点去复制新主库节点，整个过程需要人为干预，比较繁琐；
    - 主库的写能力受到单机的限制，可以考虑分片；
    - 主库的存储能力受到单机的限制，可以考虑Pika；
    - 原生复制的弊端在早期的版本中也会比较突出，如：Redis复制中断后，Slave会发起psync，此时如果同步不成功，则会进行全量同步，主库执行全量备份的同时可能会造成毫秒或秒级的卡顿；又由于COW机制，导致极端情况下的主库内存溢出，程序异常退出或宕机；主库节点生成备份文件导致服务器磁盘IO和CPU（压缩）资源消耗；发送数GB大小的备份文件导致服务器出口带宽暴增，阻塞请求，建议升级到最新版本。
- 哨兵模式
  - 优点
    - Redis Sentinel集群部署简单；
    - 能够解决Redis主从模式下的高可用切换问题；
    - 很方便实现Redis数据节点的线形扩展，轻松突破Redis自身单线程瓶颈，可极大满足Redis大容量或高性能的业务需求；
    - 可以实现一套Sentinel监控一组Redis数据节点或多组数据节点。
  - 缺点
    - 部署相对Redis主从模式要复杂一些，原理理解更繁琐；
    - 资源浪费，Redis数据节点中slave节点作为备份节点不提供服务；
    - Redis Sentinel主要是针对Redis数据节点中的主节点的高可用切换，对Redis的数据节点做失败判定分为主观下线和客观下线两种，对于Redis的从节点有对节点做主观下线操作，并不执行故障转移。
    - 不能解决读写分离问题，实现起来相对复杂。
- 集群模式
  - 优点：
    - 无中心架构；
    - 数据按照slot存储分布在多个节点，节点间数据共享，可动态调整数据分布；
    - 可扩展性：可线性扩展到1000多个节点，节点可动态添加或删除；
    - 高可用性：部分节点不可用时，集群仍可用。通过增加Slave做standby数据副本，能够实现故障自动 failover，节点之间通过gossip协议交换状态信息，用投票机制完成Slave到Master的角色提升；
    - 降低运维成本，提高系统的扩展性和可用性。
  - 缺点：
    - Client实现复杂，驱动要求实现Smart Client，缓存slots mapping信息并及时更新，提高了开发难度，客户端的不成熟影响业务的稳定性。目前仅JedisCluster相对成熟，异常处理部分还不完善，比如常见的“max redirect exception”。
    - 节点会因为某些原因发生阻塞（阻塞时间大于clutser-node-timeout），被判断下线，这种failover是没有必要的。
    - 数据通过异步复制，不保证数据的强一致性。
    - 多个业务使用同一套集群时，无法根据统计区分冷热数据，资源隔离性较差，容易出现相互影响的情况。
    - Slave在集群中充当“冷备”，不能缓解读压力，当然可以通过SDK的合理设计来提高Slave资源的利用率。
    - Key批量操作限制，如使用mset、mget目前只支持具有相同slot值的Key执行批量操作。对于映射为不同slot值的Key由于Keys不支持跨slot查询，所以执行mset、mget、sunion等操作支持不友好。
    - Key事务操作支持有限，只支持多key在同一节点上的事务操作，当多个Key分布于不同的节点上时无法使用事务功能。
    - Key作为数据分区的最小粒度，不能将一个很大的键值对象如hash、list等映射到不同的节点。
      不支持多数据库空间，单机下的redis可以支持到16个数据库，集群模式下只能使用1个数据库空间，即db 0。
    - 复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构。
    - 避免产生hot-key，导致主库节点成为系统的短板。
    - 避免产生big-key，导致网卡撑爆、慢查询等。
    - 重试时间应该大于cluster-node-time时间。
    - Redis Cluster不建议使用pipeline和multi-keys操作，减少max redirect产生的场景。

参考文档：

1. [Redis的几种部署方式及持久化策略 - CSDN](https://blog.csdn.net/zwdwinter/article/details/88636397)

## 脑裂问题

- 哨兵+主从

- 原主服务器接受到客户端的信息后，还未同步到从服务器上就是去连接了，但是重启后又由主变为了从服务器，无法同步数据了，所以这部分数据就丢失了，这就是脑裂问题

- 解决办法

  - 在配置文件中添加如下配置

    ```
    min-slaves-to-write 1
    min-slaves-max-lag 10
    ```

  - 这种方法**不可能百分百避免数据丢失**

参考文档

1. [Redis常见问题——脑裂问题 - CSDN](https://blog.csdn.net/cxy_t/article/details/110825175)

## 分布式一致性协议（Raft、Gossip）

- Raft
  - Raft是一种为分布式系统设计的共识算法，用于管理复制日志的一致性。
  - 在Redis的Sentinel模式下，Raft协议被用于选举一个Leader Sentinel来执行自动故障转移操作。
  - 在Raft协议中，节点有三种状态
    - Leader（领导）
    - Follower（跟随者）
    - Candidate（候选人）。
  - 系统的时间被划分为一个个的Term（任期），每个Term开始时，所有节点都成为Follower。
  - 如果一个Follower在一段时间内没有收到来自Leader的心跳信息，它将转变为Candidate状态并发起选举。其他节点收到选举请求后，将比较自己的Term和请求中的Term，如果请求中的Term更大，则认可该Candidate为Leader。
- Gossip
  - 在Redis Cluster模式下，Gossip协议被用于实现无中心式的节点通信。
  - Gossip协议是一种最终一致性算法，它不要求节点知道所有其他节点的状态，因此具有去中心化的特点。
    - 节点之间通过交换信息来维护系统状态的一致性，虽然无法保证在某个时刻所有节点状态一致，但可以保证在“最终”所有节点一致。
  - 在实际应用中，Redis Cluster通过Gossip协议实现了节点间的动态信息交换，包括节点状态、槽位信息等。
    - 这种去中心化的通信方式使得Redis Cluster具有很高的可扩展性和容错性。
    - 当一个节点出现故障时，其他节点可以通过Gossip协议感知到这一变化，并自动调整集群状态，保证服务的可用性。
- 这两种协议如何协同工作
  - 在Redis的分布式环境中，Sentinel和Cluster模式可以同时使用。
    - Sentinel负责监控主从节点的健康状况，并在必要时进行故障转移。
    - 而Cluster则负责数据的分片存储和负载均衡。
  - 当Sentinel检测到主节点故障时，它会通过Raft协议选举出一个Leader Sentinel来执行故障转移操作。
  - 在这个过程中，Cluster中的节点通过Gossip协议保持通信，确保集群状态的一致性。

参考文档：

1. [Redis中的分布式一致性协议：Raft与Gossip的协同工作](https://cloud.baidu.com/article/3193422)

## 虚拟内存

- 应用场景

  - 在Redis中，有一个非常重要的概念，即keys一般不会被交换，所以如果你的数据库中有大量的keys，其中每个key仅仅关联很小的value，那么这种场景就不是非常适合使用虚拟内存。
  - 如果恰恰相反，**数据库中只是包含少量的keys，而每一个key所关联的value却非常大**，那么这种场景对于使用虚拟内存就非常合适了。
  - 在实际的应用中，为了能让虚拟内存更为充分的发挥作用以帮助我们提高系统的运行效率，我们可以将带有很多较小值的Keys合并为带有少量较大值的Keys。其中最主要的方法就是将原有的Key/Value模式改为基于Hash的模式，这样可以让很多原来的Keys成为Hash中的属性。

- 配置 Redis 虚拟内存

  - （1）在配置文件中添加以下配置项，以使当前Redis服务器在启动时**打开虚拟内存功能**。

    ```
    vm-enabled yes
    ```

  - （2）**在配置文件中设定Redis最大可用的虚拟内存字节数**。如果内存中的数据大于该值，则有部分对象被持久化到磁盘中，其中被持久化对象所占用的内存将被释放，直到已用内存小于该值时才停止持久化。

    ```
    vm-max-memory (bytes)
    ```

    Redis的交换规则是尽量考虑"最老"的数据，即最长时间没有使用的数据将被持久化。如果两个对象的 age 相同，那么Value较大的数据将先被持久化。需要注意的是，Redis不会将Keys持久化到磁盘，因此如果仅仅keys的数据就已经填满了整个虚拟内存，那么这种数据模型将不适合使用虚拟内存机制，或者是将该值设置的更大，以容纳整个Keys的数据。在实际的应用，如果考虑使用Redis虚拟内存，我们应尽可能的分配更多的内存交给Redis使用，以避免频繁的将数据持久化到磁盘上。

  - （3）**在配置文件中设定页的数量及每一页所占用的字节数**。为了将内存中的数据传送到磁盘上，我们需要使用交换文件。这些文件与数据持久性无关，Redis会在退出前会将它们全部删除。由于对交换文件的访问方式大多为随机访问，因此建议将交换文件存储在固态磁盘上，这样可以大大提高系统的运行效率。

    ```
    vm-pages 134217728
    vm-page-size 32
    ```

    在上面的配置中，Redis将需要持久化的文件划分为vm-pages个页，其中每个页所占用的字节为vm-page-size，那么Redis最终可用的交换文件大小为：vm-pages * vm-page-size。由于一个value可以存放在一个或多个页上，但是一个页不能持有多个value，鉴于此，我们在设置vm-page-size时需要充分考虑Redis的该特征。

  - （4）在Redis的配置文件中有一个非常重要的配置参数，即：

    ```
    vm-max-threads 4
    ```

    该参数表示Redis在**对交换文件执行IO操作时所应用的最大线程数量**。通常而言，我们推荐该值等于主机的CPU cores。如果将该值设置为0，那么Redis在与交换文件进行IO交互时，将以同步的方式执行此操作。

- **Redis 同步数据方式**

  - 对于Redis而言，如果操作交换文件是以同步的方式进行，那么当某一客户端正在访问交换文件中的数据时，其它客户端如果再试图访问交换文件中的数据，该客户端的请求就将被挂起，直到之前的操作结束为止。特别是在相对较慢或较忙的磁盘上读取较大的数据值时，这种阻塞所带来的影响就更为突兀了。
  - 然而同步操作也并非一无是处，事实上，从全局执行效率视角来看，同步方式要好于异步方式，毕竟同步方式节省了线程切换、线程间同步，以及线程拉起等操作产生的额外开销。特别是当大部分频繁使用的数据都可以直接从主内存中读取时，同步方式的表现将更为优异。
  - 至于最终选用哪种配置方式，最好的方式是不断的实验和调优。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景七：讲讲 Redis 的虚拟内存？

# Kafka

## 高可用

- Kafka 一个最基本的架构认识
  - 由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。
- 这就是**天然的分布式消息队列**，就是说一个 topic 的数据，是**分散放在多个机器上的，每个机器就放一部分数据**。
- 实际上 RabbmitMQ 之类的，并不是分布式消息队列，它就是传统的消息队列，只不过提供了一些集群、HA(High Availability, 高可用性) 的机制而已，因为无论怎么玩儿，RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。
- Kafka 0.8 以前，是没有 HA 机制的，就是任何一个 broker 宕机了，那个 broker 上的 partition 就废了，没法写也没法读，没有什么高可用性可言。
  - 比如说，我们假设创建了一个 topic，指定其 partition 数量是 3 个，分别在三台机器上。但是，如果第二台机器宕机了，会导致这个 topic 的 1/3 的数据就丢了，因此这个是做不到高可用的。
- Kafka 0.8 以后，提供了 HA 机制，就是 replica（复制品） 副本机制。
  - 每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。
  - 所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。
  - 写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader？很简单，**要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题**，系统复杂度太高，很容易出问题。Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。
- 这么搞，就有所谓的**高可用性**了，因为如果某个 broker 宕机了，没事儿，那个 broker上面的 partition 在其他机器上都有副本的。如果这个宕机的 broker 上面有某个 partition 的 leader，那么此时会从 follower 中**重新选举**一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了
  - **写数据**的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为）
  - **消费**的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - Kafka 的高可用

## 不重复消费（幂等性）

其实还是得结合业务来思考，我这里给几个思路：

- 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。
- 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。
- 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。
- 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - 如何保障消息不重复消费（幂等性）？

## 可靠传输（不丢失）

- **消费者丢数据**
  - 唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边**自动提交了 offset**，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。
  - 这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要**关闭自动提交** offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是**可能会有重复消费**，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。
  - 生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。
- **Kafka 丢数据**
  - 这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。
  - 生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。
  - 所以此时一般是要求起码设置如下 4 个参数：
    - 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。
    - 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。
    - 在 producer 端设置 acks=all ：这个是要求每条数据，必须是**写入所有 replica 之后，才能认为是写成功了**。
    - 在 producer 端设置 retries=MAX （很大很大很大的一个值，无限次重试的意思）：这个是**要求一旦写入失败，就无限重试**，卡在这里了。
  - 我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。
- **生产者丢数据**
  - 如果按照上述的思路设置了 acks=all ，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - Kafka 如何保障消息的可靠

## 顺序性

- 先看看顺序会错乱的场景：
  - **Kafka**：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。 消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。
  - 接着，我们在消费者里可能会搞**多个线程来并发处理消息**。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。
- **Kafka 解决方案**
  - 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。
  - 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - 如何保证消息的顺序性？ - Kafka 解决方案

## 消息堆积

- **大量消息在 mq 里积压了几个小时了还没解决**
  - 一个消费者一秒是 1000 条，一秒 3 个消费者是 3000 条，一分钟就是 18 万条。所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概 1 小时的时间才能恢复过来。
  - 一般这个时候，只能临时紧急扩容了，具体操作步骤和思路如下：
    - 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。
    - 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。
    - 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，**消费之后不做耗时的处理**，直接均匀轮询写入临时建立好的 10 倍数量的 queue。
    - 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。
    - 等快速消费完积压数据之后，**得恢复原先部署的架构**，**重新**用原先的 consumer 机器来消费消息。
- **mq 中的消息过期失效了**
  - 假设你用的是 RabbitMQ，RabbtiMQ 是可以设置过期时间的，也就是 TTL。如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在 mq 里，而是**大量的数据会直接搞丢**。
  - 这个情况下，就不是说要增加 consumer 消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是**批量重导**，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来。也只能是这样了。
  - 假设 1 万个订单积压在 mq 里面，没有处理，其中 1000 个订单都丢了，你只能手动写程序把那 1000 个订单给查出来，手动发到 mq 里去再补一次。
- **mq 都快写满了**
  - 如果消息积压在 mq 里，你很长时间都没有处理掉，此时导致 mq 都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，**消费一个丢弃一个，都不要了**，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - 如何处理消息堆积？

# RabbitMQ

## 概念 / 架构

- RabbitMQ 是一个由 Erlang 语言开发的 AMQP 的开源实现。

  - AMQP ：Advanced Message Queue，高级消息队列协议。它是应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，并不受产品、开发语言等条件的限制。

- RabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。具体特点包括：

  1. 可靠性（Reliability）：RabbitMQ 使用一些机制来保证可靠性，如持久化、传输确认、发布确认。
  2. 灵活的路由（Flexible Routing）：在消息进入队列之前，通过 Exchange 来路由消息的。对于典型的路由功能，RabbitMQ 已经提供了一些内置的 Exchange 来实现。针对更复杂的路由功能，可以将多个 Exchange 绑定在一起，也通过插件机制实现自己的 Exchange 。
  3. 消息集群（Clustering）：多个 RabbitMQ 服务器可以组成一个集群，形成一个逻辑 Broker 。
  4. 高可用（Highly Available Queues）：队列可以在集群中的机器上进行镜像，使得在部分节点出问题的情况下队列仍然可用。
  5. 多种协议（Multi-protocol）：RabbitMQ 支持多种消息队列协议，比如 STOMP、MQTT 等等。
  6. 多语言客户端（Many Clients）：RabbitMQ 几乎支持所有常用语言，比如 Java、.NET、Ruby 等等。
  7. 管理界面（Management UI）:RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方面。
  8. 跟踪机制（Tracing）:如果消息异常，RabbitMQ 提供了消息跟踪机制，使用者可以找出发生了什么。
  9. 插件机制（Plugin System）:RabbitMQ 提供了许多插件，来从多方面进行扩展，也可以编写自己的插件。

- **架构**

  - ```mermaid
    graph LR
    subgraph Broker
    	subgraph Virtual Host
    		Exchange --Binding--- Queue
    	end
    end
    Publish --- Exchange
    subgraph Connection
    	c1[Channel]
    	c2[Channel]
    	c3[Channel]
    end
    Queue --- c2 --- Consumer
    ```

  - **Message**

    - 消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括 routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。

  - **Publisher**

    - 消息的生产者，也是一个向交换器发布消息的客户端应用程序。

  - **Exchange**（将消息路由给队列）

    - 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。
    - 类型
      - Exchange 分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。
        - headers 匹配 AMQP 消息的 header 而不是路由键，此外 headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型
        - **Direct 键（routing key）分布**
          - Direct：消息中的路由键（routing key）如果和 Binding 中的 binding key 一致，交换器就将消息发到对应的队列中。它是完全匹配、单播的模式。
        - **Fanout（广播分发）**
          - Fanout：每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。
        - **topic 交换器（模式匹配）**
          - topic 交换器：topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符：符号“#”和符号“”。#匹配 0 个或多个单词，匹配不多不少一个单词。

  - **Binding**（消息队列和交换器之间的关联）

    - 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表

  - **Queue**

    - 消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。

  - **Connection**

    - 网络连接，比如一个 TCP 连接。

  - **Channel**

    - 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的 TCP 连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。

  - **Consumer**

    - 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。

  - **Virtual Host**

    - 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。

  - **Broker**

    - 表示消息队列服务器实体。

参考文档：

1. 《JAVA 核心知识点整理.pdf》13、RabbitMQ

## 高可用

- RabbitMQ的高可用是**基于主从**（非分布式）做高可用性。RabbitMQ 有三种模式：**单机模式（Demo 级别）、普通集群模式（无高可用性）、镜像集群模式（高可用性）**。
- **普通集群模式**
  - 普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你**创建的queue，只会放在一个 RabbitMQ 实例上**，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。
  - 这种方式确实很麻烦，也不怎么好，**没做到所谓的分布式**，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有**数据拉取的开销**，后者导致**单实例性能瓶颈**。
  - 而且如果那个放 queue 的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你**开启了消息持久化**，让 RabbitMQ 落地存储消息的话，**消息不一定会丢**，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。
  - 所以这个事儿就比较尴尬了，这就**没有什么所谓的高可用性**，**这方案主要是提高吞吐量的**，就是说让集群中多个节点来服务某个 queue 的读写操作。
- **镜像集群模式**
  - 这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会**存在于多个实例上**，就是说，每个 RabbitMQ 节点都有这个 queue 的一个**完整镜像**，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把**消息同步**到多个实例的 queue 上。
  - 那么**如何开启这个镜像集群模式**呢？其实很简单，RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是**镜像集群模式的策略**，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。
  - 好处
    - 你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。
  - 坏处
    - 第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！
    - 第二，这么玩儿，不是分布式的，就**没有扩展性可言**了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并**没有办法线性扩展**你的 queue。你想，如果这个 queue 的数据量很大，大到这个机器上的容量无法容纳了，此时该怎么办呢？

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - RabbitMQ 的高可用

## 可靠传输（不丢失）

- **生产者丢数据**
  - 生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。此时可以选择用 RabbitMQ 提供的事务功能，就是生产者**发送数据之前**开启 RabbitMQ 事务 channel.txSelect ，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务 channel.txRollback ，然后重试发送消息；如果收到了消息，那么可以提交事务 channel.txCommit 。
    - 但是问题是，RabbitMQ 事务机制（同步）一搞，基本上**吞吐量会下来，因为太耗性能**。
  - 所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。
  - 事务机制和 confirm 机制最大的不同在于，**事务机制是同步的**，你提交一个事务之后会**阻塞**在那儿，但是 confirm 机制是**异步**的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。
  - 所以一般在生产者这块**避免数据丢失**，都是用 confirm 机制的。
- **RabbitMQ 丢数据**
  - 就是 RabbitMQ 自己弄丢了数据，这个你必须**开启** **RabbitMQ** **的持久化**，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，**恢复之后会自动读取之前存储的数据**，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，**可能导致少量数据丢失**，但是这个概率较小。
  - 设置持久化有**两个步骤**：
    - 创建 queue 的时候将其设置为持久化 这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。
    - 第二个是发送消息的时候将消息的 deliveryMode 设置为 2 就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。
  - 必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。
  - 注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。
  - 所以，持久化可以跟生产者那边的 confirm 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 ack 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 ack ，你也是可以自己重发的。
- **消费者丢数据**
  - RabbitMQ 如果丢失了数据，主要是因为你消费的时候，**刚消费到，还没处理，结果进程挂了**，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。
  - 这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack ，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - RabbitMQ 如何保障消息的可靠

## 顺序性

- 先看看顺序会错乱的场景：
  - **RabbitMQ**：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。
- 解决方案
  - 拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - 如何保证消息的顺序性？ - Kafka 解决方案

# Nginx

## 负载均衡种类

- 根据负载均衡所作用在 OSI 模型的位置不同，负载均衡可以大概分为以下几类：
  - **二层负载均衡（mac）**
    - 根据 OSI 模型分的二层负载，一般是用虚拟 MAC 地址方式，外部对虚拟 MAC 地址请求，负载均衡接收后分配后端实际的 MAC 地址响应。
  - **三层负载均衡（ip）**
    - 一般采用虚拟 IP 地址方式，外部对虚拟的 IP 地址请求，负载均衡接收后分配后端实际的 IP 地址响应。
  - **四层负载均衡（tcp）**
    - 在三层负载均衡的基础上，用 ip+port 接收请求，再转发到对应的机器。
    - 四层负载均衡（基于IP+端口的负载均衡）
      - 所谓四层负载均衡，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。
      - 对应的负载均衡器称为四层交换机（L4 switch），主要分析 IP 层及 TCP/UDP 层，实现四层负载均衡。此种负载均衡器不理解应用协议（如 HTTP/FTP/MySQL 等等）。要处理的流量进行 NAT 处理，转发至后台服务器，并记录下这个 TCP 或者 UDP 的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。
      - 实现四层负载均衡的软件有：
        - F5：硬件负载均衡器，功能很好，但是成本很高
        - lvs：重量级的四层负载软件
        - nginx：轻量级的四层负载软件，带缓存功能，正则表达式较灵活
        - haproxy：模拟四层转发，较灵活
  - **七层负载均衡（http）**
    - 根据虚拟的 url 或 IP，主机名接收请求，再转向相应的处理服务器。
    - 七层的负载均衡（基于虚拟的 URL 或主机 IP 的负载均衡)
      - 所谓七层负载均衡，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。
      - 对应的负载均衡器称为七层交换机（L7 switch），除了支持四层负载均衡以外，还有分析应用层的信息，如 HTTP 协议 URI 或 Cookie 信息，实现七层负载均衡。此种负载均衡器能理解应用协议。
      - 实现七层负载均衡的软件有：
        - haproxy：天生负载均衡技能，全面支持七层代理，会话保持，标记，路径转移
        - nginx：只在 http 协议和 mail 协议上功能比较好，性能与 haproxy 差不多
        - apache：功能较差
        - Mysql proxy：功能尚可
- **在实际应用中，比较常见的就是四层负载及七层负载**。这里也重点说下这两种负载。

参考文档：

1. [负载均衡总结（四层负载与七层负载的区别）（转）- 知乎](https://zhuanlan.zhihu.com/p/64777456)

## 负载均衡算法

1. **轮询（默认）**

   - 每个请求按时间顺序逐一分配到不同的后端服务，如果后端某台服务器死机，自动剔除故障系统，使用户访问不受影响。

   - ```nginx
     upstream bakend {  
         server 192.168.0.1;    
         server 192.168.0.2;  
     }
     ```

2. **weight（轮询权值）**

   - weight的值越大分配到的访问概率越高，主要用于后端每台服务器性能不均衡的情况下。或者仅仅为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源。

   - 指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。

   - ```nginx
     upstream bakend {  
         server 192.168.0.1 weight=10;  
         server 192.168.0.2 weight=10;  
     }
     ```

3. **ip_hash**

   - 每个请求按访问IP的哈希结果分配，使来自同一个IP的访客固定访问一台后端服务器，并且可以有效解决动态网页存在的session共享问题。

   - 每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。

   - ```nginx
     upstream bakend {  
         ip_hash;  
         server 192.168.0.1:88;  
         server 192.168.0.2:80;  
     } 
     ```

4. **fair**（第三方）

   - 比 weight、ip_hash更加智能的负载均衡算法，fair 算法可以根据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间 来分配请求，响应时间短的优先分配。Nginx本身不支持fair，如果需要这种调度算法，则必须安装 upstream_fair 模块。

   - 按后端服务器的响应时间来分配请求，响应时间短的优先分配。

   - ```nginx
     upstream backend {  
         server 192.168.0.1:88;  
         server 192.168.0.2:80;  
         fair;  
     }
     ```

5. **url_hash**（第三方）

   - 按访问的URL的哈希结果来分配请求，使每个URL定向到一台后端服务器，可以进一步提高后端缓存服务器的效率。Nginx本身不支持url_hash，如果需要这种调度算法，则必须安装Nginx的hash软件包。

   - 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。

     > 注意：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法。

   - ```nginx
     upstream backend {  
         server 192.168.0.1:88;  
         server 192.168.0.2:80;  
         hash $request_uri;  
         hash_method crc32;  
     }
     ```

在Nginx upstream模块中，可以设定每台后端服务器在负载均衡调度中的状态，常用的状态有：

1. down，表示当前的server暂时不参与负载均衡
2. weight 默认为1，weight越大，负载的权重就越大。
3. backup，预留的备份机器。当其他所有的非backup机器出现故障或者忙的时候，才会请求backup机器，因此这台机器的访问压力最低
4. max_fails，允许请求失败的次数，默认为1，当超过最大次数时，返回proxy_next_upstream模块定义的错误。
5. fail_timeout，请求失败超时时间，在经历了max_fails次失败后，暂停服务的时间。max_fails和fail_timeout可以一起使用。

例如：

```nginx
upstream bakend{ 
      ip_hash; 
      server 192.168.0.1:90 down; 
      server 192.168.0.1:80 weight=2; 
      server 192.168.0.2:90; 
      server 192.168.0.2:80 backup; 
}
```

参考文档：

1. [nginx负载均衡的五种算法 - CSDN](https://blog.csdn.net/apple9005/article/details/79961391)

## 实现四层负载均衡

- 负载均衡可以分为**静态负载均衡**和**动态负载均衡**，接下来，我们就一起来分析下Nginx如何实现四层静态负载均衡和四层动态负载均衡。

- 静态负载均衡

  - Nginx的四层静态负载均衡需要启用ngx_stream_core_module模块，默认情况下，ngx_stream_core_module是没有启用的，需要在安装Nginx时，添加--with-stream配置参数启用，如下所示。

    - ```shell
      ./configure --prefix=/usr/local/nginx-1.17.2 --with-openssl=/usr/local/src/openssl-1.0.2s --with-pcre=/usr/local/src/pcre-8.43 --with-zlib=/usr/local/src/zlib-1.2.11 --with-http_realip_module --with-http_stub_status_module --with-http_ssl_module --with-http_flv_module --with-http_gzip_static_module --with-cc-opt=-O3 --with-stream --with-http_ssl_module
      ```

  - 配置四层负载均衡

    - 配置HTTP负载均衡时，都是配置在http指令下，配置四层负载均衡，则是在stream指令下，结构如下所示.

    - ```nginx
      stream {
          upstream mysql_backend {
          	......
          }
          server {
          	......
          }
      }
      ```

  - 配置 upstream

    - ```nginx
      upstream mysql_backend {
          server 192.168.175.201:3306 max_fails=2 fail_timeout=10s weight=1;
          server 192.168.175.202:3306 max_fails=2 fail_timeout=10s weight=1;
          least_conn;
      }
      ```

  - 配置 server

    - ```nginx
      server {
          #监听端口，默认使用的是tcp协议，如果需要UDP协议，则配置成listen 3307 udp;
          listen 3307;
          #失败重试
          proxy_next_upstream on;
          proxy_next_upstream_timeout 0;
          proxy_next_upstream_tries 0;
          #超时配置
          #配置与上游服务器连接超时时间，默认60s
          proxy_connect_timeout 1s;
          #配置与客户端上游服务器连接的两次成功读/写操作的超时时间，如果超时，将自动断开连接
          #即连接存活时间，通过它可以释放不活跃的连接，默认10分钟
          proxy_timeout 1m;
          #限速配置
          #从客户端读数据的速率，单位为每秒字节数，默认为0，不限速
          proxy_upload_rate 0;
          #从上游服务器读数据的速率，单位为每秒字节数，默认为0，不限速
          proxy_download_rate 0;
          #上游服务器
          proxy_pass mysql_backend;
      }
      ```

  - 配置完之后，就可以连接Nginx的3307端口，访问数据库了。

- 动态负载均衡

  - 配置Nginx四层静态负载均衡后，重启Nginx时，Worker进程一直不退出，会报错，如下所示。

    - `nginx: worker process is shutting down;`
    - 这是因为Worker进程维持的长连接一直在使用，所以无法退出，只能杀掉进程。可以使用Nginx的四层动态负载均衡解决这个问题。

  - 使用Nginx的四层动态负载均衡有两种方案：使用商业版的Nginx和使用开源的nginx-stream-upsync-module模块。

    - 注意：四层动态负载均衡可以使用nginx-stream-upsync-module模块，七层动态负载均衡可以使用nginx-upsync-module模块。

  - 使用如下命令为Nginx添加nginx-stream-upsync-module模块和nginx-upsync-module模块，此时，Nginx会同时支持四层动态负载均衡和HTTP七层动态负载均衡。

    - ```shell
      git clone https://github.com/xiaokai-wang/nginx-stream-upsync-module.git
      git clone https://github.com/weibocom/nginx-upsync-module.git
      git clone https://github.com/CallMeFoxie/nginx-upsync.git
      cp -r nginx-stream-upsync-module/* nginx-upsync/nginx-stream-upsync-module/
      cp -r nginx-upsync-module/* nginx-upsync/nginx-upsync-module/
      
      ./configure --prefix=/usr/local/nginx-1.17.2 --with-openssl=/usr/local/src/openssl-1.0.2s --with-pcre=/usr/local/src/pcre-8.43 --
      with-zlib=/usr/local/src/zlib-1.2.11 --with-http_realip_module --with-http_stub_status_module --with-http_ssl_module --with-http_flv_module --with-http_gzip_static_module --with-cc-opt=-O3 --with-stream --add-module=/usr/local/src/nginx-upsync --with-http_ssl_module
      ```

  - 配置四层负载均衡

    - 配置HTTP负载均衡时，都是配置在http指令下，配置四层负载均衡，则是在stream指令下，结构如下所示，

    - ```nginx
      stream {
          upstream mysql_backend {
          	......
          }
          server {
          	......
          }
      }
      ```

  - 配置 upstream

    - ```nginx
      upstream mysql_backend {
          server 127.0.0.1:1111; #占位server
          upsync 192.168.175.100:8500/v1/kv/upstreams/mysql_backend upsync_timeout=6m
          upsync_interval=500ms upsync_type=consul strong_dependency=off;
          upsync_dump_path /usr/local/nginx-1.17.2/conf/mysql_backend.conf;
      }
      ```

    - upsync指令指定从consul哪个路径拉取上游服务器配置；

    - upsync_timeout配置从consul拉取上游服务器配置的超时时间；

    - upsync_interval配置从consul拉取上游服务器配置的间隔时间；

    - upsync_type指定使用consul配置服务器；

    - strong_dependency配置nginx在启动时是否强制依赖配置服务器，如果配置为on，则拉取配置失败时Nginx启动同样失败。

    - upsync_dump_path指定从consul拉取的上游服务器后持久化到的位置，这样即使consul服务器出现问题，本地还有一个备份。

  - 配置 server

    - ```nginx
      server {
          #监听端口，默认使用的是tcp协议，如果需要UDP协议，则配置成listen 3307 udp;
          listen 3307;
          #失败重试
          proxy_next_upstream on;
          proxy_next_upstream_timeout 0;
          proxy_next_upstream_tries 0;
          #超时配置
          #配置与上游服务器连接超时时间，默认60s
          proxy_connect_timeout 1s;
          #配置与客户端上游服务器连接的两次成功读/写操作的超时时间，如果超时，将自动断开连接
          #即连接存活时间，通过它可以释放不活跃的连接，默认10分钟
          proxy_timeout 1m;
          #限速配置
          #从客户端读数据的速率，单位为每秒字节数，默认为0，不限速
          proxy_upload_rate 0;
          #从上游服务器读数据的速率，单位为每秒字节数，默认为0，不限速
          proxy_download_rate 0;
          #上游服务器
          proxy_pass mysql_backend;
      }
      ```

  - 从 Consul 添加上游服务器

    - ```shell
      curl -X PUT -d "{\"weight\":1, \"max_fails\":2, \"fail_timeout\":10}"
      http://192.168.175.100:8500/v1/kv/upstreams/mysql_backend/192.168.175.201:3306
      curl -X PUT -d "{\"weight\":1, \"max_fails\":2, \"fail_timeout\":10}"
      http://192.168.175.100:8500/v1/kv/upstreams/mysql_backend/192.168.175.202:3306
      ```

  - 从 Consul 删除上游服务器

    - ```shell
      curl -X DELETE
      http://192.168.175.100:8500/v1/kv/upstreams/mysql_backend/192.168.175.202:3306
      ```

  - 配置 upstream_show

    - ```nginx
      server {
      	listen 13307;
      	upstream_show;
      }
      ```

    - 配置upstream_show指令后，可以通过curl http://192.168.175.100:13307/upstream_show查看当前动态负载均衡上游服务器列表。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十三：讲讲 Nginx 如何实现四层负载均衡？

## 配置静态文件访问

- 一.首先安装好nginx,启动nginx服务且能够正常访问 Welcome to nginx!界面

- 二.配置静态资源访问核心是配置nginx.conf文件，找到nginx.conf文件

- 三.配置nginx.conf

  - 3.1.在nginx.conf的http节点中添加配置，参考下方格式：

    ```nginx
    server {
        listen       8000;
        listen       somename:8080;
        server_name  somename  alias  another.alias;
    
        location / {
            root   html;
            index  index.html index.htm;
        }
    }
    ```

  - 3.2 解读server节点各参数含义

    - listen：代表nginx要监听的端口
    - server_name:代表nginx要监听的域名
    - location ：nginx拦截路径的匹配规则
    - location块：location块里面表示已匹配请求需要进行的操作

- 四.实例

- 五.重点

  - 重点是理解alias与root的区别，root与alias主要区别在于nginx如何解释location后面的uri，这使两者分别以不同的方式将请求映射到服务器文件上。

  - alias（别名）是一个目录别名。

    ```nginx
    location /123/abc/ {
        root /ABC;
    }
    ```

    当请求 http://qingshan.com/123/abc/logo.png 时，会返回 /ABC/123/abc/logo.png 文件，即用 /ABC 加上 /123/abc。

  - root（根目录）是最上层目录的定义。

    ```nginx
    location /123/abc/ {
        alias /ABC;
    }
    ```

    当请求 http://qingshan.com/123/abc/logo.png 时，会返回 /ABC/logo.png 文件，即用 /ABC 替换 /123/abc。

参考文档：

1. [nginx配置静态资源访问 - 博客园](https://www.cnblogs.com/qingshan-tang/p/12763522.html)

## 限流

- Nginx作为一款高性能的Web代理和负载均衡服务器，往往会部署在一些互联网应用比较前置的位置。此时，我们就可以在Nginx上进行设置，对访问的IP地址和并发数进行相应的限制。

- Nginx官方版本限制IP的连接和并发分别有两个模块：

  - limit_req_zone 用来限制单位时间内的请求数，即速率限制,采用的漏桶算法 "leaky bucket"。
  - limit_req_conn 用来限制同一时间连接数，即并发限制。

- limit_req_zone 参数配置

  - limit_req_zone 参数说明

    - ```nginx
      Syntax: limit_req zone=name [burst=number] [nodelay];
      Default: —
      Context: http, server, location
      limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s;
      ```

      - 第一个参数：`$binary_remote_addr` 表示通过remote_addr这个标识来做限制，“binary_”的目的是缩写内存占用量，是限制同一客户端ip地址。
      - 第二个参数：`zone=one`:10m表示生成一个大小为 10M，名字为 one 的内存区域，用来存储访问的频次信息。
      - 第三个参数：`rate=1r/s` 表示允许相同标识的客户端的访问频次，这里限制的是每秒 1 次，还可以有比如 30r/m 的。

    - ```nginx
      limit_req zone=one burst=5 nodelay;
      ```

      - 第一个参数：`zone=one` 设置使用哪个配置区域来做限制，与上面limit_req_zone 里的name对应。
      - 第二个参数：burst=5，重点说明一下这个配置，burst爆发的意思，这个配置的意思是设置一个大小为 5 的缓冲区当有大量请求（爆发）过来时，超过了访问频次限制的请求可以先放到这个缓冲区内。
      - 第三个参数：nodelay，如果设置，超过访问频次而且缓冲区也满了的时候就会直接返回 503，如果没有设置，则所有请求会等待排队。

  - limit_req_zone示例

    - ```nginx
      http {
          limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s;
          server {
              location /search/ {
              	limit_req zone=one burst=5 nodelay;
              }
          }
      }
      ```

    - 下面配置可以限制特定UA（比如搜索引擎）的访问：

      - ```nginx
        limit_req_zone $anti_spider zone=one:10m rate=10r/s;
        limit_req zone=one burst=100 nodelay;
        if ($http_user_agent ~* "googlebot|bingbot|Feedfetcher-Google") {
        	set $anti_spider $http_user_agent;
        }
        ```

    - 其它参数

      - ```nginx
        Syntax: limit_req_log_level info | notice | warn | error;
        Default:
        limit_req_log_level error;
        Context: http, server, location
        ```

        - 当服务器由于limit被限速或缓存时，配置写入日志。延迟的记录比拒绝的记录低一个级别。例子：limit_req_log_level notice 延迟的的基本是info。

      - ```nginx
        Syntax: limit_req_status code;
        Default:
        limit_req_status 503;
        Context: http, server, location
        ```

        - 设置拒绝请求的返回值。值只能设置 400 到 599 之间。

- ngx_http_limit_conn_module 参数配置

  - ngx_http_limit_conn_module 参数说明

    - 这个模块用来限制单个IP的请求数。并非所有的连接都被计数。只有在服务器处理了请求并且已经读取了整个请求头时，连接才被计数。

    - ```nginx
      Syntax: limit_conn zone number;
      Default: —
      Context: http, server, location
      limit_conn_zone $binary_remote_addr zone=addr:10m;
      
      server {
          location /download/ {
          	limit_conn addr 1;
          }
      }
      ```

      - 一次只允许每个IP地址一个连接。

    - ```nginx
      limit_conn_zone $binary_remote_addr zone=perip:10m;
      limit_conn_zone $server_name zone=perserver:10m;
      server {
          ...
          limit_conn perip 10;
          limit_conn perserver 100;
      }
      ```

      - 可以配置多个limit_conn指令。例如，以上配置将限制每个客户端IP连接到服务器的数量，同时限制连接到虚拟服务器的总数。

    - ```nginx
      Syntax: limit_conn_zone key zone=name:size;
      Default: —
      Context: http
      limit_conn_zone $binary_remote_addr zone=addr:10m;
      ```

      - 在这里，客户端IP地址作为关键。请注意，不是 $remote_addr，而是使用 $binary_remote_addr 变量。
        - $remote_addr 变量的大小可以从7到15个字节不等。存储的状态在32位平台上占用32或64字节的内存，在64位平台上总是占用64字节。
        - 对于IPv4地址，$ binary_remote_addr变量的大小始终为4个字节，对于IPv6地址则为16个字节。存储状态在32位平台上始终占用32或64个字节，在64位平台上占用64个字节。
      - 一个兆字节的区域可以保持大约32000个32字节的状态或大约16000个64字节的状态。如果区域存储耗尽，服务器会将错误返回给所有其他请求。

    - ```nginx
      Syntax: limit_conn_log_level info | notice | warn | error;
      Default:
      limit_conn_log_level error;
      Context: http, server, location
      ```

      - 当服务器限制连接数时，设置所需的日志记录级别。

    - ```nginx
      Syntax: limit_conn_status code;
      Default:
      limit_conn_status 503;
      Context: http, server, location
      ```

      - 设置拒绝请求的返回值。

- Nginx 限流实战

  - 限制访问速率
  - burst 缓存处理
  - nodelay 降低排队时间
    - nodelay 参数要跟 burst 一起使用才有作用。
  - 自定义返回值

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十七：如何使用 Nginx 实现限流？

## 跨域

- **为何会跨域？**

  - 出于浏览器的同源策略限制。
  - **同源策略（Sameoriginpolicy）**是一种约定，它是浏览器最核心也最基本的安全功能，如果缺少了同源策略，则浏览器的正常功能可能都会受到影响。可以说Web是构建在同源策略基础之上的，浏览器只是针对同源策略的一种实现。
  - 同源策略会阻止一个域的 javascript 脚本和另外一个域的内容进行交互。所谓同源（即指在同一个域）就是两个页面具有**相同的协议（protocol），主机（host）和端口号（port）**。

- **Nginx 如何解决跨域？**

  - Nginx 作为反向代理服务器，就是把 http 请求转发到另一个或者一些服务器上。通过把本地一个 url 前缀映射到要跨域访问的web服务器上，就可以实现跨域访问。
  - 对于浏览器来说，访问的就是同源服务器上的一个 url。而 Nginx 通过检测 url 前缀，把 http 请求转发到后面真实的物理服务器。并通过 rewrite 命令把前缀再去掉。这样真实的服务器就可以正确处理请求，并且并不知道这个请求是来自代理服务器的。

- **Nginx 解决跨域案例**

  - 使用 Nginx 解决跨域问题时，我们可以编译 Nginx 的 nginx.conf 配置文件，例如，将 nginx.conf 文件的 server 节点的内容编辑成如下所示。

  - ```nginx
    server {
        location / {
            root html;
            index index.html index.htm;
            //允许cros跨域访问
            add_header 'Access-Control-Allow-Origin' '*';
        }
        //自定义本地路径
        location /apis {
            rewrite ^.+apis/?(.*)$ /$1 break;
            include uwsgi_params;
            proxy_pass http://www.binghe.com;
        }
    }
    ```

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景三十二：Nginx 如何解决跨域问题？

# Docker

## 三要素：仓库、镜像、容器

- 需要正确的理解仓库/镜像/容器这几个概念：
  - Docker 本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个打包好的运行环境就是 image 镜像文件。只有通过这个镜像文件才能生成 Docker 容器实例（类似 Java 中 new 出来一个对象）。
  - image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。
- **镜像文件**
  - image 文件生成的容器实例，本身也是一个文件，称为镜像文件。
  - **Docker 镜像（image）就是一个只读的模板**。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。它也相当于是一个 root 文件系统。比如官方镜像 centos:7 就包含了完整的一套 centos:7 最小系统的 root 文件系统。相当于容器的“源代码”，Docker 镜像文件类似于 Java 的类模板，而 Docker 容器实例类似于 Java 中 new 出来的实例对象，
- **容器实例**
  - 一个容器运行一种服务，当我们需要的时候，就可以通过 Docker 客户端创建一个对应的运行实例，也就是我们的容器

    1. 从面向对象角度：Docker 利用容器（Container）独立运行的一个或一组应用，应用程序或服务运行在容器里面，容器就类似于一个虚拟化的运行环境，**容器是用镜像创建的运行实例**。就像是 Java 中类和实例对象一样，镜像是静态的定义，容器是镜像运行时的实体。容器为镜像提供了一个标准的和隔离的运行环境，它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台
    2. 从镜像容器角度：可以把容器看做是一个简易版的 Linux 环境（包括 root 用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。
- **仓库**
  - 就是放一堆镜像的地方，我们可以把镜像发布到仓库中，需要的时候再从仓库中拉下来就可以了。
  - 仓库（Repository）是集中存放镜像文件的场所。类似于 Maven 仓库，存放各种 jar 包的地方；GitHub 仓库，存放各种 Git 项目的地方；Docker 公司提供的官方 Registry 被称为 Docker Hub，存放各种镜像模板的地方。
  - 仓库分为公开仓库（Public）和私有仓库（Private）两种形式。最大的公开仓库是 Docker Hub(http://hub.docker.com)存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云、网易云等

参考文档：

1. [《尚硅谷Docker实战教程》笔记.md](../视频笔记/《尚硅谷Docker实战教程》笔记.md) P8

## 整体架构 / 运行流程

- 整体架构及底层通信原理简述
  - Docker 是一个 C/S 模式的架构，后端是一个松耦合架构，众多模块各司其职。
- Docker 运行的基本流程为：
  1. 用户是使用 **Docker Client** 与 Docker Daemon 建立通信，并发送请求给后者。
  2. **Docker Daemon** 作为 Docker 架构中的主体部分，首先提供 Docker Server 的功能使其可以接受 Docker Client 的请求。
  3. **Docker Engine** 执行 Docker 内部的一系列工作，每一项工作都是以一个 Job 的形式的存在。
  4. Job 的运行过程中，当需要容器镜像时，则从 **Docker Registry** 中下载镜像，并通过镜像管理驱动 Graph driver 将下载镜像以 Graph 的形式存储。
  5. 当需要为 Docker 创建网络环境时，通过网络管理驱动 **Network driver** 创建并配置 Docker 容器网络环境。
  6. 当需要限制 Docker 容器运行资源或执行用户指令等操作时，则通过 **Exec driver** 来完成。
  7. **Lib container** 是一项独立的容器管理包，Network driver 以及 Exec driver 都是通过 Lib container 来实现具体对容器进行的操作。（包括 netlink, appmarmor, **namespaces**, **cgroups**, devices）

参考文档：

1. [《尚硅谷Docker实战教程》笔记.md](../视频笔记/《尚硅谷Docker实战教程》笔记.md) P10

## 容器的核心实现原理（Namespace、Cgroups、Rootfs）

- 首先我们思考一个问题：容器与进程有何不同？

  - 进程：就是程序运行起来后的计算机执行环境的总和。
    即：计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合。
  - 容器：核心就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。
    对于 Docker 等大多数 Linux 容器来说，Cgroups 技术是用来制造约束的主要手段，而 Namespace 技术则是用来修改进程视图的主要方法。

- **基于 Namespace 的视图隔离**

  - 当我们通过 `docker run -it` 启动并进入一个容器之后，会发现不论是进程、网络还是文件系统，好像都被隔离了,就像这样：
    - ps 命令看不到宿主机上的进程
    - ip 命令也只能看到容器内部的网卡
    - ls 命令看到的文件好像也和宿主机不一样
  - **这就是 Docker 核心之一，借助 Linux Namespace 技术实现了视图隔离。**
  - 在 Linux 下可以根据隔离的属性不同分为不同的 Namespace ：
    - 1）PID Namespace
    - 2）Mount Namespace
    - 3）UTS Namespace
    - 4）IPC Namespace
    - 5）Network Namespace
    - 6）User Namespace
  - 通过不同类型的 Namespace 就可以实现不同资源的隔离，比如前面通过 ip a 只能看到容器中的网卡信息，就是通过 Network Namespace进行了隔离。
    - 不过 Linux Namespace 技术实际上修改了应用进程看待整个计算机“视图”，即它的“视线”被操作系统做了限制，只能“看到”某些指定的内容。
    - 我们只需要进入到对应 namespace 就可以突破这个隔离了
  - 这里顺便提一下 Namespace 存在的问题，Namespace 最大的问题就是隔离得不彻底。
    - 首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。
      - 所以，也出现了像 Firecracker、gVisor、Kata 之类的沙箱容器，不使用共享内核来提升安全性。
    - 其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。
      **容器中修改了时间，实际修改的是宿主机的时间，会导致所有容器时间都被修改**，因为是共享的。

- **基于 Cgroups 的资源限制**

  - `docker run` 启动容器时可以通过增加 `--cpus` 或者 `--memory` flag 来指定 cpu、内存限制。

    - 就像这样：通过 `--cpus=0.5` 限制只能使用 0.5 个核心
    - 可以看到，因为限制了 cpu 为 0.5，因此只占用了 0.5 核心，也就是 top 命令中看到的 50。

  - **这就是 Docker 另一个核心功能，基于 Linux Cgroups 技术实现的资源限制。**

    - Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。

      > Linux Cgroups 的全称是 Linux Control Group。

  - 它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。

  - 在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，**即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下**，可以使用

    - `mount -t cgroup` 命令进行查看

    - 可以看到，在`/sys/fs/cgroup` 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。

      > 即：这台机器当前可以被 Cgroups 进行限制的资源种类。

  - 除 CPU 子系统外，Cgroups 的每一个子系统都有其独有的资源限制能力，比如：

    - blkio，为块设备设定I/O 限制，一般用于磁盘等设备；
    - cpuset，为进程分配单独的 CPU 核和对应的内存节点；
    - memory，为进程设定内存使用的限制。

  - Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。

    - 而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。

    - 而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，比如这样一条命令：

      ```shell
      docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash
      ```

      在启动这个容器后，我们可以通过查看 Cgroups 文件系统下，CPU 子系统中，“docker”这个控制组里的资源限制文件的内容来确认：

      ```shell
      $ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us
      100000
      $ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us
      20000
      ```

- 容器镜像的秘密

  - 这部分主要解释以下三个问题
    - 1）为什么在容器中修改了文件宿主机不受影响？
    - 2）容器中的文件系统是哪儿来的？
    - 3）docker 镜像又是怎么实现的？
  - **这也是 Docker 的第三个核心功能：容器镜像（rootfs）**，将运行环境打包成镜像，从而避免环境问题导致应用无法运行。
  - **1、文件系统**
    - 因为容器中的文件系统经过 Mount Namespace 隔离，所以应该是独立的。
      - 其中 Mount Namespace 修改的，是容器进程对文件系统“挂载点”的认知。只有在“挂载”这个操作发生之后，进程的视图才会被改变。而在此之前，新创建的容器会直接继承宿主机的各个挂载点。
      - 不难想到，我们可以在容器进程启动之前重新挂载它的整个根目录“/”。而由于 Mount Namespace 的存在，这个挂载对宿主机不可见，所以容器进程就可以在里面随便折腾了。
    - Linux 中 **chroot** 命令（change root file system）就能很方便的完成上述工作。
      - 而 Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，**它也是 Linux 操作系统里的第一个 Namespace**。
    - 至此，第一个问题 **为什么在容器中修改了文件宿主机不受影响**？有答案了，因为使用 Mount Namespace 隔离了。
  - **2、rootfs**
    - 上文提到 Mount Namespace 会修改容器进程对文件系统挂载点的认知，而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“**容器镜像**”。它还有一个更为专业的名字，叫作：**rootfs**（根文件系统）。
    - **rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核**。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。
    - 所以说，rootfs 只包括了操作系统的“躯壳”，并没有包括操作系统的“灵魂”。**实际上，同一台机器上的所有容器，都共享宿主机操作系统的内核**。
      - 这也是容器相比于虚拟机的主要缺陷之一：毕竟后者不仅有模拟出来的硬件机器充当沙盒，而且每个沙盒里还运行着一个完整的 Guest OS 给应用随便折腾。
    - 不过，正是**由于 rootfs 的存在，容器才有了一个被反复宣传至今的重要特性：一致性**。由于 rootfs 里打包的不只是应用，而是**整个操作系统的文件和目录**，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。
    - 第二个问题：**容器中的文件系统是哪儿来的**？实际上是我们构建镜像的时候打包进去的，然后容器启动时挂载到了根目录下。
  - **3、镜像层（Layer）**
    - Docker 在镜像的设计中，引入了层（layer）的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。
      - **通过引入层（layer）的概念，实现了 rootfs 的复用**。不必每次都重新创建一个 rootfs，而是基于某一层进行修改即可。
    - Docker 镜像层用到了一种叫做**联合文件系统（Union File System）**的能力。
      - Union File System 也叫 UnionFS，最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下。
        - 例如将目录 A 和目录 B 挂载到目录 C 下面，这样目录 C 下就包含目录 A 和目录 B 的所有文件。由于看不到目录 A 和 目标 B 的存在，因此就好像 C 目录就包含这么多文件一样
      - Docker 镜像分为多个层，然后使用 UFS 将这多个层挂载到一个目录下面，这样这个目录就包含了完整的文件了。
        - UnionFS 在不同系统有各自的实现，所以 Docker 的不同发行版使用的也不一样，可以通过 docker info 查看。常见有 aufs（ubuntu 常用）、overlay2（centos 常用）
      - 就像下图这样：union mount 在最上层，提供了统一的视图，用户看起来好像整个系统只有一层一样，实际上下面包含了很多层。
    - 镜像只包含了静态文件，但是容器会产生实时数据，所以容器的 rootfs 在镜像的基础上增加了**可读写层和 Init 层**。
      - 即容器 rootfs 包括：只读层（镜像rootfs）+ init 层（容器启动时初始化修改的部分数据） + 可读写层（容器中产生的实时数据）。
      - **只读层（镜像rootfs）**
        - 它是这个容器的 rootfs 最下面的几层，即**镜像中的所有层的总和**，它们的挂载方式都是只读的（ro+wh，即 readonly+whiteout）
      - **可读写层（容器中产生的实时数据）**
        - 它是这个容器的 rootfs 最上面的一层，它的挂载方式为：rw，即 read write。在没有写入文件之前，这个目录是空的。
        - 而一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中，删除操作实现比较特殊（类似于标记删除）。
          - AUFS 的 whiteout 的实现是通过在上层的可写的目录下建立对应的 whiteout 隐藏文件来实现的。
          - 为了实现删除操作，aufs（UnionFS 的一种实现） 会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来。
          - 比如，你要删除只读层里一个名叫 foo 的文件，那么这个删除操作实际上是在可读写层创建了一个名叫.wh.foo 的文件。这样，当这两个层被联合挂载之后，foo 文件就会被.wh.foo 文件“遮挡”起来，“消失”了。
      - **init 层（容器启动时初始化修改的部分数据）**
        - 它是一个以“-init”结尾的层，夹在只读层和读写层之间，Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。
        - **为什么需要 init 层？**
          - 比如 hostname 这样的数据，原本是属于镜像层的一部分，要修改的话只能在可读写层进行修改，但是又不想在 docker commit 的时候把这些信息提交上去，所以使用 init 层来保存这些修改。
            - 可以理解为提交代码的时候一般也不会把各种配置信息一起提交上去。
            - docker commit 只会提交 只读层和可读写层。
    - 最后一个问题：**docker 镜像又是怎么实现的**？通过引入 layer 概念进行分层，借助 联合文件系统（Union File System）进行叠加，最终构成了完整的镜像。
      - 这里只是镜像的主要内容，具体怎么把这些内容打包成 image 格式就是 OCI 规范了

- 总结

  - 至此，我们大致清楚了 Docker 容器的实现主要使用了如下 3 个功能：
    - 1）Linux Namespace 的隔离能力
    - 2）Linux Cgroups 的限制能力
    - 3）基于 rootfs 的文件系统

参考文档：

1. [深入理解 Docker 核心原理：Namespace、Cgroups 和 Rootfs](https://blog.csdn.net/java_1996/article/details/134638777)

## Namespace

- docker 的实现方式

  - 很多开发者都知道，docker容器本质上是宿主机的进程
  - Docker 通过 **namespace 实现了资源隔离**
  - 通过 **cgroups 实现了资源限制**
  - 通过**写时复制机制（copy-on-write）实现了高效的文件操作**。

- 当进一步深入 namespace 和 cgroups 等技术细节时，大部分开发者都会感到茫然无措。尤其是接下来解释libcontainer 的工作原理时，我们会接触大量容器核心知识。所以在这里，希望先带领大家走进linux内核，了解 namespace 和 cgroups 的技术细节。

  - namespace 资源隔离

    - linux内核提拱了6种namespace隔离的系统调用，如下图所示，但是真正的容器还需要处理许多其他工作。

      | namespace | 系统调用参数  | 隔离内容                   |
      | --------- | ------------- | -------------------------- |
      | UTS       | CLONE_NEWUTS  | 主机名或域名               |
      | IPC       | CLONE_NEWIPC  | 信号量、消息队列和共享内存 |
      | PID       | CLONE_NEWPID  | 进程编号                   |
      | Network   | CLONE_NEWNET  | 网络设备、网络战、端口等   |
      | Mount     | CLONE_NEWNS   | 挂载点（文件系统）         |
      | User      | CLONE_NEWUSER | 用户组和用户组             |

    - 实际上，**linux 内核实现 namespace 的主要目的，就是为了实现轻量级虚拟化技术服务**。在同一个 namespace 下的进程合一感知彼此的变化，而对外界的进程一无所知。这样就可以让容器中的进程产生错觉，仿佛自己置身一个独立的系统环境中，以达到隔离的目的。

    - 需要注意的是，本文所讨论的namespace实现针对的是linux内核3.8及以后版本。

    - PID namespace

      - PID namespace隔离非常实用，它对进程PID重新标号，即两个不同namespace下的进程可以有相同的PID。每个PID namespace都有自己的计数程序。
      - 内核为所有的PID namespace维护了一个树状结构，最顶层的是系统初始时创建的，被称为root namespace，它创建的新PID namespace被称为child namespace(树的子节点)，而原来的PID namespace就是新创建的PID namespace的parent namespace(树的父节点)。通过这种方式，不同的PID namespace会形成一个层级体系。
        - **所属的父节点可以看到子节点中的进程**，并可以通过信号等方式对子节点中的进程产生影响。反过来，**子节点却不能看到父节点PID namespace中的任何内容**

    - mount namespace

      - mount namespace通过隔离文件系统挂载点对隔离文件系统提供支持，它是历史上第一个Linux namespace，所以标示位比较特殊，就是CLONE_NEWNS。
      - 隔离后，不同的mount namespace中的文件结构发生变化也互不影响。也可以通过/proc/[pid]/mounts查看到所有挂载在当前namespace中的文件系统，还可以通过/proc/[pid]/mountstats看到mount namespace中文件设备的统计信息，包括挂载文件的名字、文件系统的类型、挂载位置等。

    - 主机上查看一下，docker-run 的进程2117和docker exec进程2354 在同一个namespace (pid对应的namespace的id 是一样)

      - 所以在两个进程中看到的内容是一样的

参考文档：

1. [docker的namespace深入理解 - 博客园](https://www.cnblogs.com/pu20065226/p/13514513.html)

## 镜像分层结构 / UnionFS（bootfs、rootfs）

- **镜像是什么**
  - 是一种轻量级、可执行的独立软件包，它包含运行某个软件所需的所有内容，我们把应用程序和配置依赖打包好形成一个可交付的运行环境（包括代码、运行时需要的库、环境变量和配置文件等），这个打包好的运行环境就是 image 镜像文件。
  - 只有通过这个镜像文件才能生成 Docker 容器实例（类似 Java 中 new 出来一个对象）
- **分层的镜像**
  - 以我们的 pull 为例，在下载的过程中我们可以看到 Docker 的镜像好像是在一层一层的下载
- **UnionFS(联合文件系统)**
  - Union 文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下（unite several directories into a single virtual filesystem）。
  - Union 文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。
  - 特性
    - 一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。
- **Docker 镜像加载原理**
  - Docker 镜像实际上由一层一层的文件系统组成，这种层级的文件系统 UnionFS。
  - **bootfs(boot file system)**主要包含 bootloader 和 kernel
    - **bootloader** 主要是引导加载 kernel，Linux 刚启动时会加载 bootfs 文件系统，在 Docker 镜像的最底层是引导文件系统 bootfs。
    - 这一层与我们典型的 Linux/Unix 系统是一样的，包含 boot 加载器和内核。当 boot 加载完成以后整个内核就在内存中了，此时内存的使用权已由 bootfs 转交给内核，此时系统也会卸载 bootfs。
  - **rootfs(root file system)**，在 bootfs 之上，包含的就是典型 Linux 系统中的 /dev, /proc, /bin, /etc 等标准目录和文件。
    - rootfs 就是各种不同的操作系统发行版，比如 Ubuntu，CentOS 等等。
  - 平时我们安装进虚拟机的 CentOS 都是好几个 G，为什么 Docker 这里才 200M？
    - 对于一个精简的 OS，rootfs 可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用 Host 的 kernel，自己只需要提供 rootfs 就行了。由此可见对于不同的 Linux 发行版，bootfs 基本是一致的，rootfs 会有差别，因此不同的发行版可以公用 bootfs。
- **为什么 Docker 镜像要采用这种分层结构呢？**
  - 镜像分层最大的一个好处就是共享资源，方便复制迁移，就是为了复用。
  - 比如说有多个镜像都从相同的 base 镜像构建而来，那么 Docker Host 只需在磁盘上保存一份 base 镜像；同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。
- **重点理解**
  - Docker 镜像层都是只读的，容器层是可写的
  - **当容器启动时，一个新的可写层被加载到镜像的顶部。 这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”**。
  - 所有对容器的改动——无论添加、删除、还是修改文件都只会发生在容器层中。**只有容器层是可写的，容器层下面的所有镜像层都是只读的**。

参考文档：

1. [《尚硅谷Docker实战教程》笔记.md](../视频笔记/《尚硅谷Docker实战教程》笔记.md) P23

## 命令

- Docker 本身相关命令
  - 启动 Docker：systemctl start docker
  - 停止 Docker：systemctl stop docker
  - 重启 Docker：systemctl restart docker
  - 查看 Docker 状态：systemctl status docker
  - 开机启动：systemctl enable docker
  - 查看 Docker 概要信息：docker info
  - 查看 Docker 总体帮助文档：docker --help
  - 查看 Docker 命令帮助文档：docker 具体命令 --help
- 镜像命令
  - **docker images**：列出本地主机上的镜像
    - OPTIONS 说明：
      - -a 列出本地所有的镜像（含历史映像层）
      - -q 只显示镜像 ID
    - 各个选项说明：
      - REPOSITORY 表示镜像的仓库源
      - TAG：镜像的标签版本号
      - IMAGE ID：镜像 ID
      - CREATED：镜像创建时间
      - SIZE：镜像大小
    - 同一个仓库源可以有多个 TAG 版本，代表这个仓库源的不同版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，Docker 将默认使用 ubuntu:latest 镜像
  - docker search 某个镜像名字：搜索镜像
    - NAME 镜像名称
    - DESCRIPTION 镜像说明
    - STARS 点赞数量
    - OFFICIAL 是否是官方的
    - AUTOMATED 是否是自动构建的
  - **docker pull 某个镜像名字**：下载镜像
  - docker system df 查看镜像/容器/数据卷所占的空间
  - **docker rmi 某个镜像名字ID**：删除镜像
    - 删除单个： docker rmi -f 镜像ID
    - 删除多个： docker rmi -f 镜像名1:TAG 镜像名2:TAG
    - 删除全部： docker rmi -f $(docker images -qa)
- 容器命令
  - **新建+启动容器：`docker run [OPTIONS] IMAGE [COMMAND] [ARG...]`**
    - OPTIONS 说明：
      - `--name="容器新名字"` 为容器指定一个名称
      - `-d` 后台运行容器并返回容器 ID，也即启动守护式容器（后台运行）
      - `-i` 以交互模式运行容器，通常与 -t 同时使用
      - `-t` 为容器重新分配一个伪输入终端，通常与 -i 同时使用，也即启动交互式容器（前台有伪终端，等待交互）
      - `-P` 随机端口映射，大写P
      - `-p` 指定端口映射，小写p
        - `-p hostPort:containerPort` 端口映射 -p 8080:80
        - `-p ip:hostPort:containerPort` 配置监听地址 -p 10.0.0.100:8080:80
        - `-p ip::containerPort` 随机分配端口 -p 10.0.0.100::80
        - `-p hostPort:containerPort:udp` 指定协议 -p 8080:80:tcp
        - `-p 81:80 -p 443:443` 指定多个
  - **列出当前所有正在运行的容器：`docker ps [OPTIONS]`**
    - OPTIONS 说明
      - `-a` 列出当前所有正在运行的容器+历史上运行过的
      - `-l` 显示最近创建的容器
      - `-n` 显示最近n个创建的容器
      - `-q` 静默模式，只显示容器编号
  - **退出容器：两种推出方式 exit 和 ctrl+p+q**
    - exit 退出，容器停止
    - ctrl+p+q 退出，容器不停止
  - **启动已停止运行的容器：`docker start 容器ID或容器名`**
  - **重启容器：`docker restart 容器ID或容器名`**
  - **停止容器：`docker stop 容器ID或容器名`**
  - **强制停止容器：`docker kill 容器ID或容器名`**
  - **删除已停止的容器：`docker rm 容器ID`**
    - 一次性删除多个容器实例
      - `docker rm -f $(docker ps -a -q)`
      - `docker ps -a -q | xargs docker rm`
  - **启动守护式容器（后台服务器）：`docker run -d 容器名`**
  - **查看容器日志：`docker logs 容器ID`**
  - 查看容器内运行的进程：`docker top 容器ID`
  - 查看容器内部细节：`docker inspect 容器ID`
  - **进入正在运行的容器并以命令行交互：`docker exec -it 容器ID bashShell`**
    - 重新进入：`docker attach 容器ID`
    - 上述两个区别
      - attach 直接进入容器启动命令的终端，不会启动新的线程。用 exit 退出，会导致容器的停止
      - exec 是在容器中打开新的终端，并且可以启动新的进程。用 exit 退出，不会导致容器的停止
    - 推荐使用 docker exec
  - **从容器内拷贝文件到主机上：`docker cp 容器ID:容器内路径 目的主机路径`**
  - 导入和导出容器：`docker export 容器ID > 文件名.tar` 和 `cat 文件名.tar | docker import - 镜像用户/镜像名:镜像版本号`
    - export 导出容器的内容流作为一个 tar 归档文件【对应 import 命令】
      - `docker export 容器ID > 文件名.tar`
    - import 从 tar 包中的内容创建一个新的文件系统再导入为镜像【对应 export】
      - `cat 文件名.tar | docker import - 镜像用户/镜像名:镜像版本号`
- **常用命令汇总**
  - `attach` 当前 shell 下 attach 连接指定运行镜像
  - `build` 通过 Dockerfile 定制镜像
  - `commit` 提交当前容器为新的镜像
  - `cp` 从容器中拷贝指定文件或者目录到宿主机中
  - `create` 创建一个新的容器，同 run，但不启动容器
  - `diff` 查看 docker 容器变化
  - `events` 从 docker 服务获取容器实时事件
  - `exec` 在已存在的容器上运行命令
  - `export` 导出容器的内容流作为一个 tar 归档文件【对应 import】
  - `history` 展示一个镜像形成历史
  - `images` 列出系统当前镜像
  - `import` 从 tar 包中的内容创建一个新的文件系统映像【对应 export】
  - `info` 显示系统相关信息
  - `inspect` 查看容器详细信息
  - `kill` 杀死指定 Docker 容器
  - `load` 从一个 tar 包中加载一个镜像【对应 save】
  - `login` 注册或者登录一个 Docker 源服务器
  - `logout` 从当前 Docker registry 退出
  - `logs` 输出当前容器日志信息
  - `port` 查看映射端口对应的容器内部源端口
  - `pause` 暂停容器
  - `ps` 列出容器列表
  - `pull` 从 Docker 镜像源服务器拉取指定镜像或者库镜像
  - `push` 推送指定镜像或者库镜像至 Docker 源服务器
  - `restart` 重启运行的容器
  - `rm` 移除一个或者多个容器
  - `rmi` 移除一个或多个镜像【无容器使用该镜像才可删除，否则需删除相关容器才可继续或 -f 强制删除】
  - `run` 创建一个新的容器并运行一个命令
  - `save` 保存一个镜像为一个 tar 包【对应 load】
  - `search` 在 Docker Hub 中搜索镜像
  - `start` 启动容器
  - `stop` 停止容器
  - `tag` 给源中镜像打标签
  - `top` 查看容器中运行的进程信息
  - `unpause` 取消暂停容器
  - `version` 查看 Docker 版本号
  - `wait` 截取容器停止时的退出状态值

参考文档：

1. [《尚硅谷Docker实战教程》笔记.md](../视频笔记/《尚硅谷Docker实战教程》笔记.md) P15 ~ 22

## 容器数据卷

- **是什么**
  - 卷就是目录或文件，存在于一个或多个容器中，**由 docker 挂载到容器，但不属于联合文件系统**，因此能够绕过 Union File System 提供一些用于持续存储或共享数据的特性。
  - **卷的设计目的就是数据的持久化，完全独立于容器的生命周期**，因此 Docker 不会在容器删除时删除其挂载的数据卷。
    - 一句话：有点类似我们 Redis 里面的 rdb 和 aof 文件
    - 将 docker 容器内的数据保存进宿主机的磁盘中
    - 运行一个带有容器卷存储功能的容器实例 `docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录 镜像名`
- **能干嘛**
  - 将运用和运行的环境打包成镜像，run 后形成容器实例运行，但是我们对数据的要求希望是持久化的
  - Docker 容器产生的数据，如果不备份，那么当容器实例删除后，容器内的数据自然也就没有了。为了能保存数据，在 Docker 中我们使用卷。
  - 特点：
    1. **数据卷可在容器之间共享或重用数据**
    2. **卷中的更改可以直接实时生效**
    3. **数据卷中的更改不会包含在镜像的更新中**
    4. **数据卷的生命周期一直持续到没有容器使用它为止**
- 数据卷案例

  1. 宿主vs容器之间映射添加容器卷
     - 直接命令添加
       - 命令：`docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录 镜像名`
       - 查看数据卷是否挂载成功
         - `docker inspect 容器ID`
         - 查看 "Mounts" 里的 "Source"、"Destination"
       - 容器和宿主机之间数据共享
         1. docker 修改，主机同步获得
         2. 主机修改，docker 同步获得
         3. docker 容器 stop，主机修改，docker 容器重启看数据是否同步。
  2. 读写规则映射添加说明
     - 读写（默认）
       - `docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:rw 镜像名`
       - 默认同上案例，**默认就是 rw**
     - 只读
       - 容器实例内部被限制，只能读取不能写
       - `docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:ro 镜像名`
       - 此时如果宿主机写入内容，可以同步给容器内，容器可以读到。
  3. 卷的继承和共享
     - 容器1完成和宿主机的映射
       - `docker run -it --privileged=true -v /mydocker/u:/tmp --name u1 ubuntu`
     - 容器2继承容器1的卷规则
       - `docker run -it --privileged=true --volume-from 父类 --name u2 ubuntu`

参考文档：

1. [《尚硅谷Docker实战教程》笔记.md](../视频笔记/《尚硅谷Docker实战教程》笔记.md) P29 ~ 33

## dockerfile

- **是什么**

  - Dockerfile 是用来构建 Docker 镜像的文本文件，是由一条条构建镜像所需的指令和参数构成的脚本
  - 概述
  - 官网
    - https://docs.docker.com/engine/reference/builder
  - 构建三步骤
    - 编写 Dockerfile 文件
    - docker build 命令构建镜像
    - docker run 依镜像运行容器实例

- **dockerfile 内容基础知识**

  1. 每条保留字指令都必须为大写字母且后面要跟随至少一个参数
  2. 指令按照从上到下，顺序执行
  3. `#` 表示注释
  4. 每条指令都会创建一个新的镜像层并对镜像进行提交

- **Docker 执行 dockerfile 的大致流程**

  1. docker 从基础镜像运行一个容器
  2. 执行一条指令并对容器做出修改
  3. 执行类似 docker commit 的操作提交一个新的镜像层
  4. docker 再基于刚提交的镜像运行一个新容器
  5. 执行 dockerfile 中的下一条指令直到所有指令都执行完成

- **小总结**

  - 从应用软件的角度来看，Dockerfile、Docker 镜像与 Docker 容器分别代表软件的三个不同阶段

    - Dockerfile 是软件的原材料
    - Docker 镜像是软件的交付品
    - Docker 容器则可以认为是软件镜像的运行态，也即依照镜像运行的容器实例

  - Dockerfile 面向开发，Docker 镜像成为交付标准，Docker 容器则涉及部署与运维，三者缺一不可，合力充当 Docker 体系的基石。
    1. Dockerfile，需要定义一个 Dockerfile，Dockerfile 定义了进程需要的一切东西。Dockerfile 涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程（当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计 namespace 的权限控制）等等。
    2. Docker 镜像，在用 Dockerfile 定义一个文件之后，docker build 时会产生一个 Docker 镜像，当运行 Docker 镜像时会真正开始提供服务
    3. Docker 容器，容器是直接提供服务的。

- **dockerfile 保留字**

  - 参考 tomcat8 的 dockerfile 入门
    - https://github.com/docker-library/tomcat
  - `FROM`：基础镜像，当前新镜像是基于哪个镜像的，指定一个已经存在的镜像作为模板，第一条必须是 FROM
  - `MAINTAINER`：镜像维护者的姓名和邮箱地址
  - `RUN`：容器构建时需要运行的命令
    - 两种格式：shell 格式和 exec 格式
      - shell 格式: `RUN <命令>`
      - exec 格式：`RUN ["可执行文件", "参数1", "参数2"...]`
    - RUN 是在 docker build 时运行
  - `EXPOSE`：当前容器对外暴露出的端口
  - `WORKDIR`：指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点
  - `USER`：指定该镜像以什么样的用户去执行，如果都不指定，默认是 root
  - `ENV`：用来在构建镜像过程中设置环境变量
    - `ENV MY_PATH /usr/mytest` 这个环境变量可以在后续的任何 RUN 指令中使用，这就如同在命令前面指定了环境变量前缀一样；也可以在其他指令中直接使用这些环境变量，比如：`WORKDIR $MY_PATH`
  - `ADD`：将宿主机目录下的文件拷贝进镜像且会自动处理 URL 和解压 tar 压缩包
  - `COPY`：类似 ADD，拷贝文件和目录到镜像中。将从构建上下文目录中<源路径>的文件/目录复制到新的一层的镜像内的<目标路径>位置
  - `VOLUME`：容器数据卷，用于数据保存和持久化工作
  - `CMD`：指定容器启动之后要干的事情
    - 两种格式：shell 格式和 exec 格式
      - shell 格式：`CMD <命令>`
      - exec 格式：`CMD ["可执行文件", "参数1", "参数2"...]`
      - 参数列表格式：`CMD ["参数1", "参数2"...]`，在指定了 `ENTRYPOINT` 指令后，用 `CMD` 指定具体的参数
    - 注意
      - Dockerfile 中可以有多个 CMD 指令，但只有最后一个会生效，CMD 会被 docker run 之后的参数替换
      - 参考官网 Tomcat 的 dockerfile 演示讲解
    - 它和前面 RUN 命令的区别
      - CMD 是在 docker run 时运行
      - RUN 是在 docker build 时运行
  - `ENTRYPOINT`：也是用来指定一个容器启动时要运行的命令
    - 类似于 CMD 命令，但是 ENTRYPOINT 不会被 docker run 后面的命令覆盖，而且这些命令行参数会被当作参数送给 ENTRYPOINT 指令指定的程序
    - 命令格式和案例说明
      - 命令格式：`ENTRYPOINT ["可执行文件", "参数1", "参数2"...]`
      - ENTRYPOINT 可以和 CMD 一起用，一般是变参才会使用 CMD，这里的 CMD 等于是在给 ENTRYPOINT 传参
      - 当指定了 ENTRYPOINT 后，CMD 的含义就发生了变化，不再是直接运行其命令而是将 CMD 的内容作为参数传递给 ENTRYPOINT 指令，他两个组合会变成 `<ENTRYPOINT> "<CMD>"`
    - 优点
    - 注意

- **dockerfile 实战**

  - 要求

    - CentOS 7 镜像具备 vim + ifconfig +  jdk8
    - JDK 的下载镜像地址

  - 编写

    - 准备编写 Dockerfile 文件（大写字母D）

      ```dockerfile
      FROM centos
      MAINTAINER zzyy<zzyybs@126.com>
      
      ENV MYPATH /usr/local
      WORKDIR $MYPATH
      
      # 安装 vim 编辑器
      RUN yum -y install vim
      # 安装 ifconfig 命令查看网络 IP
      RUN yum -y install net-tools
      # 安装 java8 及 lib 库
      RUN yum -y install glibc.i686
      RUN mkdir /usr/local/java
      # ADD 是相对路径 jar，把 jdk-8u171-linux-x64.tar.gz 添加到容器中，安装包必须要和 Dockerfile 文件在同一位置
      ADD jdk-8u171-linux-x64.tar.gz /usr/local/java/
      # 配置 java 环境变量
      ENV JAVA_HOME /usr/local/java/jdk1.8.0_171
      ENV JRE_HOME $JAVA_HOME/jre
      ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH
      ENV PATH $JAVA_HOME/bin:$PATH
      
      EXPOSE 80
      
      CMD echo $MYPATH
      CMD echo "success------------ok"
      CMD /bin/bash
      ```

  - 构建

    - `docker build -t 新镜像名字:TAG .`
    - 注意，上面 TAG 后面有个空格有个点

  - 运行

    - `docker run -it 新镜像名字:TAG`

  - 再体会下 UnionFS（联合文件系统）

- **虚悬镜像**

  - **最好别搞那么多 RUN，不然一个 RUN 对应 UnionFS 里面的一层**

  - 是什么

    - 仓库名、标签都是 `<none>` 的镜像，俗称 dangling image

      Dockerfile 写一个

      ```dockerfile
      from ubuntu
      CMD echo 'action is success'
      ```

  - 查看

    - `docker image ls -f dangling=true`

  - 删除

    - `docker image prune`
    - 虚悬镜像已经失去存在价值，可以删除

参考文档：

1. [《尚硅谷Docker实战教程》笔记.md](../视频笔记/《尚硅谷Docker实战教程》笔记.md) P57 ~ 62

## docker-compose

- **是什么**

  - Docker-Compose 是 Docker 官方的开源项目，**负责实现对 Docker 容器集群的快速编排**
  - Compose 是 Docker 公司推出的一个工具软件，可以管理多个 Docker 容器组成一个应用。你需要定义一个 YAML 格式的配置文件 docker-compose.yml，写好多个容器之间的调用关系。然后，只要一个命令，就能同时启用/关闭这些容器

- **能干嘛**

  - docker 建议我们每一个容器中只运行一个服务，因为 docker 容器本身占用资源极少，所以最好是将每个服务单独的分割开来，但是这样我们又面临了一个问题：如果我需要同时部署多个服务，难道要每个服务单独写 Dockerfile 然后再构建镜像，构建容器？
  - 这样累都累死了，所以 docker 官方给我们提供了 docker-compose 多服务部署工具
  - 例如要实现一个 Web 微服务项目，除了 Web 服务器容器本身，往往还需要再加上后端的数据库 mysql 服务容器，redis 服务器，注册中心 eureka，甚至还包括负载均衡容器等等……
  - **Compose 允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）**。
  - 可以很容易地用一个配置文件定义一个多容器的应用，然后使用一条指令安装这个应用的所有依赖，完成构建。Docker-compose 解决了容器与容器之间如何管理编排的问题。

- **下载安装**

  - 安装步骤

    - `curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose"`
    - `chmod +x /usr/local/bin/docker-compose`
    - `docker-compose --version`

  - 卸载步骤
    - `sudo rm /usr/local/bin/docker-compose`

- **核心概念**

  - 一文件：**docker-compose.yml**
  - 两要素：
    - **服务（service）**：一个个应用容器实例，比如订单微服务、库存微服务、mysql 容器、nginx 容器或者 redis 容器
    - **工程（project）**：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。

- **使用的三个步骤**

  - 编写 Dockerfile 定义各个微服务应用并构建出对应的镜像文件
  - 使用 docker-compose.yml 定义一个完整业务单元，安排好整体应用中的各个容器服务。
  - 最后，执行 docker-compose up 命令来启动并运行整个应用程序，完成一键部署上线

- **常用命令**

  - `docker-compose -h` 查看帮助
  - `docker-compose up` 启动所有 docker-compose 服务
  - `docker-compose up -d` 启动所有 docker-compose 服务并后台运行
  - `docker-compose down` 停止并删除容器、网络、卷、镜像
  - `docker-compose exec yml里面的服务id` 进入容器实例内部 `docker-compose exec docker-compose.yml文件中写的服务id /bin/bash`
  - `docker-compose ps` 展示当前 docker-compose 编排过的运行的所有容器
  - `docker-compose top` 展示当前 docker-compose  编排过的容器进程
  - `docker-compose logs yml 里面的服务id` 查看容器输出日志
  - `docker-compose config` 检查配置
  - `docker-compose config -q` 检查配置，有问题才有输出
  - `docker-compose restart` 重启服务
  - `docker-compose start` 启动服务
  - `docker-compose stop` 停止服务

- **使用 Compose**

  - 服务编排，一套带走

  - 编写 docker-compose.yml 文件

    - ```yaml
      version: "3"
      
      services:
      	microService:
      		image: zzyy_docker:1.6
      		container_name: ms01
      		ports:
      			- "6001:6001"
      		volumes:
      			- /app/microService:/data
      		networks:
      			- atguigu_net
      		depends_on:
      			- redis
      			- mysql
      	redis:
      		image: redis:6.0.8
      		ports:
      			- "6379:6379"
      		volumes:
      			- /app/redis/redis.conf:/etc/redis/redis.conf
      			- /app/redis/data:/data
      		networks:
      			- atguigu_net
      		command: redis-server /etc/redis/redis.conf
      	
      	mysql:
      		image: mysql:5.7
      		environment:
      			MYSQL_ROOT_PASSWORD: '123456'
      			MYSQL_ALLOW_EMPTY_PASSWORD: 'no'
      			MYSQL_DATA_BASE: 'db2021'
      			MYSQL_USER: 'zzyy'
      			MYSQL_PASSWORD: 'zzyy123'
      		ports:
      			- "3306:3306"
      		volumes:
      			- /app/mysql/db:/var/lib/mysql
      			- /app/mysql/conf/my.cnf:/etc/my.cnf
      			- /app/mysql/init:/docker-entry-point-initdb.d
      		networks:
      			- atguigu_net
      		command: --default-authentication-plugin=mysql_native_password # 解决外部无法访问
      
      networks:
      	atguigu_net:
      ```

  - 第二次修改微服务工程 docker_boot

    - 写 YML：通过服务名访问，IP 无关
    - mvn package 命令将微服务形成新的 jar 包并上传到 Linux 服务器 /mydocker 目录下
    - 编写 Dockerfile
    - 构建镜像：`docker build -t zzyy_docker:1.6 .`

  - 执行  docker-compose up 或加上 -d

参考文档：

1. [《尚硅谷Docker实战教程》笔记.md](../视频笔记/《尚硅谷Docker实战教程》笔记.md) P78 ~ 86

# Linux

## 创建文件

- `touch a.txt`
- `vi a.txt`
- `vim a.txt`
- `> a.txt`
  - 覆盖
- `>> a.txt`
  - 追加
  - 两个箭头指令前面常用的
    - `echo`
    - `cat`
- `cp a.txt /test`
  - 复制一个空文件

参考文档：

1. [Linux创建文件的5种方式 - CSDN](https://blog.csdn.net/xtho62/article/details/118194873)

## 权限（rwx、chmod、chown）

- 权限简介

  - Linux系统上对文件的权限有着严格的控制，如果想对某个文件执行某种操作，必须具有对应的权限方可执行成功。
  - Linux下文件的权限类型一般包括**读，写，执行**。对应字母为 **r、w、x**。
  - Linux下权限的粒度有 **拥有者 、群组 、其它组** 三种。每个文件都可以针对三个粒度，设置不同的rwx(读写执行)权限。通常情况下，一个文件只能归属于一个用户和组， 如果其它的用户想有这个文件的权限，则可以将该用户加入具备权限的群组，一个用户可以同时归属于多个组。

- Linux上通常使用 **chmod 命令**对文件的权限进行设置和更改。

  - 一般使用格式

    ```shell
    chmod [可选项] <mode> <file…>
    ```

  - 参数说明：

    - ` [可选项] `
      - `-c`, `--changes`          like verbose but report only when a change is made (若该档案权限确实已经更改，才显示其更改动作)
      - `-f`, `--silent`, `--quiet`  suppress most error messages  （若该档案权限无法被更改也不要显示错误讯息）
      - `-v`, `--verbose`          output a diagnostic for every file processed（显示权限变更的详细资料）      
      - `--no-preserve-root`  do not treat '/' specially (the default)
      - `--preserve-root`    fail to operate recursively on '/'
      - `--reference=RFILE`  use RFILE's mode instead of MODE values
      - `-R`, `--recursive`        change files and directories recursively （以递归的方式对目前目录下的所有档案与子目录进行相同的权限变更)
      - `--help`		显示此帮助信息
      - `--version`		显示版本信息
    - `[mode]`
      - 权限设定字串，详细格式如下：`[ugoa...][[+-=][rwxX]...][,...]`
      - 其中
        - `[ugoa...]` 
          - u 表示该档案的拥有者，g 表示与该档案的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示所有（包含上面三者）。  
        - `[+-=]`
          - 表示增加权限，- 表示取消权限，= 表示唯一设定权限。
        - `[rwxX]`
          - r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该档案是个子目录或者该档案已经被设定过为可执行。
    - `[file...]`
      - 文件列表（单个或者多个文件、文件夹）

  - 范例： 

    - 设置所有用户可读取文件 a.conf

    ```shell
    chmod ugo+r a.conf 
    或 
    chmod a+r  a.conf
    ```

    - 设置 c.sh 只有 拥有者可以读写及执行

    ```shell
    chmod u+rwx c.sh
    ```

    - 设置文件 a.conf 与 b.xml 权限为拥有者与其所属同一个群组 可读写，其它组可读不可写

    ```shell
    chmod a+r,ug+w,o-w a.conf b.xml
    ```

    - 设置当前目录下的所有档案与子目录皆设为任何人可读写

    ```shell
    chmod -R a+rw *
    ```

  - 数字权限使用格式

    - 在这种使用方式中，首先我们需要了解数字如何表示权限。 首先，我们规定 **数字 4 、2 和 1表示读、写、执行权限**（具体原因可见下节权限详解内容），即 **r=4，w=2，x=1** 。

    - 此时其他的权限组合也可以用其他的八进制数字表示出来，如：

      - rwx = 4 + 2 + 1 = 7
        - 若要同时设置 rwx (可读写运行） 权限则将该权限位 设置 为 4 + 2 + 1 = 7
      - rw = 4 + 2 = 6
        - 若要同时设置 rw- （可读写不可运行）权限则将该权限位 设置 为 4 + 2 = 6
      - rx = 4 +1 = 5
        - 若要同时设置 r-x （可读可运行不可写）权限则将该权限位 设置 为 4 +1 = 5

    - 范例：

      - 设置所有人可以读写及执行

        ```shell
        chmod 777 file # 等价于 chmod u=rwx,g=rwx,o=rwx file 或 chmod a=rwx file)
        ```

      - 设置拥有者可读写，其他人不可读写执行

        ```shell
        chmod 600 file # 等价于 chmod u=rw,g=---,o=--- file 或 chmod u=rw,go-rwx file
        ```

- **更改文件拥有者（chown 命令）**

  - linux/Unix 是多人多工作业系统，每个的文件都有拥有者（所有者），如果我们想变更文件的拥有者（利用 chown 将文件拥有者加以改变），一般只有系统管理员(root)拥有此操作权限，而普通用户则没有权限将自己或者别人的文件的拥有者设置为别人。

  - 语法格式：

    - ```shell
      chown [可选项] user[:group] file…
      ```

    - 使用权限：root

    - 说明： 

      - `[可选项]` : 同上文chmod
      - user : 新的文件拥有者的使用者
      - group : 新的文件拥有者的使用者群体(group)

  - 范例：

    - 设置文件 d.key、e.scrt的拥有者设为 users 群体的 tom

    ```shell
    chown tom:users file d.key e.scrt
    ```

    - 设置当前目录下与子目录下的所有文件的拥有者为 users 群体的 James

    ```shell
    chown -R James:users  *
    ```

- **十二位权限（Linux附加权限）**

  - linux除了设置正常的读写操作权限外，还有关于一类设置也是涉及到权限，叫做 Linux 附加权限。包括 **SET位权限**（suid，sgid）和**粘滞位权限**（sticky）。

    - SET位权限：

      - suid/sgid是为了使“没有取得特权用户要完成一项必须要有特权才可以执行的任务”而产生的。

      - 一般用于给可执行的程序或脚本文件进行设置，其中SUID表示对属主用户增加SET位权限，SGID表示对属组内用户增加SET位权限。

      - 执行文件被设置了SUID、SGID权限后，任何用户执行该文件时，将获得该文件属主、属组账号对应的身份。

        - **suid(set User ID,set UID)** 的意思是进程执行一个文件时通常保持进程拥有者的UID。然而，如果设置了可执行文件的suid位，进程就获得了该文件拥有者的UID。

        - **sgid(set Group ID,set GID)** 意思也是一样，只是把上面的进程拥有者改成了文件拥有组（group）。

      - 在许多场景下，使用suid 和 sgid 非常实用，但是不恰当地使用这些权限可能为系统带来安全风险。所以应该尽量避免使用SET位权限程序。（passwd 命令是为数不多的必须要使用“suid”的命令之一）。

      - **SET位权限表示形式（10位权限）：**

        - 如果一个文件被设置了suid或sgid位，会分别表现在所有者或同组用户的权限的可执行位上；如果文件设置了suid还设置了x（执行）位，则相应的执行位表示为s(小写)。但是，如果没有设置x位，它将表示为S(大写)。如：

        - ```
          1、-rwsr-xr-x 表示设置了suid，且拥有者有可执行权限
          2、-rwSr--r-- 表示suid被设置，但拥有者没有可执行权限
          3、-rwxr-sr-x 表示sgid被设置，且群组用户有可执行权限
          4、-rw-r-Sr-- 表示sgid被设置，但群组用户没有可执行权限
          ```

    - **粘滞位权限：**

      - 粘滞位权限即sticky。一般用于为目录设置特殊的附加权限，当目录被设置了粘滞位权限后，即便用户对该目录有写的权限，也不能删除该目录中其他用户的文件数据。设置了粘滞位权限的目录，是用ls查看其属性时，其他用户权限处的x将变为t。 使用chmod命令设置目录权限时，+t、-t权限模式可分别用于添加、移除粘滞位权限。

      - **粘滞位权限表示形式（10位权限）**：

        - 一个文件或目录被设置了粘滞位权限，会表现在其他组用户的权限的可执行位上。如果文件设置了sticky还设置了x（执行）位，其他组用户的权限的可执行位为t(小写)。但是，如果没有设置x位，它将表示为T(大写)。如：

        - ```
          1、-rwsr-xr-t 表示设置了粘滞位且其他用户组有可执行权限
          2、-rwSr--r-T 表示设置了粘滞位但其他用户组没有可执行权限
          ```

  - 十二位的权限表示方法

    - 附加权限除了用十位权限形式表示外，还可以用用十二位字符表示。

    - ```
      11 10 9 8 7 6 5 4 3 2 1 0
      S  G  T r w x r w x r w x
      ```

    - SGT分别表示SUID权限、SGID权限、和 粘滞位权限，这十二位分别对应关系如下：

      - 第11位为SUID位，第10位为SGID位，第9位为sticky位，第8-0位对应于上面的三组rwx位（后九位）。

参考文档：

1. [Linux权限详解（chmod、600、644、700、711、755、777、4755、6755、7755）「建议收藏」- 腾讯云](https://cloud.tencent.com/developer/article/2069886)

## 文件夹和文件的读写执行的区别

- **对于文件而言**
  - 读权限允许用户标识读取该文件；
  - 写权限允许用户修改该文件；
  - 执行权限允许用户标识执行该文件：
    - 对于一个不可执行的文件来说，拥有执行权限是没有任何意义的；
    - 如果文件是一个程序或者某种类型的脚本时，那么它就是可执行的
  - 假设我们对其他用户关于脚本文件uu.sh赋予了只读的权限，那么其他用户可以查看该文件的内容，但是不能修改，且如果用sh uu.sh去执行该脚本时，脚本也是能执行的，因为这里使用sh，相当于进入了一个子进程sh，由于脚本文件uu.sh具有可读权限，那么sh会读取脚本文件中uu.sh中的每一行去执行，结果就是，脚本文件uu.sh被执行；如果只用uu.sh去执行该脚本文件，那么脚本将无法执行
- **对于目录而言**
  - 读权限允许用户标识读取目录中的文件名，只能列举目录中的文件名，不能进入该目录，相应也不能查看目录下各文件的大小；
  - 写权限允许用户标识修改目录（创建、移动、复制、删除）；
  - 执行权限允许用户搜索该目录：
    - 比如：我们在目录/home/test下建立了一个新目录test1，且该目录的权限为744，即没有可执行权限，那么：其他用户将不能搜索该目录，即执行下述搜索命令时，将显示没有权限执行：
      find /home/test/test1 -name ‘’ —->报错

参考文档

1. [linux下文件夹与文件的读/写/执行权的区别 - 简书](https://www.jianshu.com/p/2ddecfe78566)

## 查看端口占用情况

- Linux 查看端口占用情况可以使用 **lsof** 和 **netstat** 命令。

- lsof

  - lsof(list open files) 是一个列出当前系统打开文件的工具。

  - lsof 查看端口占用语法格式：

    ```
    lsof -i:端口号
    ```

  - 更多 lsof 的命令如下：

    ```
    lsof -i:8080：查看8080端口占用
    lsof abc.txt：显示开启文件abc.txt的进程
    lsof -c abc：显示abc进程现在打开的文件
    lsof -c -p 1234：列出进程号为1234的进程所打开的文件
    lsof -g gid：显示归属gid的进程情况
    lsof +d /usr/local/：显示目录下被进程开启的文件
    lsof +D /usr/local/：同上，但是会搜索目录下的目录，时间较长
    lsof -d 4：显示使用fd为4的进程
    lsof -i -U：显示所有打开的端口和UNIX domain文件
    ```

- netstat

  - **netstat -tunlp** 用于显示 tcp，udp 的端口和进程等相关情况。

  - netstat 查看端口占用语法格式：

    ```
    netstat -tunlp | grep 端口号
    ```

    - -t (tcp) 仅显示tcp相关选项
    - -u (udp)仅显示udp相关选项
    - -n 拒绝显示别名，能显示数字的全部转化为数字
    - -l 仅列出在Listen(监听)的服务状态
    - -p 显示建立相关链接的程序名

参考文档：

1. [Linux 查看端口占用情况 - RUNOOB](https://www.runoob.com/w3cnote/linux-check-port-usage.html)

## 系统调用

- 什么是系统调用

  - **Linux内核中设置了一组用于实现各种系统功能的子程序，称为系统调用**。用户可以通过系统调用命令在自己的应用程序中调用它们。
  - 从某种角度来看，系统调用和普通的函数调用非常相似。
    - 区别仅仅在于，**系统调用由操作系统核心提供，运行于核心态**;
    - 而普通的函数调用由函数库或用户自己提供，运行于用户态。
  - 随Linux核心还提供了一些C语言函数库，这些库对系统调用进行了一些包装和扩展，因为这些库函数与系统调用的关系非常紧密，所以习惯上把这些函数也称为系统调用。

- 为什么要用系统调用?

  - 实际上，很多已经被我们习以为常的C语言标准函数，在Linux平台上的实现都是靠系统调用完成的，所以如果想对系统底层的原理作深入的了解，掌握各种系统调用是初步的要求。进一步，若想成为一名Linux下编程高手，也就是我们常说的Hacker，其标志之一也是能对各种系统调用有透彻的了解。
  - 即使除去上面的原因，在平常的编程中你也会发现，在很多情况下，系统调用是实现你的想法的简洁有效的途径，所以有可能的话应该尽量多掌握一些系统调用，这会对你的程序设计过程带来意想不到的帮助。

- 系统调用是怎么工作的?

  - 一般的，进程是不能访问内核的。它不能访问内核所占内存空间也不能调用内核函数。CPU硬件决定了这些(这就是为什么它被称作"保护模式")。**系统调用是这些规则的一个例外**。其原理是进程先用适当的值填充寄存器，然后调用一个特殊的指令，这个指令会跳到一个事先定义的内核中的一个位置(当然，这个位置是用户进程可读但是不可写的)。在Intel CPU中，这个由中断0x80实现。硬件知道一旦你跳到这个位置，你就不是在限制模式下运行的用户，而是作为操作系统的内核--所以你就可以为所欲为。
  - 进程可以跳转到的内核位置叫做sysem_call。这个过程检查系统调用号，这个号码告诉内核进程请求哪种服务。然后，它查看系统调用表(sys_call_table)找到所调用的内核函数入口地址。接着，就调用函数，等返回后，做一些系统检查，最后返回到进程(或到其他进程，如果这个进程时间用尽)。

- 调用性能问题

  - **系统调用需要从用户空间陷入内核空间，处理完后，又需要返回用户空间**。其中除了系统调用服务例程的实际耗时外，陷入/返回过程和系统调用处理程序(查系统调用表、存储\恢复用户现场)也需要花销一些时间，这些时间加起来就是一个系统调用的响应速度。系统调用不比别的用户程序，它对性能要求很苛刻，因为它需要陷入内核执行，所以和其他内核程序一样要求代码简洁、执行迅速。幸好Linux具有令人难以置信的上下文切换速度，使得其进出内核都被优化得简洁高效;同时所有Linux系统调用处理程序和每个系统调用本身也都非常简洁。
  - 绝大多数情况下，Linux系统调用性能是可以接受的，但是对于一些对性能要求非常高的应用来说，它们虽然希望利用系统调用的服务，但却希望加快相应速度，避免陷入/返回和系统调用处理程序带来的花销，因此采用由内核直接调用系统调用服务例程，最好的例子就HTTPD——它为了避免上述开销，从内核调用socket等系统调用服务例程。

- Linux系统调用列表

  - 进程控制
    - fork 创建一个新进程
    - clone 按指定条件创建子进程
    - execve 运行可执行文件
    - exit 中止进程
    - _exit 立即中止当前进程
    - getdtablesize 进程所能打开的最大文件数
    - getpgid 获取指定进程组标识号
    - setpgid 设置指定进程组标志号
    - getpgrp 获取当前进程组标识号
    - setpgrp 设置当前进程组标志号
    - getpid 获取进程标识号
    - getppid 获取父进程标识号
    - getpriority 获取调度优先级
    - setpriority 设置调度优先级
    - modify_ldt 读写进程的本地描述表
    - nanosleep 使进程睡眠指定的时间
    - nice 改变分时进程的优先级
    - pause 挂起进程，等待信号
    - personality 设置进程运行域
    - prctl 对进程进行特定操作
    - ptrace 进程跟踪
    - sched_get_priority_max 取得静态优先级的上限
    - sched_get_priority_min 取得静态优先级的下限
    - sched_getparam 取得进程的调度参数
    - sched_getscheduler 取得指定进程的调度策略
    - sched_rr_get_interval 取得按RR算法调度的实时进程的时间片长度
    - sched_setparam 设置进程的调度参数
    - sched_setscheduler 设置指定进程的调度策略和参数
    - sched_yield 进程主动让出处理器,并将自己等候调度队列队尾
    - vfork 创建一个子进程，以供执行新程序，常与execve等同时使用
    - wait 等待子进程终止
    - wait3 参见wait
    - waitpid 等待指定子进程终止
    - wait4 参见waitpid
    - capget 获取进程权限
    - capset 设置进程权限
    - getsid 获取会晤标识号
    - setsid 设置会晤标识号
  - 文件系统控制
    1. 文件读写操作
       - fcntl 文件控制
       - open 打开文件
       - creat 创建新文件
       - close 关闭文件描述字
       - read 读文件
       - write 写文件
       - readv 从文件读入数据到缓冲数组中
       - writev 将缓冲数组里的数据写入文件
       - pread 对文件随机读
       - pwrite 对文件随机写
       - lseek 移动文件指针
       - _llseek 在64位地址空间里移动文件指针
       - dup 复制已打开的文件描述字
       - dup2 按指定条件复制文件描述字
       - flock 文件加/解锁
       - **poll I/O多路转换**
       - truncate 截断文件
       - ftruncate 参见truncate
       - umask 设置文件权限掩码
       - **fsync 把文件在内存中的部分写回磁盘**
    2. 文件系统操作
       - access 确定文件的可存取性
       - chdir 改变当前工作目录
       - fchdir 参见chdir
       - chmod 改变文件方式
       - fchmod 参见chmod
       - chown 改变文件的属主或用户组
       - fchown 参见chown
       - lchown 参见chown
       - chroot 改变根目录
       - stat 取文件状态信息
       - lstat 参见stat
       - fstat 参见stat
       - statfs 取文件系统信息
       - fstatfs 参见statfs
       - readdir 读取目录项
       - getdents 读取目录项
       - mkdir 创建目录
       - mknod 创建索引节点
       - rmdir 删除目录
       - rename 文件改名
       - link 创建链接
       - symlink 创建符号链接
       - unlink 删除链接
       - readlink 读符号链接的值
       - mount 安装文件系统
       - umount 卸下文件系统
       - ustat 取文件系统信息
       - utime 改变文件的访问修改时间
       - utimes 参见utime
       - quotactl 控制磁盘配额
  - 系统控制
    - ioctl I/O总控制函数
    - _sysctl 读/写系统参数
    - acct 启用或禁止进程记账
    - getrlimit 获取系统资源上限
    - setrlimit 设置系统资源上限
    - getrusage 获取系统资源使用情况
    - uselib 选择要使用的二进制函数库
    - ioperm 设置端口I/O权限
    - iopl 改变进程I/O权限级别
    - outb 低级端口操作
    - reboot 重新启动
    - swapon 打开交换文件和设备
    - swapoff 关闭交换文件和设备
    - bdflush 控制bdflush守护进程
    - sysfs 取核心支持的文件系统类型
    - sysinfo 取得系统信息
    - adjtimex 调整系统时钟
    - alarm 设置进程的闹钟
    - getitimer 获取计时器值
    - setitimer 设置计时器值
    - gettimeofday 取时间和时区
    - settimeofday 设置时间和时区
    - stime 设置系统日期和时间
    - time 取得系统时间
    - times 取进程运行时间
    - uname 获取当前UNIX系统的名称、版本和主机等信息
    - vhangup 挂起当前终端
    - nfsservctl 对NFS守护进程进行控制
    - vm86 进入模拟8086模式
    - create_module 创建可装载的模块项
    - delete_module 删除可装载的模块项
    - init_module 初始化模块
    - query_module 查询模块信息
    - *get_kernel_syms 取得核心符号,已被query_module代替
  - 内存管理
    - brk 改变数据段空间的分配
    - sbrk 参见brk
    - mlock 内存页面加锁
    - munlock 内存页面解锁
    - mlockall 调用进程所有内存页面加锁
    - munlockall 调用进程所有内存页面解锁
    - **mmap 映射虚拟内存页**
    - munmap 去除内存页映射
    - mremap 重新映射虚拟内存地址
    - msync 将映射内存中的数据写回磁盘
    - mprotect 设置内存映像保护
    - getpagesize 获取页面大小
    - sync 将内存缓冲区数据写回硬盘
    - cacheflush 将指定缓冲区中的内容写回磁盘
  - 网络管理
    - getdomainname 取域名
    - setdomainname 设置域名
    - gethostid 获取主机标识号
    - sethostid 设置主机标识号
    - gethostname 获取本主机名称
    - sethostname 设置主机名称
  - socket控制
    - socketcall socket系统调用
    - socket 建立socket
    - bind 绑定socket到端口
    - connect 连接远程主机
    - accept 响应socket连接请求
    - send 通过socket发送信息
    - sendto 发送UDP信息
    - sendmsg 参见send
    - recv 通过socket接收信息
    - recvfrom 接收UDP信息
    - recvmsg 参见recv
    - listen 监听socket端口
    - **select 对多路同步I/O进行轮询**
    - shutdown 关闭socket上的连接
    - getsockname 取得本地socket名字
    - getpeername 获取通信对方的socket名字
    - getsockopt 取端口设置
    - setsockopt 设置端口参数
    - **sendfile 在文件或端口间传输数据**
    - socketpair 创建一对已联接的无名socket
  - 用户管理
    - getuid 获取用户标识号
    - setuid 设置用户标志号
    - getgid 获取组标识号
    - setgid 设置组标志号
    - getegid 获取有效组标识号
    - setegid 设置有效组标识号
    - geteuid 获取有效用户标识号
    - seteuid 设置有效用户标识号
    - setregid 分别设置真实和有效的的组标识号
    - setreuid 分别设置真实和有效的用户标识号
    - getresgid 分别获取真实的,有效的和保存过的组标识号
    - setresgid 分别设置真实的,有效的和保存过的组标识号
    - getresuid 分别获取真实的,有效的和保存过的用户标识号
    - setresuid 分别设置真实的,有效的和保存过的用户标识号
    - setfsgid 设置文件系统检查时使用的组标识号
    - setfsuid 设置文件系统检查时使用的用户标识号
    - getgroups 获取后补组标志清单
    - setgroups 设置后补组标志清单
  - 进程间通信
    - ipc 进程间通信总控制调用
  - 信号
    - sigaction 设置对指定信号的处理方法
    - sigprocmask 根据参数对信号集中的信号执行阻塞/解除阻塞等操作
    - sigpending 为指定的被阻塞信号设置队列
    - sigsuspend 挂起进程等待特定信号
    - signal 参见signal
    - kill 向进程或进程组发信号
    - *sigblock 向被阻塞信号掩码中添加信号,已被sigprocmask代替
    - *siggetmask 取得现有阻塞信号掩码,已被sigprocmask代替
    - *sigsetmask 用给定信号掩码替换现有阻塞信号掩码,已被sigprocmask代替
    - *sigmask 将给定的信号转化为掩码,已被sigprocmask代替
    - *sigpause 作用同sigsuspend,已被sigsuspend代替
    - sigvec 为兼容BSD而设的信号处理函数,作用类似sigaction
    - ssetmask ANSI C的信号处理函数,作用类似sigaction
  - 消息
    - msgctl 消息控制操作
    - msgget 获取消息队列
    - msgsnd 发消息
    - msgrcv 取消息
  - 管道
    - pipe 创建管道
  - 信号量
    - semctl 信号量控制
    - semget 获取一组信号量
    - semop 信号量操作
  - 共享内存
    - shmctl 控制共享内存
    - shmget 获取共享内存
    - shmat 连接共享内存
    - shmdt 拆卸共享内存

参考文档：

1. [什么是系统调用?为什么要用系统调用? - CSDN](https://blog.csdn.net/haigand/article/details/90284303)

## IO 多路复用

epoll 是现在最先进的 IO 多路复用器，Redis、Nginx、Linux 中的 Java NIO 都使用的是 epoll

|                      | select                                                | poll                                                | epoll                                                        |
| -------------------- | ----------------------------------------------------- | --------------------------------------------------- | ------------------------------------------------------------ |
| 操作方式             | 遍历                                                  | 遍历                                                | 回调                                                         |
| 数据结构             | bitmap                                                | 数组                                                | 红黑树                                                       |
| 最大连接数           | 1024(x86) 或 2048（x64）                              | 无上限                                              | 无上限                                                       |
| 最大支持文件描述符数 | 一般有最大值限制                                      | 65535                                               | 65535                                                        |
| fd 拷贝              | 每次调用 select，都需要把 fd 集合从用户态拷贝到内核态 | 每次调用 poll，都需要把 fd 集合从用户态拷贝到内核态 | fd 首次调用 epoll_ctl 拷贝，每次调用 epoll_wait 不拷贝       |
| 工作效率             | 每次调用都进行线性遍历，时间复杂度为 O(n)             | 每次调用都进行线性遍历，时间复杂度 O(n)             | 事件通知方式，每当 fd 就绪，系统注册的回调函数就会被调用，将就绪 fd 放到 readyList 里面，时间复杂度 O(1) |

## 软链接和硬链接

- 软链接与硬链接是用来干什么的呢？

  - 为解决文件的共享使用，Linux 系统引入了两种链接：硬链接 (hard link) 与软链接（又称符号链接，即 soft link 或 symbolic link）。
  - 链接为 Linux 系统解决了文件的共享使用，还带来了隐藏文件路径、增加权限安全及节省存储等好处。

- 硬链接

  - 特性说明

    - **由于linux下的文件是通过索引节点（Inode）来识别文件，硬链接可以认为是一个指针，指向文件索引节点的指针，系统并不为它重新分配inode**。每添加一个一个硬链接，文件的链接数就加1。
    - 硬连接之间没有主次之分，删除某个硬链接，只是将其从目录的数据块中删除相关信息，并且文件链接数减一。不会从inode表中删除inode,除非只剩下一个链接数。

  - 使用：

    - ```shell
      #在hardlink内创建文件aa.txt
      touch aa.txt
      #查看其链接数为1
      #注意，此处链接数就是硬链接的次数
      ls -il
      #创建硬链接
      ln aa.txt aa.link
      ls -il
      #往aa.txt文件中写入一些内容
      echo "123" >aa.txt
      #发现aa.link中也同样可以看到
      cat aa.txt
      cat aa.link
      #删除aa.txt
      rm -f aa.txt
      #aa.link依然可以正常查看内容
      cat aa.link 
      #l链接数 -1，inode不变
      ls -il
      ```

    - ls 选项

      - **-i** 列出当前目录下所有可见文件的inode号
      - **-l** 列出当前目录下可见文件的详细信息

  - 那这时候，就有个疑问了，如果链接数只剩下一个，再次删除，会进行什么样的操作呢？

    - 实际上，如果我们这时候再删除aa.link这个文件，系统只会删除掉inode table中aa.link指向的inode信息，和hardlink 目录项中aa.link的相关信息。然后会将aa.link inode指针所指向的数据块设置为空闲的状态，告诉系统这些数据块可以被再次使用。而里面的内容却不会被删除，直到新的数据覆盖为止！
    - 这也就说明了，实际上 **linux中被删除的文件数据是可以被找回的，只要相应的数据块没有被再次覆盖使用**。

  - 存在的问题

    1. **无法跨分区，跨设备创建硬链接**
       - 因为每个分区都有自己独立的inode体系，假设A分区的文件在B分区做了一个硬链接，此时访问B分区的此链接，按照我们想的是需要它访问A分区的inode,进行数据查询，但是它只会根据B分区的inode，在B数据块中查找数据。就相当于两套独立的数据库，你不可能拿着A数据库的某个主键去B数据库搜索数据，是一样的道理。
    2. **无法创建文件夹的硬链接**
       - 至于为何设置为无法创建文件夹，这里还是用反推来验证。假设可以创建文件夹的硬链接：**ln /hardlink /test/dir.link**。那么有个问题，首当其冲。
       - hardlink目录下的 . 是当前目录的意思，此处代表hardlink，那dir.link中的 `**.**` 是代表test目录呢？还是 hardlink目录呢？
       - 其次，假设存在目录 /A/B.link 和 /B/A.link。如果B.link是B目录的硬链接，A.link是
         A目录的硬链接。那A.link既然是/A的链接，那它里面肯定有B.link。同理B.link里面肯定有A.link。这样依次循环 /A/B.link/A.link/B.link/A.link/…。就造成了死循环的现象。这也就是为什么不允许创建文件夹硬链接的原因了。

  - 由于硬链接局限性比较多，所以工作中使用的不多。

- 软链接

  - （又称符号链接，即 soft link 或 symbolic link）：**相当于我们 Windows 中的快捷方式，即如果你软链接一个目录，只是一个目录的快捷方式到指定位置，操作系统找这个快捷方式会直接找到真实目录下的文件。**

  - 使用

    - ```shell
      touch aa.txt
      # 创建软链接
      ln -s aa.txt soft.link
      ls -il
      ```

    - **注意 `ln -s` 多了 -s 选项**

    - 如上可以看出，软连接与原文件并不是同一inode，链接数也没有增加。并且为何连大小也不一样呢？

    - 这里我们写入一些内容到aa.txt中。再次查看，原文件大小发生了改变，而链接文件大小依旧没变化。

  - 这其实就是软链接的特性之一，**因为软链接的inode指向的数据块保存的是 原文件的路径，如果没有路径，是由文件名，默认会在软链接所在路径查找**

    - **创建的软连接，指向的文件，默认会以软链接的路径为主，去寻找指向的文件，所以创建时，请以软链接的路径作为起点路径 去写原文件的相对路径**

  - 那既然相对路径创建这么麻烦，为什么还要使用这种方式呢？

    - 因为使用相对路径的话，迁移只要相对迁移，不会影响链接的使用，更加灵活。
    - 而绝对路径的软链接，则必须要求路径的正确性。

  - **而由于软链接 inode指向的数据块只保存 原文件的地址字符串，所以可以跨分区、跨设备创建，并且文件夹也可以创建。当然如果原文件被删除了，链接则也会失效，无法向硬链接那样拥有独立性。**

- 软链接与硬链接的区别

  1. **本质**：
     - 硬链接：同一个inode，只是多个名字。
     - 软链接：是不同的文件，inode不同
  2. **跨分区**
     - 硬链接无法跨分区、跨设备建立，软链接可以
  3. **目录**
     - 硬链接无法创建目录硬链接，软链接可以
  4. **相互关系**
     - 硬链接没有主次之分，相互独立
     - 软链接依赖于原文件，原文件被删除，软链接即不可用
  5. **链接数**
     - 硬链接会删除增加会影响链接数，软链接不会，因为inode不一样。
  6. **相对路径**
     - 硬链接创建时，原始文件路径是相对于当前路径。
     - 软链接创建时，原始文件路径是 相对于软链接的路径
  7. **文件类型**
     - 硬链接的类型与原始文件类型一致，软链接则会显示 symbolic link
  8. **创建方式**
     - 硬链接创建：ln [原文件] [硬链接]
     - 软链接创建：ln -s [原文件] [软链接]

参考文档：

1. [软链接与硬链接 详细讲解](https://blog.csdn.net/Giyomwd/article/details/104143426)

## 查看文件有多少硬链接和软链接

- 问题1：我创建了一个硬链接文件，但是我不知道放哪里了，请问怎么办？

  - 思路：

    - 查找文件可以利用 find 命令
    - 硬链接的特性，不会跨文件系统，所以源文件在哪个文件系统中，就在那里找
    - 硬链接文件和源文件具有相同的 inode 号，可以利用 find 的 -inum 参数和 -samefile 参数来查找
    - -num 参数表示按照 inode 号去查找，首先要查看文件的 inode 号
    - -samefile 参数表示查找和某个文件相同的 inode 的文件，不用查看文件的 inode 号

  - ```shell
    stat oldboy.txt # 获取文件的 inode 号码
    find / -inum 1486772>/dev/null # 根据 Inode 号码查找，把错误提示丢弃
    ```

- 问题2：我对文件 n 创建了一个软连接文件，但是我不知道放在哪里了，也不知道叫什么名字，请问怎么办？

  - 思路：

    - 软链接可以跨文件系统，并且 inode、属性都和源文件的不同没有规律可以寻
    - 当我们使用 ls -l 命令查看链接文件的时候，它都会以：filename -> link filename 方式显示
    - 我们寻找当前系统内所有的链接文件，然后过滤我们要找的源文件名 n，就能查看有哪些文件指向了源文件。

  - ```shell
    find -type l -exec ls -l {} \;
    find -type l -exec ls -l {}\;| grep 'oldboy.txt' # 找到文件后，长格式显示，然后过滤源文件名
    ```

参考文档：

1. [Linux下如何查看一个文件是否拥有软链接或硬链接文件？- 51CTO](https://blog.51cto.com/lixin15/1764780)

# 计算机网络

## HTTP 和 HTTPS 区别

- 加密性
  - HTTP 明文传输，数据都是未加密的，安全性较差
  - HTTPS（SSL+HTTP） 数据传输过程是加密的，安全性较好。
- 认证
  - 使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、GoDaddy 和 GlobalSign 等。
- 速度
  - HTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包
  - 而 HTTPS除了 TCP 的三个包，还要加上 ssl 握手需要的 9 个包，所以一共是 12 个包。
- 端口
  - http 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。
- 服务器资源
  - HTTPS 其实就是建构在 SSL/TLS 之上的 HTTP 协议，所以，要比较 HTTPS 比 HTTP 要更耗费服务器资源。

参考文档：

1. [HTTP 与 HTTPS 的区别 - RUNOOB](https://www.runoob.com/w3cnote/http-vs-https.html)

## HTTPS 保证安全

- HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。
  - SSL：（Secure Socket Layer，安全套接字层），位于可靠的面向连接的**网络层协议**和**应用层协议**之间的一种协议层。SSL通过互相认证、使用数字签名确保完整性、使用加密确保私密性，以实现客户端和服务器之间的安全通讯。该协议由两层组成：SSL记录协议和SSL握手协议。
    - SSL是TLS的前世。
  - TLS：(Transport Layer Security，传输层安全协议)，用于两个应用程序之间提供保密性和数据完整性。该协议由两层组成：TLS 记录协议和 TLS 握手协议。
    - TLS 以 SSL 3.0 为基础于 1999 年作为 SSL 的新版本推出。
    - SSL2.0 和 SSL3.0 已经被 IEFT 组织废弃（分别在 2011 年，2015 年）多年来，在被废弃的 SSL 协议中一直存在漏洞并被发现 (e.g. POODLE, DROWN)。大多数现代浏览器遇到使用废弃协议的 web 服务时，会降低用户体验（红线穿过挂锁标志或者https表示警告）来表现。
      - 因为这些原因，你应该在服务端禁止使用 SSL 协议，仅仅保留 TLS 协议开启。
- 常规的 HTTP 通信，有以下的问题。
  1. **窃听风险**（eavesdropping）：第三方可以获知通信内容。
  2. **篡改风险**（tampering）：第三方可以修改通信内容。
  3. **冒充风险**（pretending）：第三方可以冒充他人身份参与通信。
- 因此，SSL/TLS 协议就是为了解决这三大风险而设计的，希望达到：
  1. 所有信息都是**加密传播**，第三方无法窃听。
  2. 具有**校验机制**，一旦被篡改，通信双方会立刻发现。
  3. 配备**身份证书**，防止身份被冒充。

参考文档：

1. [Https 是如何保证安全的？ - 知乎](https://zhuanlan.zhihu.com/p/110216210)

## HTTP 1.0、1.1 和 2.0 的区别

- 结论
  - 结论1：从 HTTP/1.0 到 HTTP/2，都是利用 TCP 作为底层协议进行通信的。
  - 结论2：HTTP/1.1，引进了长连接(keep-alive)，减少了建立和关闭连接的消耗和延迟。
  - 结论3：HTTP/2，引入了多路复用：连接共享，提高了连接的利用率，降低延迟。

参考文档：

1. [HTTP1.0、HTTP1.1 和 HTTP2.0 的区别 - 知乎](https://zhuanlan.zhihu.com/p/370862112)

## TCP 可靠传输

- 实现了可靠性传输
  - 检验和
    - 通过检验和的方式，接收端可以检测出来数据是否有差错和异常，假如有差错就会直接丢弃TCP段，重新发送
  - 序列号/确认应答
    - 只要发送端有一个包传输，接收端没有回应确认包（ACK包），都会重发。或者接收端的应答包，发送端没有收到也会重发数据。这就可以保证数据的完整性。
  - 超时重传
    - 超时重传是指发送出去的数据包到接收到确认包之间的时间，如果超过了这个时间会被认为是丢包了，需要重传。
  - 最大消息长度
    - 在建立TCP连接的时候，双方约定一个最大的长度（MSS）作为发送的单位，重传的时候也是以这个单位来进行重传。理想的情况下是该长度的数据刚好不被网络层分块。
  - 滑动窗口控制
    - 我们上面提到的超时重传的机制存在效率低下的问题，发送一个包到发送下一个包要经过一段时间才可以。所以我们就想着能不能不用等待确认包就发送下一个数据包呢？这就提出了一个滑动窗口的概念。
    - 窗口的大小就是在无需等待确认包的情况下，发送端还能发送的最大数据量。这个机制的实现就是使用了大量的缓冲区，通过对多个段进行确认应答的功能。通过下一次的确认包可以判断接收端是否已经接收到了数据，如果已经接收了就从缓冲区里面删除数据。
  - 拥塞控制
    - 窗口控制解决了 两台主机之间因传送速率而可能引起的丢包问题，在一方面保证了TCP数据传送的可靠性。然而如果网络非常拥堵，此时再发送数据就会加重网络负担，那么发送的数据段很可能超过了最大生存时间也没有到达接收方，就会产生丢包问题。为此TCP引入慢启动机制，先发出少量数据，就像探路一样，先摸清当前的网络拥堵状态后，再决定按照多大的速度传送数据。

参考文档：

1. [TCP的可靠性传输是如何保证的 - 知乎](https://zhuanlan.zhihu.com/p/112317245)

## TCP 三次握手

- 过程
  - **第一次握手**：
    - 客户端将TCP报文**标志位SYN置为1**，随机产生一个序号值seq=J，保存在TCP首部的序列号(Sequence Number)字段里，指明客户端打算连接的服务器的端口，并将该数据包发送给服务器端
    - 发送完毕后，客户端进入`SYN_SENT`状态，等待服务器端确认。
  - **第二次握手**：
    - 服务器端收到数据包后由标志位SYN=1知道客户端请求建立连接
    - 服务器端将TCP报文**标志位SYN和ACK都置为1**，ack=J+1，随机产生一个序号值seq=K，并将该数据包发送给客户端以确认连接请求
    - 服务器端进入`SYN_RCVD`状态。
  - **第三次握手**：
    - 客户端收到确认后，检查ack是否为 J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给服务器端
    - 服务器端检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，客户端和服务器端进入`ESTABLISHED`状态
    - 完成三次握手，随后客户端与服务器端之间可以开始传输数据了。
- 为什么需要三次
  - 我们假设client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。
  - 假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。
  - 所以，采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。

参考文档：

1. [一文彻底搞懂 TCP三次握手、四次挥手过程及原理 - 知乎](https://zhuanlan.zhihu.com/p/108504297)

## TCP 半连接状态

- 客户端向服务端发起连接请求，服务器第一次收到客户端的SYN之后，就会处于**SYN_RCVD状态**，此时双方还没有完全建立其连接，服务器会把此种状态下请求连接放到一个队列里，我们将这种队列称之为**半连接队列**。
- **全连接队列**就是已经完成了三次握手，建立起连接就会放到全连接队列中去，如果全连接队列满了就可能会出现丢包的现象。
- 补充关于SYN-ACK重传次数的问题
  - 服务器发送完SYN-ACK包，如果未收到客户端确认包，服务器进行首次重传，等待一段时间仍未收到客户确认包，进行第二次重传。如果重传次数超过系统规定的最大重传次数，系统将该连接信息从半连接队列中删除。
  - 每次重传等待的时间一般不同，一般是指数增长。如时间间隔是1，2，4，8

参考文档：

1. [TCP之半连接状态 - CSDN](https://blog.csdn.net/weixin_44390164/article/details/119077124)

## TCP 四次挥手

- 四次挥手即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。

- 挥手请求可以是 Client 端，也可以是 Server 端发起的，我们假设是Client端发起：

  - **第一次挥手**： 
    - Client 端发起挥手请求，向Server端发送标志位是FIN报文段，设置序列号seq
    - 此时，Client端进入`FIN_WAIT_1`状态，这表示Client端没有数据要发送给Server端了。
  - **第二次分手**
    - Server 端收到了 Client 端发送的 FIN 报文段进入 `CLOSE_WAIT` 状态
    - Server 向 Client 端返回一个标志位是ACK的报文段，ack 设为 seq 加 1，Client 端收到后进入`FIN_WAIT_2`状态
    - Server端告诉Client端，我确认并同意你的关闭请求。
  - **第三次分手**： 
    - Server 端向 Client 端发送标志位是FIN的报文段，请求关闭连接
    - 同时 Server 端进入`LAST_ACK`状态。
  - **第四次分手** ： 
    - Client 端收到 Server 端发送的FIN报文段，向 Server 端发送标志位是 ACK 的报文段，然后 Client 端进入`TIME_WAIT` 状态。
    - Server端收到Client端的 ACK 报文段以后，就关闭连接。
    - 此时，Client 端等待 **2MSL** 的时间后依然没有收到回复，则证明 Server 端已正常关闭，那好，Client端也可以关闭连接了。

- 为什么需要四次

  - 建立连接时因为当Server端收到Client端的SYN连接请求报文后，可以直接发送**SYN+ACK**报文。其中ACK报文是用来应答的，SYN报文是用来同步的。所以建立连接只需要三次握手。
  - 由于TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议，TCP是**全双工模式**。这就意味着，关闭连接时，当Client端发出FIN报文段时，只是表示Client端告诉Server端数据已经发送完毕了。当Server端收到FIN报文并返回ACK报文段，表示它已经知道Client端没有数据发送了，但是Server端还是可以发送数据到Client端的，所以Server很可能并不会立即关闭SOCKET，直到Server端把数据也发送完毕。当Server端也发送了FIN报文段时，这个时候就表示Server端也没有数据要发送了，就会告诉Client端，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。

- 为什么要等待2MSL？

  **MSL**：报文段最大生存时间，它是任何报文段被丢弃前在网络内的最长时间。有以下两个原因：

  - **第一点：保证TCP协议的全双工连接能够可靠关闭**：
    由于IP协议的不可靠性或者是其它网络原因，导致了Server端没有收到Client端的ACK报文，那么Server端就会在超时之后重新发送FIN，如果此时Client端的连接已经关闭处于`CLOESD`状态，那么重发的FIN就找不到对应的连接了，从而导致连接错乱，所以，Client端发送完最后的ACK不能直接进入`CLOSED`状态，而要保持`TIME_WAIT`，当再次收到FIN的收，能够保证对方收到ACK，最后正确关闭连接。
  - **第二点：保证这次连接的重复数据段从网络中消失**
    如果Client端发送最后的ACK直接进入`CLOSED`状态，然后又再向Server端发起一个新连接，这时不能保证新连接的与刚关闭的连接的端口号是不同的，也就是新连接和老连接的端口号可能一样了，那么就可能出现问题：如果前一次的连接某些数据滞留在网络中，这些延迟数据在建立新连接后到达Client端，由于新老连接的端口号和IP都一样，TCP协议就认为延迟数据是属于新连接的，新连接就会接收到脏数据，这样就会导致数据包混乱。所以TCP连接需要在TIME_WAIT状态等待2倍MSL，才能保证本次连接的所有数据在网络中消失。

参考文档：

1. [一文彻底搞懂 TCP三次握手、四次挥手过程及原理 - 知乎](https://zhuanlan.zhihu.com/p/108504297)

## OSI 七层架构

- 七层模型，亦称OSI（Open System Interconnection）参考模型，是参考模型是国际标准化组织（ISO）制定的一个用于计算机或通信系统间互联的标准体系。它是一个七层的、抽象的模型体，不仅包括一系列抽象的术语或概念，也包括具体的协议。
- 分层如下。
  - **应用层 (Application)**
    - 网络服务与最终用户的一个接口。协议有：HTTP FTP TFTP SMTP SNMP DNS TELNET HTTPS POP3 DHCP
  - **表示层（Presentation Layer）**
    - 数据的表示、安全、压缩。（在五层模型里面已经合并到了应用层）格式有，JPEG、ASCll、DECOIC、加密格式等
  - **会话层（Session Layer）**
    - 建立、管理、终止会话。（在五层模型里面已经合并到了应用层）对应主机进程，指本地主机与远程主机正在进行的会话
  - **传输层 (Transport)**
    - 定义传输数据的协议端口号，以及流控和差错校验。协议有：TCP UDP，数据包一旦离开网卡即进入网络传输层
  - **网络层 (Network)**
    - 进行逻辑地址寻址，实现不同网络之间的路径选择。协议有：ICMP IGMP IP（IPV4 IPV6） ARP RARP
  - **数据链路层 (Link)**
    - 建立逻辑连接、进行硬件地址寻址、差错校验等功能。（由底层网络定义协议）将比特组合成字节进而组合成帧，用MAC地址访问介质，错误发现但不能纠正。
  - **物理层（Physical Layer）**
    - 建立、维护、断开物理连接。（由底层网络定义协议）

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景十二：讲讲七层网络模型与 TCP 三次握手与四次断开？ - OSI 七层架构

## 从输入 URL 到页面展示的过程

- 1、输入网址
- 2、DNS 解析
- 3、建立 tcp 连接
- 4、客户端发送 HTTP 请求
- 5、服务器处理请求
  - （可能出现重定向，可能的 Nginx 反向代理）
- 6、服务器响应请求
  - 状态码
    - 1xx：信息性状态码，表示服务器已接收了客户端请求，客户端可继续发送请求。
      - 100 Continue
      - 101 Switching Protocols
    - 2xx：成功状态码，表示服务器已成功接收到请求并进行处理。
      - 200 OK 表示客户端请求成功
      - 204 No Content 成功，但不返回任何实体的主体部分
      - 206 Partial Content 成功执行了一个范围（Range）请求
    - 3xx：重定向状态码，表示服务器要求客户端重定向。
      - 301 Moved Permanently 永久性重定向，响应报文的Location首部应该有该资源的新URL
      - 302 Found 临时性重定向，响应报文的Location首部给出的URL用来临时定位资源
      - 303 See Other 请求的资源存在着另一个URI，客户端应使用GET方法定向获取请求的资源
      - 304 Not Modified 服务器内容没有更新，可以直接读取浏览器缓存
      - 307 Temporary Redirect 临时重定向。与302 Found含义一样。302禁止POST变换为GET，但实际使用时并不一定，307则更多浏览器可能会遵循这一标准，但也依赖于浏览器具体实现
    - 4xx：客户端错误状态码，表示客户端的请求有非法内容。
      - 400 Bad Request 表示客户端请求有语法错误，不能被服务器所理解
      - 401 Unauthonzed 表示请求未经授权，该状态代码必须与 WWW-Authenticate 报头域一起使用
      - 403 Forbidden 表示服务器收到请求，但是拒绝提供服务，通常会在响应正文中给出不提供服务的原因
      - 404 Not Found 请求的资源不存在，例如，输入了错误的URL
    - 5xx：服务器错误状态码，表示服务器未能正常处理客户端的请求而出现意外错误。
      - 500 Internel Server Error 表示服务器发生不可预期的错误，导致无法完成客户端的请求
      - 503 Service Unavailable 表示服务器当前不能够处理客户端的请求，在一段时间之后，服务器可能会恢复正常
- 7、浏览器展示 HTML
  - 解析过程中，浏览器首先会解析HTML文件构建DOM树，然后解析CSS文件构建渲染树，等到渲染树构建完成后，浏览器开始布局渲染树并将其绘制到屏幕上。
- 8、浏览器发送请求获取其他在 HTML 中的资源
  - （部分可能从 CDN 获取；部分可能浏览器缓存）

参考文档：

1. [从输入URL到页面展示的详细过程 - CSDN](https://blog.csdn.net/wlk2064819994/article/details/79756669)

# 分布式系统

## CAP 理论

- CAP
  - Consistency（一致性）
    - 对于客户端的每次读操作，要么读到的是最新的数据，要么读取失败。换句话说，一致性是站在分布式系统的角度，对访问本系统的客户端的一种承诺：要么我给您返回一个错误，要么我给你返回绝对一致的最新数据，不难看出，其强调的是数据正确。
  - Availability（可用性）
    - 任何客户端的请求都能得到响应数据，不会出现响应错误。换句话说，可用性是站在分布式系统的角度，对访问本系统的客户的另一种承诺：我一定会给您返回数据，不会给你返回错误，但不保证数据最新，强调的是不出错。
  - Partition tolerance（分区容忍性）
    - 由于分布式系统通过网络进行通信，网络是不可靠的。当任意数量的消息丢失或延迟到达时，系统仍会继续提供服务，不会挂掉。换句话说，分区容忍性是站在分布式系统的角度，对访问本系统的客户端的再一种承诺：我会一直运行，不管我的内部出现何种数据同步问题，强调的是不挂掉。
- 权衡 C、A
  - 对于一个分布式系统而言，P是前提，必须保证，因为只要有网络交互就一定会有延迟和数据丢失，这种状况我们必须接受，必须保证系统不能挂掉。
  - 当选择了C（一致性）时，如果由于网络分区而无法保证特定信息是最新的，则系统将返回错误或超时。
  - 当选择了A（可用性）时，系统将始终处理客户端的查询并尝试返回最新的可用的信息版本，即使由于网络分区而无法保证其是最新的。
- 为什么不能同时满足三个
  - 可以举例：A 更新数据，此时 A、B 网络断开，B 未能同步，此时需要保证分区容错性的前提下，B 无法在满足一致性和可用性的同时返回最新的数据

参考文档：

1. [轻松理解CAP理论 - 知乎](https://zhuanlan.zhihu.com/p/50990721)

# 设计模式

## 七大基本原则

1. **单一职责原则 (Single Responsibility Principle)**
   - 单一职责原则表示一个模块的组成元素之间的功能相关性。从软件变化的角度来看，就一个类而言，应该仅有一个让它变化的原因；通俗地说，即一个类只负责一项职责。
   - 总结:
     1. SRP 是一个简单又直观的原则，但是在实际编码的过程中很难将它恰当地运用，需要结合实际情况进行运用。
     2. 单一职责原则可以降低类的复杂度，一个类仅负责一项职责，其逻辑肯定要比负责多项职责简单。
     3. 提高了代码的可读性，提高系统的可维护性。
2. **开放-关闭原则 (Open-Closed Principle)**
   - 开放-关闭原则表示软件实体 (类、模块、函数等等) 应该是可以被扩展的，但是不可被修改。(Open for extension, close for modification)
   - 如果一个软件能够满足 OCP 原则，那么它将有两项优点：
     1. 能够扩展已存在的系统，能够提供新的功能满足新的需求，因此该软件有着很强的适应性和灵活性。
     2. 已存在的模块，特别是那些重要的抽象模块，不需要被修改，那么该软件就有很强的稳定性和持久性。
   - 总结：
     1. OCP 可以具有良好的可扩展性，可维护性。
     2. 不可能让一个系统的所有模块都满足 OCP 原则，我们能做到的是尽可能地不要修改已经写好的代码，已有的功能，而是去扩展它。
3. **里氏替换原则 (Liskov Substitution Principle)**
   - 里氏替换原则告诉我们，当使用继承时候，类 B 继承类 A 时，除添加新的方法完成新增功能 P2，尽量不要修改父类方法预期的行为。
   - **里氏替换原则的重点在不影响原功能，而不是不覆盖原方法。**
   - 继承包含这样一层含义：父类中凡是已经实现好的方法（相对于抽象方法而言），实际上是在设定一系列的规范和契约，虽然它不强制要求所有的子类必须遵从这些契约，但是如果子类对这些非抽象方法任意修改，就会对整个继承体系造成破坏。而里氏替换原则就是表达了这一层含义。
   - 里氏替换原则通俗的来讲就是：子类可以扩展父类的功能，但不能改变父类原有的功能。
4. **依赖倒转原则 (Dependence Inversion Principle)**
   - 定义：高层模块不应该依赖低层模块，二者都应该于抽象。进一步说，抽象不应该依赖于细节，细节应该依赖于抽象。
   - **依赖倒转原则的核心思想就是面向接口编程**
   - 所以遵循依赖倒转原则可以降低类之间的耦合性，提高系统的稳定性，降低修改程序造成的风险。**依赖倒转原则的核心就是要我们面向接口编程，理解了面向接口编程，也就理解了依赖倒转。**
5. **接口隔离原则 (Interface Segregation Principle)**
   - 接口隔离原则，其 "隔离" 并不是准备的翻译，真正的意图是 “分离” 接口(的功能)
   - 接口隔离原则强调：**客户端不应该依赖它不需要的接口；一个类对另一个类的依赖应该建立在最小的接口上。**
   - 总结：
     1. 接口隔离原则的思想在于建立单一接口，尽可能地去细化接口，接口中的方法尽可能少
     2. 但是凡事都要有个度，如果接口设计过小，则会造成接口数量过多，使设计复杂化。所以一定要适度。
6. **迪米特法则（Law Of Demeter）**
   - 迪米特法则又称为 最少知道原则，它表示一个对象应该对其它对象保持最少的了解。通俗来说就是，只与直接的朋友通信。
     - 首先来解释一下什么是直接的朋友：**每个对象都会与其他对象有耦合关系，只要两个对象之间有耦合关系，我们就说这两个对象之间是朋友关系**。
     - 耦合的方式很多，依赖、关联、组合、聚合等。其中，我们称出现成员变量、方法参数、方法返回值中的类为直接的朋友，而出现在局部变量中的类则不是直接的朋友。也就是说，陌生的类最好不要作为局部变量的形式出现在类的内部。
   - 对于被依赖的类来说，无论逻辑多么复杂，都尽量的将逻辑封装在类的内部，对外提供 public 方法，不对泄漏任何信息。
7. **组合/聚合复用原则 (Composite/Aggregate Reuse Principle)**
   - 组合/聚合复用原则就是在一个新的对象里面使用一些已有的对象，使之成为新对象的一部分; 新的对象通过向这些对象的委派达到复用已有功能的目的。
   - 在面向对象的设计中，如果直接继承基类，会破坏封装，因为继承将基类的实现细节暴露给子类；如果基类的实现发生了改变，则子类的实现也不得不改变；从基类继承而来的实现是静态的，不可能在运行时发生改变，没有足够的灵活性。**于是就提出了组合/聚合复用原则，也就是在实际开发设计中，尽量使用组合/聚合，不要使用类继承**。
   - 总结：
     1. 总体说来，组合/聚合复用原则告诉我们：组合或者聚合好过于继承。
     2. 聚合组合是一种 “黑箱” 复用，因为细节对象的内容对客户端来说是不可见的。

参考文档：

1. [设计模式之七大基本原则 - 知乎](https://zhuanlan.zhihu.com/p/24614363)

## 单例模式

- 实现方式

  1. 懒汉式，线程不安全

     **是否 Lazy 初始化：**是

     **是否多线程安全：**否

     ```java
     public class Singleton {  
         private static Singleton instance;  
         private Singleton (){}  
       
         public static Singleton getInstance() {  
             if (instance == null) {  
                 instance = new Singleton();  
             }  
             return instance;  
         }  
     }
     ```

  2. 懒汉式，线程安全

     **是否 Lazy 初始化：**是

     **是否多线程安全：**是

     ```java
     public class Singleton {  
         private static Singleton instance;  
         private Singleton (){}  
         public static synchronized Singleton getInstance() {  
             if (instance == null) {  
                 instance = new Singleton();  
             }  
             return instance;  
         }  
     }
     ```

  3. 饿汉式

     **是否 Lazy 初始化：**否

     **是否多线程安全：**是

     ```java
     public class Singleton {  
         private static Singleton instance = new Singleton();  
         private Singleton (){}  
         public static Singleton getInstance() {  
         return instance;  
         }  
     }
     ```

  4. 双检锁/双重校验锁（DCL，即 double-checked locking）

     **是否 Lazy 初始化：**是

     **是否多线程安全：**是

     ```java
     public class Singleton {  
         private volatile static Singleton singleton;  
         private Singleton (){}  
         public static Singleton getSingleton() {  
             if (singleton == null) {  
                 synchronized (Singleton.class) {  
                     if (singleton == null) {  
                         singleton = new Singleton();  
                     }  
                 }  
             }  
             return singleton;  
         }  
     }
     ```

  5. 登记式/静态内部类

     **是否 Lazy 初始化：**是

     **是否多线程安全：**是

     ```java
     public class Singleton {  
         private static class SingletonHolder {  
         private static final Singleton INSTANCE = new Singleton();  
         }  
         private Singleton (){}  
         public static final Singleton getInstance() {  
             return SingletonHolder.INSTANCE;  
         }  
     }
     ```

  6. 枚举

     **是否 Lazy 初始化：**否

     **是否多线程安全：**是

     ```java
     public enum Singleton {  
         INSTANCE;  
         public void whateverMethod() {  
         }  
     }
     ```

  7. 单例注册表

     单例注册表是Spring中Bean单例的核心实现方案。可以通过一个ConcurrentHashMap存储Bean对象，保证Bean名称唯一的情况下也能保证线程安全。

     spring依赖注入时，使用了 DCL 双重判断加锁 的单例模式

参考文档：

1. [单例模式 - RUNOOB](https://www.runoob.com/design-pattern/singleton-pattern.html)
2. [详解单例模式及其在Sping中的最优实践 - 知乎](https://zhuanlan.zhihu.com/p/442496004)

# 系统设计

## 大数据量：两个文件对比找相同

- **题目描述**
  - 给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，内存限制是 4G。请找出 a、b 两个文件共同的 URL。
- **解答思路**
  - 每个 URL 占 64B，那么 50 亿个 URL占用的空间大小约为 320GB。
    - 5,000,000,000 * 64B ≈ 5GB * 64 = 320GB
  - 由于内存大小只有 4G，因此，我们不可能一次性把所有 URL 加载到内存中处理。对于这种类型的题目，一般采用分治策略，即：把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。
- **思路如下：**
  - 首先遍历文件 a，对遍历到的 URL 求 hash(URL) % 1000，根据计算结果把遍历到的 URL 存储到 a0, a1, a2, …, a999，这样每个大小约为 300MB。
  - 使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, …, b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, …, a999 对应 b999，不对应的小文件不可能有相同的 URL。
  - 那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。
  - 接着遍历 ai( i∈[0,999])，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。
- **方法总结**
  - 分而治之，进行哈希取余；
  - 对每个子文件进行 HashSet 统计。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十六：BAT 大数据面试题 - 1、如何从大量的 URL 中找出相同的 URL？

## 大数据量：频度排序

- **题目描述**
  - 有 10 个文件，每个文件大小为 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求按照 query 的频度排序。
- **解答思路**
  - 如果 query 的重复度比较大，可以考虑一次性把所有 query 读入内存中处理；如果 query 的重复率不高，那么可用内存不足以容纳所有的 query，这时候就需要采用分治法或其他的方法来解决。
- **方法一：HashMap 法**
  - 如果 query 重复率高，说明不同 query 总数比较小，可以考虑把所有的 query 都加载到内存中的HashMap 中。接着就可以按照 query 出现的次数进行排序。
- **方法二：分治法**
  - 分治法需要根据数据量大小以及可用内存的大小来确定问题划分的规模。
  - 对于这道题，可以顺序遍历 10 个文件中的 query，通过 Hash 函数 hash(query) % 10 把这些 query 划分到 10 个小文件中。
  - 之后对每个小文件使用 HashMap 统计 query 出现次数，根据次数排序并写入到零外一个单独文件中。
  - 接着对所有文件按照 query 的次数进行排序，这里可以使用归并排序（由于无法把所有 query 都读入内存，因此需要使用外排序）。
- **方法总结**
  - 内存若够，直接读入进行排序；
  - 内存不够，先划分为小文件，小文件排好序后，整理使用外排序进行归并。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十六：BAT 大数据面试题 - 2、如何按照 query 的频度排序？（百度）

## 大数据量：去重后个数

- 8 位数
  - **题目描述**
    - 已知某个文件内包含一些电话号码，每个号码为 8 位数字，统计不同号码的个数。
  - **解答思路**
    - 这道题本质还是求解数据重复的问题，对于这类问题，一般首先考虑位图法。
    - 对于本题，8 位电话号码可以表示的号码个数为 10^8 个，即 1 亿个。我们每个号码用一个 bit 来表示，则总共需要 1 亿个 bit，内存占用约 100M。
  - **思路如下：**
    - 申请一个位图数组，长度为 1 亿，初始化为 0。
    - 然后遍历所有电话号码，把号码对应的位图中的位置置为 1。
    - 遍历完成后，如果 bit 为 1，则表示这个电话号码在文件中存在，否则不存在。bit 值为 1 的数量即为 不同电话号码的个数。
  - **方法总结**
    - 求解数据重复问题，记得考虑位图法。
- 2.5 亿个整数
  - **题目描述**
    - 在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。
  - **解答思路**
    - **方法一：分治法**
      - 与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。
    - **方法二：位图法**
      - 位图，就是用一个或多个 bit 来标记某个元素对应的值，而键就是该元素。采用位作为单位来存储数据，可以大大节省存储空间。
      - 位图通过使用位数组来表示某些元素是否存在。它可以用于快速查找，判重，排序等。不是很清楚？我先举个小例子。
    - 对于整数相关的算法的求解，位图法是一种非常实用的算法。假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 2^32。
    - 那么对于这道题，我们用 2 个 bit 来表示各个数字的状态：
      - 00 表示这个数字没出现过；
      - 01 表示这个数字出现过一次（即为题目所找的不重复整数）；
      - 10 表示这个数字出现了多次。
    - 那么这 2^32 个整数，总共所需内存为 2^32*2b=1GB。因此，当可用内存超过 1GB 时，可以采用位图法。
    - 假设内存满足位图法需求，进行下面的操作：
      - 遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。
  - **方法总结**
    - 判断数字是否重复的问题，位图法是一种非常高效的方法。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十六：BAT 大数据面试题 - 3、如何统计不同电话号码的个数？（百度）; 7、如何在大量的数据中找出不重复的整数？（百度）

## 大数据量：判断存在性

- **题目描述**
  - 给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中？
- **解答思路**
  - **方法一：分治法**
    - 依然可以用分治法解决，方法与前面类似，就不再次赘述了。
  - **方法二：位图法**
    - 40 亿个不重复整数，我们用 40 亿个 bit 来表示，初始位均为 0，那么总共需要内存：4,000,000,000b≈512M。
    - 我们读取这 40 亿个整数，将对应的 bit 设置为 1。接着读取要查询的数，查看相应位是否为 1，如果为 1 表示存在，如果为 0 表示不存在。
- **方法总结**
  - 判断数字是否存在、判断数字是否重复的问题，位图法是一种非常高效的方法。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十六：BAT 大数据面试题 - 8、如何在大量的数据中判断一个数是否存在？（腾讯）

## 大数据量：中位数

- **题目描述**
  - 从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为第 (N+1)/2 个数；当样本数为偶数时，中位数为 第 N/2 个数与第 1+N/2 个数的均值。
- **解答思路**
  - 如果这道题没有内存大小限制，则可以把所有数读到内存中排序后找出中位数。但是最好的排序算法的时间复杂度都为 O(NlogN)。这里使用其他方法。
  - **方法一：双堆法**
    - 维护两个堆，一个大顶堆，一个小顶堆。大顶堆中最大的数小于等于小顶堆中最小的数；保证这两个堆中的元素个数的差不超过 1。
    - 若数据总数为偶数，当这两个堆建好之后，中位数就是这两个堆顶元素的平均值。
    - 当数据总数为奇数时，根据两个堆的大小，中位数一定在数据多的堆的堆顶。
  - 以上这种方法，需要把所有数据都加载到内存中。当数据量很大时，就不能这样了，因此，这种方法适用于数据量较小的情况。5 亿个数，每个数字占用 4B，总共需要 2G 内存。如果可用内存不足 2G，就不能使用这种方法了，下面介绍另一种方法。
  - **方法二：分治法**
    - 分治法的思想是把一个大的问题逐渐转换为规模较小的问题来求解。
    - 对于这道题，顺序读取这 5 亿个数字，对于读取到的数字 num，如果它对应的二进制中最高位为1，则把这个数字写到 f1 中，否则写入 f0 中。通过这一步，可以把这 5 亿个数划分为两部分，而且 f0 中的数都大于 f1 中的数（最高位是符号位）。
    - 划分之后，可以非常容易地知道中位数是在 f0 还是 f1 中。假设 f1 中有 1 亿个数，那么中位数一定在 f0 中，且是在 f0 中，从小到大排列的第 1.5 亿个数与它后面的一个数的平均值。
    - 提示，5 亿数的中位数是第 2.5 亿与右边相邻一个数求平均值。若 f1 有一亿个数，那么中位数就是 f0 中从第 1.5 亿个数开始的两个数求得的平均值。
    - 对于 f0 可以用次高位的二进制继续将文件一分为二，如此划分下去，直到划分后的文件可以被加载到内存中，把数据加载到内存中以后直接排序，找出中位数。
    - 注意，当数据总数为偶数，如果划分后两个文件中的数据有相同个数，那么中位数就是数据较小的文件中的最大值与数据较大的文件中的最小值的平均值。
- **方法总结**
  - 分治法，真香！

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十六：BAT 大数据面试题 - 4、如何从 5 亿个数中找出中位数？（百度）

## 大数据量：Top N

- Top N
  - **题目描述**
    - 有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)。
  - **解答思路**
    - 由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用分治策略，把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。
  - **思路如下：**
    - 首先遍历大文件，对遍历到的每个词x，执行 hash(x) % 5000，将结果为 i 的词存放到文件 ai 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。
    - 接着统计每个小文件中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。具体方法是：对于遍历到的词 x，如果在 map 中不存在，则执行 map.put(x, 1)；若存在，则执行 map.put(x, map.get(x)+1)，将该词频数加 1。
    - 上面我们统计了每个小文件单词出现的频数。接下来，我们可以通过维护一个小顶堆来找出所有词中出现频数最高的 100 个。具体方法是：依次遍历每个小文件，构建一个小顶堆，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为小顶堆，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。
  - **方法总结**
    - 分而治之，进行哈希取余；
    - 使用 HashMap 统计频数；
    - 求解最大的 TopN 个，用小顶堆；求解最小的 TopN 个，用大顶堆。
- Top 1
  - **题目描述**
    - 现有海量日志数据保存在一个超大文件中，该文件无法直接读入内存，要求从中提取某天访问百度次数最多的那个 IP。
  - **解答思路**
    - 这道题只关心某一天访问百度最多的 IP，因此，可以首先对文件进行一次遍历，把这一天访问百度 IP 的相关信息记录到一个单独的大文件中。接下来采用的方法与上一题一样，大致就是先对 IP 进行哈希映射，接着使用 HashMap 统计重复 IP 的次数，最后计算出重复次数最多的 IP。
    - 注：这里只需要找出出现次数最多的 IP，可以不必使用堆，**直接用一个变量 max 即可**。
  - **方法总结**
    - 分而治之，进行哈希取余；
    - 使用 HashMap 统计频数；
    - 求解最大的 TopN 个，用小顶堆；求解最小的 TopN 个，用大顶堆。
- 字符串 Top N
  - **题目描述**
    - 搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询床的长度不超过 255 字节。
    - 假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。）
  - **解答思路**
    - 每个查询串最长为 255B，1000w 个串需要占用 约 2.55G 内存，因此，我们无法将所有字符串全部读入到内存中处理。
    - **方法一：分治法**
      - 分治法依然是一个非常实用的方法。
      - 划分为多个小文件，保证单个小文件中的字符串能被直接加载到内存中处理，然后求出每个文件中出现次数最多的 10 个字符串；最后通过一个小顶堆统计出所有文件中出现最多的 10 个字符串。
      - 方法可行，但不是最好，下面介绍其他方法。
    - **方法二：HashMap 法**
      - 虽然字符串总数比较多，但去重后不超过 300w，因此，可以考虑把所有字符串及出现次数保存在一个 HashMap 中，所占用的空间为 300w*(255+4)≈777M（其中，4表示整数占用的4个字节）。由此可见，1G 的内存空间完全够用。
      - 思路如下：
        - 首先，遍历字符串，若不在 map 中，直接存入 map，value 记为 1；若在 map 中，则把对应的 value 加 1，这一步时间复杂度 O(N)。
        - 接着遍历 map，构建一个 10 个元素的小顶堆，若遍历到的字符串的出现次数大于堆顶字符串的出现次数，则进行替换，并将堆调整为小顶堆。
        - 遍历结束后，堆中 10 个字符串就是出现次数最多的字符串。这一步时间复杂度 O(Nlog10)。
    - **方法三：前缀树法**
      - 方法二使用了 HashMap 来统计次数，当这些字符串有大量相同前缀时，可以考虑使用前缀树来统计字符串出现的次数，树的结点保存字符串出现次数，0 表示没有出现。
      - **思路如下：**
        - 在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构建新结点，构建完成后把叶子结点中字符串的出现次数置为 1。
        - 最后依然使用小顶堆来对字符串的出现次数进行排序。
      - **方法总结**
        - 前缀树经常被用来统计字符串的出现次数。它的另外一个大的用途是字符串查找，判断是否有重复的字符串等。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十六：BAT 大数据面试题 - 5、如何从大量数据中找出高频词？（百度）；6、如何找出某一天访问百度网站最多的 IP？（百度）；9、如何查询最热门的查询串？（腾讯）

# 算法

## 堆排序

```java
/**
 * 堆排序演示
 *
 * @author Lvan
 */
public class HeapSort {
    public static void main(String[] args) {
//        int[] arr = {5, 1, 7, 3, 1, 6, 9, 4};
        int[] arr = {16, 7, 3, 20, 17, 8};
        heapSort(arr);
        for (int i : arr) {
            System.out.print(i + " ");
        }
    }

    /**
     * 创建堆，
     * @param arr 待排序列
     */
    private static void heapSort(int[] arr) {
        //创建堆
        for (int i = (arr.length - 1) / 2; i >= 0; i--) {
            //从第一个非叶子结点从下至上，从右至左调整结构
            adjustHeap(arr, i, arr.length);
        }
        //调整堆结构+交换堆顶元素与末尾元素
        for (int i = arr.length - 1; i > 0; i--) {
            //将堆顶元素与末尾元素进行交换
            int temp = arr[i];
            arr[i] = arr[0];
            arr[0] = temp;
            //重新对堆进行调整
            adjustHeap(arr, 0, i);
        }
    }

    /**
     * 调整堆
     * @param arr 待排序列
     * @param parent 父节点
     * @param length 待排序列尾元素索引
     */
    private static void adjustHeap(int[] arr, int parent, int length) {
        //将temp作为父节点
        int temp = arr[parent];
        //左孩子
        int lChild = 2 * parent + 1;
        while (lChild < length) {
            //右孩子
            int rChild = lChild + 1;
            // 如果有右孩子结点，并且右孩子结点的值大于左孩子结点，则选取右孩子结点
            if (rChild < length && arr[lChild] < arr[rChild]) {
                lChild++;
            }
            // 如果父结点的值已经大于孩子结点的值，则直接结束
            if (temp >= arr[lChild]) {
                break;
            }
            // 把孩子结点的值赋给父结点
            arr[parent] = arr[lChild];

            //选取孩子结点的左孩子结点,继续向下筛选
            parent = lChild;
            lChild = 2 * lChild + 1;
        }
        arr[parent] = temp;
    }
}
```

参考文档：

1. [堆排序——Java实现 - 博客园](https://www.cnblogs.com/luomeng/p/10618709.html)

## 快速排序

```java
public class QuickSort {
	private void swap(int[] arr, int i, int j) {
		int temp = arr[i];
		arr[i] = arr[j];
		arr[j] = temp;
	}
	
	public void quickSort(int[] arr, int start, int end) {
		if (start >= end)
			return;
		int k = arr[start];
		int i = start, j = end;
		while (i != j) {
			while (i < j && arr[j] >= k)
				--j;
			swap(arr, i, j);
			while (i < j && arr[i] <= k)
				++i;
			swap(arr, i, j);
		}
		quickSort(arr, start, i - 1);
		quickSort(arr, i + 1, end);
	}
	
	public static void main(String[] args) {
		int[] arr = {5, 2, 6, 9, 1, 3, 4, 8, 7, 10};
		new QuickSort().quickSort(arr, 0, arr.length - 1);
		System.out.println(Arrays.toString(arr));
	}
}
```

参考文档：

1. [快速排序Java代码简洁实现 - 知乎](https://zhuanlan.zhihu.com/p/144738954)

## Java 排序实现

- Collections.sort()排序有两种实现方式
  - 一是让元素类实现Comparable接口并覆盖compareTo()方法
  - 二是给Collecitons.sort()方法传入比较器，通常采用匿名内部内的方式传入。
- Collections.sort() 通过调用Arrays.sort()方法进行排序
  - 在Java1.6+中，如果集合大小<32则采用Tim-Sort算法，如果>=32则采用归并排序。
  - ComparableTimSort.sort()
    - 如果**2<= nRemaining <=32**,即MIN_MERGE的初始值，表示需要排序的数组是小数组，**可以使用mini-TimSort方法进行排序**
      - mini-TimSort排序方法：先找出数组中从下标为0开始的第一个升序序列，或者找出降序序列后转换为升序重新放入数组，将这段升序数组作为初始数组，将之后的每一个元素通过二分法排序插入到初始数组中。注意，这里就调用到了我们重写的compareTo()方法了。
    - **否则 nRemaining > 32 需要使用归并排序**。

参考文档

1. [Java集合排序功能实现分析 - 博客园](https://www.cnblogs.com/dudadi/p/8007167.html)

