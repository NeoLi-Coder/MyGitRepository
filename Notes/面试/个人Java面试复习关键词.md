# Java 基础

## 父子类加载顺序

1. 父类静态成员变量
2. 父类静态代码块
3. 子类静态成员变量
4. 子类静态代码块
5. 父类非静态成员变量 
6. 父类非静态代码块
7. 父类构造方法
8. 子类非静态成员变量
9. 子类非静态代码块
10. 子类构造方法

这里帮大家小结几个特点：

1. 成员变量 > 代码块 > 构造方法（构造器）。
2. 静态（共有） > 非静态（私有）。
3. 子类静态 > 父类非静态（私有）。

参考文档：

1. [Java父子类加载顺序｜8月更文挑战 - 掘金](https://juejin.cn/post/6991707135117967373)

## 反射

- 具体使用、API
  - Class 类
    - **Class 实例的三种方式**
      - .class
      - 对象 .getClass()
      - Class.forName()
    - 常用方法
      - newInstance() 通过类的无参构造方法创建对象
      - getDeclaredXxxs() 获得类的所有的（属性/构造器/方法等）
      - getFields() 获得类的public类型的属性。
      - getField(String name) 获得类的指定属性
      - getMethods() 获得类的public类型的方法
      - getMethod (String name,Class [] args) 获得类的指定方法
      - getConstrutors() 获得类的public类型的构造方法
      - getConstrutor(Class[] args) 获得类的特定构造方法
      - getName() 获得类的完整名字
      - getPackage() 获取此类所属的包
      - getSuperclass() 获得此类的父类对应的Class对象
  - Field 类
    - set()
    - get()
  - Method 类
    - invoke()
  - Constructor 类
    - newInstance()
  - AccessibleObject
    - **setAccessible(boolean)**
- 使用场景
  - Spring AOP 动态代理
  - BeanUtils 拷贝
  - Spring IOC 加载 XML 配置文件
  - Spring 等框架的注解式使用
- 原理
  - 在程序运行时动态加载类并获取类的详细信息，从而操作类或对象的属性和方法。
  - 本质是 JVM 得到 class 对象之后，再通过 class 对象进行反编译，从而获取对象的各种信息。
- 缺点
  - 安全
  - 性能

参考文档：

1. [2024年面试准备-1-Java篇.md](./2024年面试准备-1-Java篇.md) 反射

## Channel

- 通道（Channel）是 **java.nio** 的第二个主要创新。

  - 它们既不是一个扩展也不是一项增强，而是全新、极好的Java I/O示例，提供与I/O服务的直接连接。
  - Channel用于在字节缓冲区和位于通道另一侧的实体（通常是一个文件或套接字）之间有效地传输数据。

- 通道是访问I/O服务的导管。

  - I/O可以分为广义的两大类别：File I/O和Stream I/O。
  - 那么相应地有两种类型的通道也就不足为怪了，它们是文件（file）通道和套接字（socket）通道。
  - 我们看到在api里有一个 **FileChannel** 类和三个 socket 通道类：**SocketChannel**、**ServerSocketChannel** 和 **DatagramChannel**。

- 通道可以以多种方式创建。

  - Socket 通道有可以直接创建新 socket 通道的工厂方法。
  - 但是一个FileChannel对象却只能通过在一个打开的RandomAccessFile、FileInputStream或FileOutputStream对象上调用getChannel( )方法来获取。**你不能直接创建一个 FileChannel 对象**。

- SocketChannel

  - 可以设置 SocketChannel 为非阻塞模式（non-blocking mode）.设置之后，就可以在异步模式下调用connect(), read() 和write()了。

    ```java
    socketChannel.configureBlocking(false);
    ```

  - 如果 SocketChannel 在非阻塞模式下，此时调用connect()，该方法可能在连接建立之前就返回了。为了确定连接是否建立，可以调用 finishConnect() 的方法。

- **Scatter/Gather**

  - 通道提供了一种被称为Scatter/Gather的重要新功能（有时也被称为矢量I/O）。它是指在多个缓冲区上实现一个简单的I/O操作。
  - 对于一个write操作而言，数据是从几个缓冲区按顺序抽取（称为gather）并沿着通道发送的。缓冲区本身并不需要具备这种gather的能力（通常它们也没有此能力）。该gather过程的效果就好比全部缓冲区的内容被连结起来，并在发送数据前存放到一个大的缓冲区中。
  - 对于read操作而言，从通道读取的数据会按顺序被散布（称为scatter）到多个缓冲区，将每个缓冲区填满直至通道中的数据或者缓冲区的最大空间被消耗完。
  - scatter / gather经常用于需要将传输的数据分开处理的场合，例如传输一个由消息头和消息体组成的消息，你可能会将消息体和消息头分散到不同的buffer中，这样你可以方便的处理消息头和消息体。
  - 使用得当的话，Scatter/Gather会是一个极其强大的工具。它允许你委托操作系统来完成辛苦活：将读取到的数据分开存放到多个存储桶（bucket）或者将不同的数据区块合并成一个整体。
    - 这是一个巨大的成就，因为操作系统已经被高度优化来完成此类工作了。它节省了您来回移动数据的工作，也就避免了缓冲区拷贝和减少了您需要编写、调试的代码数量。

参考文档：

1. [详解java NIO之Channel（通道） - 知乎](https://zhuanlan.zhihu.com/p/351327314)

# Java 集合

## ArrayList

- 继承关系

  ```mermaid
  classDiagram
  	AbstractList <|-- ArrayList
  	List <|.. ArrayList
  	RandomAccess <|.. ArrayList
  	Serializable <|.. ArrayList
  	Cloneable <|.. ArrayList
  	AbstractCollection <|-- AbstractList
  	Collection <|.. AbstractCollection
  	Collection <|-- List
  	Iterable <|-- Collection
  ```

  - 继承 `AbstractList<E>` 抽象类
    - 继承 `AbstractCollection<E>` 抽象类
      - 实现 `Collection<E>` 接口
        - 继承 `Iterable<T>` 接口
  - 实现 `List<E>` 接口
    - 继承 `Collection<E>` 接口
      - 继承 `Iterable<T>` 接口
  - 实现 `RandomAccess` 接口
  - 实现 `Serializable` 接口
  - 实现 `Cloneable` 接口

- 和 LinkedList 对比

  - ArrayList 基于数组，LinkedList 基于双向链表
    - 随机访问：ArrayList 实现 RandomAccess 标识接口，按索引访问 O(1)；LinkedList O(n)
    - 插入删除：ArrayList 插入删除 O(n)【除了尾插尾删 O(1)】; LinkedList 插入删除 O(n)【除了头尾插删 O(1)】（因为不能随机访问，得 O(n) 到达指定位置；LinkedList 注意特殊情况：使用 Iterator 删除时就都是 O(1) 了）
    - 内存占用：ArrayList 开辟的空间大于等于实际使用元素个数；LinkedList 元素节点大小比 ArrayList 大
  - 线程安全：都是线程不安全的
  - 一般场景都是直接用 ArrayList，LinkedList 对比之下基本没优势。（15年 LinkedList 作者发推说自己写了但自己也不用）
    - LinkedList 优势
      - 作为队列（Queue）使用
      - 作为栈使用
      - 头插删、Iterator 删除时 O(1)
      - 无需扩容

- 初始化长度

  - 以无参数构造方法创建 `ArrayList` 时
    - 实际上初始化赋值的是一个空数组。
    - 当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。（即扩容逻辑）
  - int 有参构造方法直接指定大小（小于0 异常），0 的时候也是空数组
  - Collection 有参构造方法长度和入参大小一样

- 扩容

  - **空数组直接扩容到 10**
  - 其他按照 **1.5 倍扩容**（grow() `int newCapacity = oldCapacity + (oldCapacity >> 1)`）
    - 注意例外的情况：比如 1，1.5 倍后还是 1。这时走特殊判断：检查新容量是否大于最小需要容量（原元素数 + 1），若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量。即 1 扩容到 2。
  - 如果新容量 - `MAX_ARRAY_SIZE`（`= Integer.MAX_VALUE - 8`）> 0，则执行 `hugeCapacity(minCapacity)` 方法（minCapacity 就是原元素数量 + 1）：
    - 如果入参 `minCapacity` 大于 `MAX_ARRAY_SIZE`，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 `MAX_ARRAY_SIZE`。
  - 可以使用 ensureCapacity() 方法在大量添加元素前主动指定大小

- 去重

  - 用 HashSet + for 循环去重
  - Stream：`lits.stream().distinct().collect(Collections.toList())`

参考文档：

1. [2024年面试准备-2-集合篇.md](./2024年面试准备-2-集合篇.md) ArrayList

## LinkedList

- 和 ArrayList 对比

  - 参考上面 ArrayList 中的具体分析

- 继承关系

  - ```mermaid
    classDiagram
        AbstractSequentialList <|-- LinkedList
        AbstractList <|-- AbstractSequentialList
        AbstractCollection <|-- AbstractList
        Collection <|.. AbstractCollection
        List <|.. LinkedList
        List <|.. AbstractList
        Collection <|-- List
        Cloneable <|.. LinkedList
        Serializable <|.. LinkedList
        Deque <|.. LinkedList
        Queue <|-- Deque
        Collection <|-- Queue
        Iterable <|-- Collection
    ```

- 优势

  - 作为队列（Queue）使用
    - 可以使用 **ArrayDeque** 替换（推荐，刷过力扣看过大佬代码就知道）
      - 数据结构：**循环数组**
      - 初始化和扩容：**初始 16，扩容 2 倍**
      - 也是线程不安全
      - 查询 O(1) 插入删除 O(n)
      - 插入新元素时不像 LinkedList 需要创建新的节点对象，但是可能扩容
  - 作为栈使用
    - 同样可以替换为 ArrayDeque 类（推荐）
    - 不推荐使用 Stack 类
      - 继承 Vector，方法有锁，效率慢
  - 头插删、Iterator 删除时 O(1)
  - 无需扩容

参考文档：

1. [2024年面试准备-2-集合篇.md](./2024年面试准备-2-集合篇.md) LinkedList

## HashMap

- 继承关系

  ```mermaid
  classDiagram
  	AbstractMap <|-- HashMap
  	Map <|.. AbstractMap
  	Map <|.. HashMap
  	Cloneable <|.. HashMap
  	Serializable <|.. HashMap
  ```

- 实现原理

  - 数组 + 链表/红黑树

    - table 数组：

      ```java
      transient Node<K,V>[] table;
      ```

    - Java 8 引入的红黑树

- **key 可以为 null，value 也可以为 null**

- 哈希冲突

  - HashMap 使用的是**链地址法**
  - 所有的方法
    - 开放定址法
      - 线性探测法
        - ThreadLocal：内部的 ThreadLocalMap 就是用的这种
      - 平方探测法（二次探测）
    - 再哈希法
    - 链地址法（也叫拉链法）
      - Redis 的 hash 也是
    - 建立公共溢出区

- put() 方法详解

  - 利用 key 的 hashCode() 在 hash() 中运算出 hash 值，再利用 & 计算出数组的 index;
  - 如果没碰撞直接放到桶里
  - 如果碰撞了，在链表或红黑树中判断是否存在 key。如果节点已经存在就替换 old value (保证 key 的唯⼀性)
  - 如果不存在，则如果是链表的形式就使用尾插法插入新节点（Java 8 起）
    - Java 7 的时候还是头插法，多线程扩容时会有链表死循环的风险
  - 如果插入会导致链表过长(大于等于 `TREEIFY_THRESHOLD`)，就把链表转换成红黑树（Java 8 起）；
  - 如果桶满了(超过 loadFactor * currentCapacity)，就要 resize()

- 重要变量

  - `DEFAULT_INITIAL_CAPACITY` 
    - Table 数组的初始化长度： `1 << 4` `2^4=16`
  - `MAXIMUM_CAPACITY`
    - Table 数组的最大长度： `1<<30` `2^30=1073741824`
  - `DEFAULT_LOAD_FACTOR`
    - 负载因子：默认值为 `0.75`。 当元素的总个数 > 当前数组的长度 * 负载因子。数组会进行扩容，扩容为原来的两倍
  - `TREEIFY_THRESHOLD`
    - 链表树化阈值： 默认值为 `8` 。表示在一个node（Table）节点下的值的个数大于 8 时候，会将链表转换成为红黑树。
  - `UNTREEIFY_THRESHOLD`
    - 红黑树链化阈值： 默认值为 `6` 。 表示在进行扩容期间，单个 Node 节点下的红黑树节点的个数小于 6 时候，会将红黑树转化成为链表。
  - `MIN_TREEIFY_CAPACITY = 64`
    - **最小树化阈值**，当 Table 所有元素超过改值，才会进行树化（为了防止前期阶段频繁扩容和树化过程冲突）。否则进行扩容

- 为什么长度要是 2 的 n 次方？

  - 如何保证的

    - 初始长度
      - 默认 2^4 = 16
      - 指定长度时，设置 threshold（扩容的阈值）的 tableSizeFor() 方法也会把值调整为 `MAXIMUM_CAPACITY`（2 ^ 30）以内的最接近入参的 2 的 n 次方数字。第一次 put 引起扩容初始化 table 时，就会以这个值为 table 数组大小
    - 扩容按 2 倍扩容
      - 一个桶中的元素正好只会对应分散到两个新桶

  - 计算 hash 值

    - 不是简单调用 hashCode()：

      ```java
      static final int hash(Object key) {
       	int h;
          return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
      }
      ```

  - 目的：都是 2 的 n 次方的话，计算所属哈希桶时，**可以使用 & 按位与优化 % 取模计算**（hash & 15 == hash % 16）

- Java 7 和 Java 8 的 HashMap 区别

  - 哈希冲突：链表 -> 链表/红黑树

    - Java 8 引入树化机制：链表长度大于等于 8 时，全表超过 64 个节点后，转红黑树；没超过最小树化阈值就扩容；红黑树节点数量小于等于 6 时，退化回链表。

  - 插入链表时：头插法 -> 尾插法

    - 避免多线程下扩容可能形成死循环链表

  - 扩容和插入时机：扩容后插入，转移数据时单独计算 -> 扩容前插入，转移数据时统一计算

    - 统一计算也是防止了扩容时形成死循环链表

  - 扩容后的存储位置：按 hashCode() 扰动处理 再 (h & length -1) 重新计算一遍 -> 扩容后的桶索引要么是原索引，要么是原索引+原容量

  - hash() 的实现：四次右移连续 ^  -> hashCode() ^ 一次右移

    - Java 7：

      ```java
      h ^= (h >>> 20) ^ (h >>> 12);
      return h ^ (h >>> 7) ^ (h >>> 4);
      ```

    - Java 8：

      ```java
      int h;
      return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
      ```

  - 初始化方法：单独方法 `inflateTable()` -> 直接在第一次 `put()` 时利用 `resize()` 初始化 table


参考文档：

1. [2024年面试准备-2-集合篇.md](./2024年面试准备-2-集合篇.md) HashMap

## ConcurrentHashMap

- 继承关系

  ```mermaid
  classDiagram
  	AbstractMap <|-- ConcurrentHashMap
  	Map <|.. AbstractMap
  	ConcurrentMap <|.. ConcurrentHashMap
  	Map <|-- ConcurrentMap
  	Serializable <|.. ConcurrentHashMap
  ```

- 线程安全

- **不支持 key、value 为 null**

  - 反证 value 不能为 null
    - 如果支持 value 为 null，外界使用时需要保证 containsKey() 和 get() 两次操作之间没有其他多线程在操作，否则 get() 的时候无法判断 null 的含义是真的 value 为 null 还是没有对应的 key。
    - 这就导致用户使用 ConcurrentHashMap 集合需要自己保证 containsKey() 和 get() 操作的原子性，增加了开发负担，所以在设计上直接禁止了 null
  - 而 key 不能为 null，主要就是 JUC 作者 Doug 不喜欢 null，所以设计之初就不允许 null 的 key 存在
  - **线程安全的 ConcurrentSkipListMap、HashTable 也是一样的禁止两者为 null**

- get()

  - 不需要加锁
    - 因为相关共享变量（不管是 7 的 HashEntry 还是 8 的 Node）都用 volatile 修饰，保证了多线程下的可见性
  - Java 7 源码逻辑
    - 首先，根据 key 计算出 hash 值定位到具体的 Segment 
    - 再根据 hash 值获取定位 HashEntry 对象
    - 对 HashEntry 对象进行链表遍历，找到对应元素。
  - Java 8 源码逻辑
    - 根据计算出来的 hashcode 寻址，如果就在桶上首节点，那么直接返回值。
    - 如果是红黑树那就按照树的方式获取值。
    - 都不满足那就按照链表的方式遍历获取值。

- put()

  - Java 7 源码逻辑
    - 先定位到相应的 Segment s，然后再对 s 进行 s.put() 操作。
    - 首先会尝试获取锁 `tryLock()`，如果获取失败肯定就有其他线程存在竞争，则利用 `scanAndLockForPut()` 自旋获取锁。
    - 尝试自旋获取锁。
    - 如果重试的次数达到了 `MAX_SCAN_RETRIES` 则改为阻塞锁获取，保证能获取成功。
    - 拿到锁后，得到 table 对应位置上的 HashEntry。如果非空，则遍历该 HashEntry，如果不为空则判断传入的 key 和当前遍历的 key 是否相等，相等则覆盖旧 value
    - HashEntry 为空则需要新建一个 HashEntry 并加入到 Segment 中，同时会判断是否需要扩容。
  - Java 8 源码逻辑
    - 根据 key 计算出 hashcode，然后开始遍历 table；
    - 判断是否需要初始化；
    - f 即为当前 key 定位出的 Node，如果为 null，表示当前位置可以写入数据，**利用 CAS 尝试写入，失败则自旋保证成功**。
      - 而在执行写操作时，会先尝试使用 CAS 操作来无锁地修改数据，如果 CAS 操作失败，则会使用 **synchronized** 来获取对应 Node 的锁，并在获取锁后进行数据的修改操作。
    - 如果当前位置的 hash 值 == MOVED == -1, 说明其他线程正在扩容，则需要参与一起扩容 `helpTransfer()`。
    - 如果都不满足，则利用 synchronized 锁住 f 节点，判断是链表还是红黑树，遍历写入数据。
    - 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。

- Java 7 和 8 的区别

  - 结构：数组+链表 -> 数组+链表/红黑树
  - 节点：**HashEntry** 类 -> **Node** 类（对 val 和 next 属性使用 volatile 修饰）
  - 锁：继承 ReentrantLock 的 **Segment 类，并发度 16**（自定义的话，取大于设置值的最小 2 的幂指数） -> **没有分段锁了，采用 CAS + synchronized 实现**，并发度取决于数组大小

- 扩容、哈希冲突树化等部分参数和 HashMap 都一样

  - 链表树化阈值 8
  - 最小树化容量 64
  - 初始大小 16
  - 复杂因子 0.75

参考文档：

1. [2024年面试准备-2-集合篇.md](./2024年面试准备-2-集合篇.md) ConcurrentHashMap

## Hashtable

- 较为远古的使用Hash算法的容器结构了，现在基本已被淘汰

- 注意一下类名是 Hashtable, t 小写！

- 继承结构

  ```mermaid
  classDiagram
  	Dictionary <|--Hashtable
  	Map <|.. Hashtable
  	Cloneable <|.. Hashtable
  	Serializable <|.. Hashtable
  ```

- 线程安全

  - 几乎所有都加了 synchronized 锁
  - 不允许 key、value 为 null，理由参考 ConcurrentHashMap

- 结构：数组 + 单向链表

- 默认初始长度 11

- 默认加载因子 0.75

- 求 hash 值：

  ```java
  int hash = key.hashCode();
  int index = (hash & 0x7FFFFFFF) % tab.length;
  ```

- 扩容机制：

  - 默认的阈值是选取 `initialCapacity * loadFactor` 和 `Integer.MAX_VALUE - 8 + 1` 的最小值
  - 新的长度 = 原长度 * 2倍 + 1
  - 头插法链表

- 目前不推荐使用，单线程可以用 HashMap，多线程用 ConcurrentHashMap

参考文档：

1. [2024年面试准备-2-集合篇.md](./2024年面试准备-2-集合篇.md) Hashtable

# JVM

## 对象结构

- 对象头（Header）
  - MarkWord（4 字节）
    - 其内容是一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等
  - Class 对象指针（4 字节）
    - 其指向的位置是对象对应的Class对象（其对应的元数据对象）的内存地址
- 实例数据（Instance Data）
  - 对象实际数据（实际数据大小）
    - 这里面包括了对象的所有成员变量，其大小由各个成员变量的大小决定，比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节
- 对齐填充（Padding）
  - 可选（按 8 字节对齐）

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景一：Java 中的 String 类占用多大的内存空间 - Java 对象的结构

## 对象什么时候进入老年代

- **大对象直接进入老年代**
  - 大对象就是指需要大量连续内存空间的Java对象，最典型的大对象便是那种很长的字符串，或者元素数量很庞大的数组
    - 本节例子中的byte[]数组就是典型的大对象
  - 大对象对虚拟机的内存分配来说就是一个不折不扣的坏消息，比遇到一个大对象更加坏的消息就是遇到一群“朝生夕灭”的“短命大对象”，我们写程序的时候应注意避免。
  - 在Java虚拟机中要避免大对象的原因
    - 在分配空间时，它容易导致内存明明还有不少空间时就提前触发垃圾收集，以获取足够的连续空间才能安置好它们
    - 当复制对象时，大对象就意味着高额的内存复制开销。
  - HotSpot 虚拟机提供了 **-XX:PretenureSizeThreshold** 参数，指定大于该设置值的对象直接在老年代分配，这样做的目的就是避免在 Eden 区及两个 Survivor 区之间来回复制，产生大量的内存复制操作。
    - -XX:PretenureSizeThreshold 参数只对 Serial 和 ParNew 两款新生代收集器有效，HotSpot 的其他新生代收集器，如 Parallel Scavenge 并不支持这个参数。
- **长期存活的对象将进入老年代**
  - 虚拟机给每个对象定义了一个对象年龄（Age）计数器，存储在对象头中
  - 对象通常在 Eden 区里诞生，如果经过第一次 Minor GC 后仍然存活，并且能被 Survivor 容纳的话，该对象会被移动到 Survivor 空间中，并且将其对象年龄设为 1 岁。对象在 Survivor 区中每熬过一次 Minor GC，年龄就增加 1 岁，当它的年龄增加到一定程度（**默认为 15**），就会被晋升到老年代中。
  - 对象晋升老年代的年龄阈值，可以通过参数 **-XX:MaxTenuringThreshold** 设置。
- **动态对象年龄判定**
  - 为了能更好地适应不同程序的内存状况，HotSpot 虚拟机并不是永远要求对象的年龄必须达到 -XX:MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 空间中**相同年龄所有对象大小的总和大于 Survivor 空间的一半**，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到 -XX:MaxTenuringThreshold 中要求的年龄。
- 题外话 - 其他分配方式：**栈上分配、TLAB、PLAB**
  - 栈上分配
    - 对于这些其他线程不会访问的对象，我们能不能让线程自己分配在它自己的栈空间上？这样不就可以节省不少交互时间了
    - 参考后续**逃逸分析**相关
  - TLAB
    - **TLAB（Thread Local Allocation Buffer），即线程本地分配缓存**。这是一块线程专用的内存分配区域，TLAB 占用的是 eden 区的空间。在 TLAB 启用的情况下（默认开启），JVM 会为每一个线程分配一块 TLAB 区域。
  - PLAB
    - **PLAB（Promotion Local Allocation Buffers），即晋升本地分配缓存**。PLAB 的作用于 TLAB 类似，都是为了加速对象分配效率，避免多线程竞争而诞生的。 只不过 **PLAB 是应用于对象晋升到 Survivor 区或老年代**。与 TLAB 类似，每个线程都有独立的 PLAB 区。
  - 对象内存分配流程
    - **尝试栈上分配 -> 尝试 TLAB 分配 -> 是否可以直接进入老年代 -> Eden 分配**


参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 3.8.2 大对象直接进入老年代 ~ 3.8.4 动态对象年龄判定

## 类加载时机和过程

- 一个类型从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期将会经历**加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（Using）和卸载（Unloading）**七个阶段
  - 其中**验证、准备、解析**三个部分统称为**连接（Linking）**
  - **加载、验证、准备、初始化和卸载这五个阶段的顺序是确定的**，类型的加载过程必须按照这种顺序按部就班地开始
    - 请注意，这里笔者写的是按部就班地“开始”，而不是按部就班地“进行”或按部就班地“完成”，强调这点是因为这些阶段通常都是互相交叉地混合进行的，会在一个阶段执行的过程中调用、激活另一个阶段。
  - **而解析阶段则不一定**：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定特性（也称为动态绑定或晚期绑定）。
- 关于在什么情况下需要开始类加载过程的第一个阶段“加载”，《Java虚拟机规范》中并没有进行强制约束，这点可以交给虚拟机的具体实现来自由把握
- 但是对于初始化阶段，《Java虚拟机规范》则是严格规定了**有且只有**六种情况必须立即对类进行“初始化”（而加载、验证、准备自然需要在此之前开始）：这六种场景中的行为称为**对一个类型进行主动引用**。
  - 遇到**new、getstatic、putstatic或invokestatic这四条字节码指令**时，如果类型没有进行过初始化，则需要先触发其初始化阶段。
    - 能够生成这四条指令的典型Java代码场景有：
      - 使用 **new 关键字**实例化对象的时候。
      - 读取或设置一个类型的**静态字段**（被final修饰、已在编译期把结果放入常量池的静态字段除外）的时候。
      - 调用一个类型的**静态方法**的时候。
  - 使用 **java.lang.reflect 包**的方法对类型进行**反射调用**的时候，如果类型没有进行过初始化，则需要先触发其初始化。
  - 当初始化类的时候，如果发现其**父类**还没有进行过初始化，则需要先触发其父类的初始化。
  - 当虚拟机启动时，用户需要指定一个要执行的**主类（包含main()方法的那个类）**，虚拟机会先初始化这个主类。
  - 当使用JDK 7新加入的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果为**REF_getStatic、REF_putStatic、REF_invokeStatic、REF_newInvokeSpecial四种类型的方法句柄**，并且这个方法句柄对应的类没有进行过初始化，则需要先触发其初始化。
  - 当一个接口中定义了JDK 8新加入的**默认方法（被default关键字修饰的接口方法）**时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 7.2 类加载的时机

## 类加载过程 1：加载

- 在加载阶段，Java虚拟机需要完成以下三件事情：
  - 通过一个类的全限定名来获取定义此类的**二进制字节流**。
    - 它并没有指明二进制字节流必须得从某个Class文件中获取，确切地说是根本没有指明要从哪里获取、如何获取。许多举足轻重的Java技术都建立在这一基础之上，例如：
      - 从ZIP压缩包中读取，这很常见，最终成为日后JAR、EAR、WAR格式的基础。
      - 从网络中获取，这种场景最典型的应用就是Web Applet。
      - 运行时计算生成，这种场景使用得最多的就是动态代理技术，在java.lang.reflect.Proxy中，就是用了ProxyGenerator.generateProxyClass()来为特定接口生成形式为“*$Proxy”的代理类的二进制字节流。
      - 由其他文件生成，典型场景是JSP应用，由JSP文件生成对应的Class文件。
      - 从数据库中读取，这种场景相对少见些，例如有些中间件服务器（如SAP Netweaver）可以选择把程序安装到数据库中来完成程序代码在集群间的分发。
      - 可以从加密文件中获取，这是典型的防Class文件被反编译的保护措施，通过加载时解密Class文件来保障程序运行逻辑不被窥探。
  - 将这个字节流所代表的静态存储结构转化为**方法区的运行时数据结构**。
  - 在内存中生成一个代表这个类的**java.lang.Class对象**，作为方法区这个类的各种数据的访问入口。
- 相对于类加载过程的其他阶段，**非数组类型的加载阶段（准确地说，是加载阶段中获取类的二进制字节流的动作）是开发人员可控性最强的阶段**。
  - 加载阶段既可以使用Java虚拟机里内置的引导类加载器来完成，也可以由用户自定义的类加载器去完成，开发人员通过定义自己的类加载器去控制字节流的获取方式（重写一个类加载器的findClass()或loadClass()方法），实现根据自己的想法来赋予应用程序获取运行代码的动态性。
- 对于数组类而言，情况就有所不同，**数组类本身不通过类加载器创建**，它是由Java虚拟机直接在内存中动态构造出来的。但数组类与类加载器仍然有很密切的关系，因为**数组类的元素类型（ElementType，指的是数组去掉所有维度的类型）最终还是要靠类加载器来完成加载**
  - 一个数组类（下面简称为C）创建过程遵循以下规则：
    - 如果数组的组件类型（Component Type，指的是数组去掉一个维度的类型，注意和前面的元素类型区分开来）是引用类型，那就递归采用本节中定义的加载过程去加载这个组件类型，数组C将被标识在加载该组件类型的类加载器的类名称空间上（这点很重要，在7.4节会介绍，一个类型必须与类加载器一起确定唯一性）。
    - 如果数组的组件类型不是引用类型（例如int[]数组的组件类型为int），Java虚拟机将会把数组C标记为与引导类加载器关联。
    - 数组类的可访问性与它的组件类型的可访问性一致，如果组件类型不是引用类型，它的数组类的可访问性将默认为public，可被所有的类和接口访问到。
  - 加载阶段结束后，Java虚拟机外部的二进制字节流就按照虚拟机所设定的格式存储在方法区之中了，方法区中的数据存储格式完全由虚拟机实现自行定义
- 加载阶段与连接阶段的部分动作（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的一部分，这两个阶段的开始时间仍然保持着固定的先后顺序。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 7.3.1 加载

## 类加载过程 2：验证

- **验证是连接阶段的第一步**，这一阶段的目的是确保Class文件的字节流中包含的信息符合《Java虚拟机规范》的全部约束要求，保证这些信息被当作代码运行后不会危害虚拟机自身的安全。
- 验证阶段大致上会完成下面四个阶段的检验动作
  - **文件格式验证**
    - 第一阶段要验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理
    - 这一阶段可能包括下面这些验证点：
      - 是否以魔数0xCAFEBABE开头。
      - 主、次版本号是否在当前Java虚拟机接受范围之内。
      - 常量池的常量中是否有不被支持的常量类型（检查常量tag标志）。
      - 指向常量的各种索引值中是否有指向不存在的常量或不符合类型的常量。
      - CONSTANT_Utf8_info型的常量中是否有不符合UTF-8编码的数据。
      - Class文件中各个部分及文件本身是否有被删除的或附加的其他信息。
      - ……
    - 该验证阶段的主要目的是保证输入的字节流能正确地解析并存储于方法区之内，格式上符合描述一个Java类型信息的要求。
    - 这阶段的验证是基于二进制字节流进行的，只有通过了这个阶段的验证之后，这段字节流才被允许进入Java虚拟机内存的方法区中进行存储
      - 所以后面的三个验证阶段全部是基于方法区的存储结构上进行的，不会再直接读取、操作字节流了。
  - **元数据验证**
    - 第二阶段是对字节码描述的信息进行语义分析，以保证其描述的信息符合《Java语言规范》的要求
    - 这个阶段可能包括的验证点如下：
      - 这个类是否有父类（除了java.lang.Object之外，所有的类都应当有父类）。
      - 这个类的父类是否继承了不允许被继承的类（被final修饰的类）。
      - 如果这个类不是抽象类，是否实现了其父类或接口之中要求实现的所有方法。
      - 类中的字段、方法是否与父类产生矛盾（例如覆盖了父类的final字段，或者出现不符合规则的方法重载，例如方法参数都一致，但返回值类型却不同等）。
      - ……
    - 第二阶段的主要目的是对类的元数据信息进行语义校验，保证不存在与《Java语言规范》定义相悖的元数据信息。
  - **字节码验证**
    - 第三阶段是整个验证过程中最复杂的一个阶段，主要目的是通过数据流分析和控制流分析，确定程序语义是合法的、符合逻辑的。
    - 这阶段就要对类的方法体（Class文件中的Code属性）进行校验分析，保证被校验类的方法在运行时不会做出危害虚拟机安全的行为，例如：
      - 保证任意时刻操作数栈的数据类型与指令代码序列都能配合工作，例如不会出现类似于“在操作栈放置了一个int类型的数据，使用时却按long类型来加载入本地变量表中”这样的情况。
      - 保证任何跳转指令都不会跳转到方法体以外的字节码指令上。
      - 保证方法体中的类型转换总是有效的，例如可以把一个子类对象赋值给父类数据类型，这是安全的，但是把父类对象赋值给子类数据类型，甚至把对象赋值给与它毫无继承关系、完全不相干的一个数据类型，则是危险和不合法的。
      - ……
    - 由于数据流分析和控制流分析的高度复杂性，Java虚拟机的设计团队为了避免过多的执行时间消耗在字节码验证阶段中，在JDK 6之后的Javac编译器和Java虚拟机里进行了一项联合优化，把尽可能多的校验辅助措施挪到Javac编译器里进行。
      - 具体做法是给方法体Code属性的属性表中新增加了一项名为“StackMapTable”的新属性，这项属性描述了方法体所有的基本块（Basic Block，指按照控制流拆分的代码块）开始时本地变量表和操作栈应有的状态，在字节码验证期间，Java虚拟机就不需要根据程序推导这些状态的合法性，只需要检查StackMapTable属性中的记录是否合法即可。
      - 这样就将字节码验证的类型推导转变为类型检查，从而节省了大量校验时间。
      - 而到了 JDK 7 之后，尽管虚拟机中仍然保留着类型推导验证器的代码，但是对于主版本号大于50（对应JDK6）的Class文件，使用类型检查来完成数据流分析校验则是唯一的选择，不允许再退回到原来的类型推导的校验方式。
  - **符号引用验证**
    - 最后一个阶段的校验行为发生在虚拟机将符号引用转化为直接引用的时候，**这个转化动作将在连接的第三阶段——解析阶段中发生**。
    - 符号引用验证可以看作是对类自身以外（常量池中的各种符号引用）的各类信息进行匹配性校验，通俗来说就是，该类是否缺少或者被禁止访问它依赖的某些外部类、方法、字段等资源。
    - 本阶段通常需要校验下列内容：
      - 符号引用中通过字符串描述的全限定名是否能找到对应的类。
      - 在指定类中是否存在符合方法的字段描述符及简单名称所描述的方法和字段。
      - 符号引用中的类、字段、方法的可访问性（private、protected、public、`<package>`）是否可被当前类访问。
    - 符号引用验证的主要目的是确保解析行为能正常执行
      - 如果无法通过符号引用验证，Java虚拟机将会抛出一个java.lang.IncompatibleClassChangeError的子类异常，典型的如：java.lang.IllegalAccessError、java.lang.NoSuchFieldError、java.lang.NoSuchMethodError等。
- 验证阶段对于虚拟机的类加载机制来说，是一个非常重要的、但却不是必须要执行的阶段，因为验证阶段只有通过或者不通过的差别，只要通过了验证，其后就对程序运行期没有任何影响了。
  - 如果程序运行的全部代码（包括自己编写的、第三方包中的、从外部加载的、动态生成的等所有代码）都已经被反复使用和验证过，在生产环境的实施阶段就可以考虑**使用 -Xverify:none 参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间**。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 7.3.2 验证

## 类加载过程 3：准备

- 准备阶段是**正式为类中定义的变量（即静态变量，被static修饰的变量）分配内存并设置类变量初始值的阶段**
  - 从概念上讲，这些变量所使用的内存都应当在方法区中进行分配，但必须注意到方法区本身是一个逻辑上的区域
    - 在JDK 7及之前，HotSpot使用永久代来实现方法区时，实现是完全符合这种逻辑概念的；
    - 而在JDK 8及之后，类变量则会随着Class对象一起存放在Java堆中，这时候“类变量在方法区”就完全是一种对逻辑概念的表述了
- 关于准备阶段，还有两个容易产生混淆的概念笔者需要着重强调
  - 首先是这时候进行内存分配的仅包括类变量，而**不包括实例变量**，实例变量将会在对象实例化时随着对象一起分配在Java堆中
  - 其次是这里所说的初始值**“通常情况”下是数据类型的零值**
    - “特殊情况”：如果类字段的字段属性表中存在 ConstantValue 属性，那在准备阶段变量值就会被初始化为 **ConstantValue 属性所指定的初始值**
      - 目前 Oracle 公司实现的 Javac 编译器的选择是，如果**同时使用 final 和 static 来修饰一个变量**（按照习惯，这里称“常量”更贴切），并且这个变量的数据类型是**基本类型或者 java.lang.String** 的话，就将会生成 ConstantValue 属性来进行初始化；
      - 如果这个变量没有被 final 修饰，或者并非基本类型及字符串，则将会选择在 `<clinit>()` 方法中进行初始化。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 7.3.3 准备

## 类加载过程 4：解析

- 解析阶段是**Java虚拟机将常量池内的符号引用替换为直接引用的过程**
  - 符号引用在第6章讲解 Class 文件格式的时候已经出现过多次，在Class文件中它以CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info等类型的常量出现
  - 那解析阶段中所说的直接引用与符号引用又有什么关联呢？
    - **符号引用（Symbolic References）**：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。
    - **直接引用（Direct References）**：直接引用是可以直接指向目标的指针、相对偏移量或者是一个能间接定位到目标的句柄。
- 《Java虚拟机规范》之中并未规定解析阶段发生的具体时间，只要求了在执行ane-warray、checkcast、getfield、getstatic、instanceof、invokedynamic、invokeinterface、invoke-special、invokestatic、invokevirtual、ldc、ldc_w、ldc2_w、multianewarray、new、putfield和putstatic这17个用于操作符号引用的字节码指令之前，先对它们所使用的符号引用进行解析。
- 对同一个符号引用进行多次解析请求是很常见的事情，**除 invokedynamic 指令以外**，虚拟机实现可以**对第一次解析的结果进行缓存**，譬如在运行时直接引用常量池中的记录，并把常量标识为已解析状态，从而避免解析动作重复进行。
  - 不过对于invokedynamic指令，上面的规则就不成立了。当碰到某个前面已经由invokedynamic指令触发过解析的符号引用时，并不意味着这个解析结果对于其他invokedynamic指令也同样生效。
  - 因为 invokedynamic 指令的目的本来就是用于动态语言支持，它对应的引用称为“动态调用点限定符（Dynamically-Computed Call Site Specifier）”，这里“动态”的含义是指必须等到程序实际运行到这条指令时，解析动作才能进行。
  - 相对地，其余可触发解析的指令都是“静态”的，可以在刚刚完成加载阶段，还没有开始执行代码时就提前进行解析。
- 解析动作主要针对**类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符这7类符号引用**进行
  - 分别对应于常量池的CONSTANT_Class_info、CON-STANT_Fieldref_info、CONSTANT_Methodref_info、CONSTANT_InterfaceMethodref_info、CONSTANT_MethodType_info、CONSTANT_MethodHandle_info、CONSTANT_Dyna-mic_info和CONSTANT_InvokeDynamic_info 8种常量类型
  - 1.类或接口的解析
  - 2.字段解析
  - 3.方法解析
  - 4.接口方法解析

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 7.3.4 解析

## 类加载过程 5：初始化

- 类的初始化阶段是类加载过程的最后一个步骤，之前介绍的几个类加载的动作里，除了在加载阶段用户应用程序可以通过自定义类加载器的方式局部参与外，其余动作都完全由Java虚拟机来主导控制。
  - 直到初始化阶段，Java虚拟机才真正开始执行类中编写的Java程序代码，将主导权移交给应用程序。
- 进行准备阶段时，变量已经赋过一次系统要求的初始零值，而在初始化阶段，则会根据程序员通过程序编码制定的主观计划去初始化类变量和其他资源。
  - 我们也可以从另外一种更直接的形式来表达：**初始化阶段就是执行类构造器 `<clinit>()` 方法的过程**。
  - `<clinit>()` 并不是程序员在 Java 代码中直接编写的方法，它是 Javac 编译器的自动生成物
- `<clinit>()`
  - `<clinit>()` 方法是由编译器自动收集类中的**所有类变量的赋值动作和静态语句块（static{}块）中的语句**合并产生的，**编译器收集的顺序是由语句在源文件中出现的顺序决定的**，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问
  - `<clinit>()` 方法与类的构造函数（即在虚拟机视角中的实例构造器 `<init>()` 方法）不同，它不需要显式地调用父类构造器，**Java虚拟机会保证在子类的 `<clinit>()` 方法执行前，父类的 `<clinit>()` 方法已经执行完毕**。因此在Java虚拟机中第一个被执行的 `<clinit>()` 方法的类型肯定是 java.lang.Object。
  - 由于父类的 `<clinit>()` 方法先执行，也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作
  - `<clinit>()` 方法对于类或接口来说并不是必需的，如果一个类中没有静态语句块，也没有对变量的赋值操作，那么编译器可以不为这个类生成 `<clinit>()` 方法。
  - 接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此接口与类一样都会生成 `<clinit>()` 方法。但**接口与类不同的是，执行接口的 `<clinit>()` 方法不需要先执行父接口的 `<clinit>()` 方法，因为只有当父接口中定义的变量被使用时，父接口才会被初始化**。此外，接口的实现类在初始化时也一样不会执行接口的  `<clinit>()` 方法。
  - Java 虚拟机必须保证一个类的 `<clinit>()` 方法在多线程环境中被正确地加锁同步，如果多个线程同时去初始化一个类，那么只会有其中一个线程去执行这个类的 `<clinit>()` 方法，其他线程都需要阻塞等待，直到活动线程执行完毕 `<clinit>()` 方法。如果在一个类的 `<clinit>()` 方法中有耗时很长的操作，那就可能造成多个进程阻塞，在实际应用中这种阻塞往往是很隐蔽的。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 7.3.5 初始化

## 栈上分配、逃逸分析

- Java中的对象和数组都是在堆上分配的吗？
  - 你可以这样回答：Java中的对象不一定是在堆上分配的，因为JVM通过**逃逸分析**，能够分析出一个新对象的使用范围，并以此确定是否要将这个对象分配到堆上。
  - 如果JVM发现某些对象没有逃逸出方法，就很有可能被优化成在**栈上分配**。
- 先以官方的形式来说下什么是逃逸分析。
  - 逃逸分析就是：一种确定指针动态范围的静态分析，它可以分析在程序的哪些地方可以访问到指针。
  - 在JVM的即时编译语境下，逃逸分析将判断新建的对象是否逃逸。即时编译判断对象是否逃逸的依据：
    - 一种是对象是否被存入堆中（静态字段或者堆中对象的实例字段）
    - 另一种就是对象是否被传入未知代码。
- 逃逸分析示例：
  - 一种典型的对象逃逸就是：**对象被复制给成员变量或者静态变量**，可能被外部使用，此时变量就发生了逃逸。
  - 另一种典型的场景就是：**对象通过return语句返回**。如果对象通过return语句返回了，此时的程序并不能确定这个对象后续会不会被使用，外部的线程可以访问到这个变量，此时对象也发生了逃逸。
- 逃逸分析优点
  - 逃逸分析的优点总体上来说可以分为三个：
    - **对象可能分配在栈上**
      - JVM通过逃逸分析，分析出新对象的使用范围，就可能将对象在栈上进行分配。
      - 栈分配可以快速地在栈帧上创建和销毁对象，不用再将对象分配到堆空间，可以有效地减少 JVM 垃圾回收的压力。
    - **分离对象或标量替换**
      - 当JVM通过逃逸分析，确定要将对象分配到栈上时，即时编译可以将对象打散，将对象替换为一个个很小的局部变量，我们将这个打散的过程叫做**标量替换**。
      - 将对象替换为一个个局部变量后，就可以非常方便的在栈上进行分配了。
      - 标量替换在 JDK1.8 中也是默认开启的(-XX:+EliminateAllocations)
    - **消除同步锁**
      - 如果JVM通过逃逸分析，发现一个对象只能从一个线程被访问到，则访问这个对象时，可以不加同步锁。如果程序中使用了synchronized锁，则JVM会将synchronized锁消除
      - 这里，需要注意的是：这种情况针对的是synchronized锁，而对于Lock锁，则JVM并不能消除。
      - 要开启同步消除，需要加上 -XX:+EliminateLocks 参数。(在 JDK1.8 中是默认开启的)
        - 因为这个参数依赖逃逸分析，所以同时要打开 -XX:+DoEscapeAnalysis 选项（**JDK 1.7之后默认开启**）。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景十七：Java 中的对象和数组都是在堆上分配的吗？

## 缓存一致性 / 引出内存模型

- 由于计算机的存储设备与处理器的运算速度有着几个数量级的差距，所以现代计算机系统都不得不加入一层或多层读写速度尽可能接近处理器运算速度的高速缓存（Cache）来作为内存与处理器之间的缓冲：将运算需要使用的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步回内存之中，这样处理器就无须等待缓慢的内存读写了。
- 基于高速缓存的存储交互很好地解决了处理器与内存速度之间的矛盾，但是也为计算机系统带来更高的复杂度，它引入了一个新的问题：**缓存一致性（Cache Coherence）**。
  - 在多路处理器系统中，每个处理器都有自己的高速缓存，而它们又共享同一主内存（Main Memory），这种系统称为共享内存多核系统（Shared Memory Multiprocessors System）。**当多个处理器的运算任务都涉及同一块主内存区域时，将可能导致各自的缓存数据不一致。**
  - 如果真的发生这种情况，那同步回到主内存时该以谁的缓存数据为准呢？**为了解决一致性的问题，需要各个处理器访问缓存时都遵循一些协议，在读写时要根据协议来进行操作**，这类协议有 MSI、MESI（Illinois Protocol）、MOSI、Synapse、Firefly及Dragon Protocol等。
    - **MESI 详解**
      - MESI 指的是缓存行的四种状态（Modified，Exclusive，Shared， Invalid），用 2 个 bit 表示。
- 从本章开始，我们将会频繁见到“**内存模型**”一词，它可以理解为在特定的操作协议下，对特定的内存或高速缓存进行读写访问的过程抽象。
  - 不同架构的物理机器可以拥有不一样的内存模型，而Java虚拟机也有自己的内存模型，并且与这里介绍的内存访问操作及硬件的缓存访问操作具有高度的可类比性。
- 除了增加高速缓存之外，为了使处理器内部的运算单元能尽量被充分利用，处理器可能会对输入代码进行**乱序执行（Out-Of-Order Execution）优化**，处理器会在计算之后将乱序执行的结果重组，保证该结果与顺序执行的结果是一致的，但并不保证程序中各个语句计算的先后顺序与输入代码中的顺序一致，因此如果存在一个计算任务依赖另外一个计算任务的中间结果，那么其顺序性并不能靠代码的先后顺序来保证。
  - 与处理器的乱序执行优化类似，Java虚拟机的即时编译器中也有**指令重排序（Instruction Reorder）优化**。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 12.2 硬件的效率与一致性

## Java 内存模型

- 《Java虚拟机规范》中曾试图定义一种**“Java内存模型” （Java Memory Model，JMM）**来屏蔽各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。

  - 在此之前，主流程序语言（如C和C++等）直接使用物理硬件和操作系统的内存模型。因此，由于不同平台上内存模型的差异，有可能导致程序在一套平台上并发完全正常，而在另外一套平台上并发访问却经常出错，所以在某些场景下必须针对不同的平台来编写程序

- 定义Java内存模型并非一件容易的事情

  - 这个模型必须定义得足够严谨，才能让Java的并发内存访问操作不会产生歧义；
  - 但是也必须定义得足够宽松，使得虚拟机的实现能有足够的自由空间去利用硬件的各种特性（寄存器、高速缓存和指令集中某些特有的指令）来获取更好的执行速度。

- 经过长时间的验证和修补，直至 **JDK 5**（实现了JSR-133 ）发布后，Java内存模型才终于成熟、完善起来了。

- 主内存与工作内存

  - **Java 内存模型的主要目的是定义程序中各种变量的访问规则，即关注在虚拟机中把变量值存储到内存和从内存中取出变量值这样的底层细节**。
    - 此处的变量（Variables）与Java编程中所说的变量有所区别，它包括了实例字段、静态字段和构成数组对象的元素，但是不包括局部变量与方法参数，因为后者是线程私有的，不会被共享，自然就不会存在竞争问题。
    - 为了获得更好的执行效能，Java内存模型并没有限制执行引擎使用处理器的特定寄存器或缓存来和主内存进行交互，也没有限制即时编译器是否要进行调整代码执行顺序这类优化措施。
  - Java内存模型规定了所有的变量都存储在**主内存（Main Memory）**中（此处的主内存与介绍物理硬件时提到的主内存名字一样，两者也可以类比，但物理上它仅是虚拟机内存的一部分）。
  - 每条线程还有自己的**工作内存（Working Memory**，可与前面讲的处理器高速缓存类比）
    - 线程的工作内存中保存了被该线程使用的变量的主内存副本
    - 线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的数据。
    - 不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成
  - 这里所讲的主内存、工作内存与第2章所讲的Java内存区域中的Java堆、栈、方法区等并不是同一个层次的对内存的划分，这两者基本上是没有任何关系的。
    - 如果两者一定要勉强对应起来，那么从变量、主内存、工作内存的定义来看，**主内存主要对应于Java堆中的对象实例数据部分**，而**工作内存则对应于虚拟机栈中的部分区域**。
    - 从更基础的层次上说，**主内存直接对应于物理硬件的内存**，而为了获取更好的运行速度，虚拟机（或者是硬件、操作系统本身的优化措施）可能会**让工作内存优先存储于寄存器和高速缓存中**，因为程序运行时主要访问的是工作内存。

- 内存间交互操作

  - 关于主内存与工作内存之间具体的交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存这一类的实现细节，Java内存模型中定义了以下8种操作来完成。
  - Java虚拟机实现时必须保证下面提及的每一种操作都是原子的、不可再分的（对于double和long类型的变量来说，load、store、read和write操作在某些平台上允许有例外，这个问题在12.3.4节会专门讨论）
    - **lock（锁定）**：作用于主内存的变量，它把一个变量标识为一条线程独占的状态。
    - **unlock（解锁）**：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。
    - **read（读取）**：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用。
    - **load（载入）**：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。
    - **use（使用）**：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。
    - **assign（赋值）**：作用于工作内存的变量，它把一个从执行引擎接收的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。
    - **store（存储）**：作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的write操作使用。
    - **write（写入）**：作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。
  - 如果要把一个变量从主内存拷贝到工作内存，那就要按**顺序执行 read 和 load 操作**，如果要把变量从工作内存同步回主内存，就要按**顺序执行 store 和 write 操作**。
    - 注意，Java内存模型只要求上述两个操作必须按顺序执行，但不要求是连续执行。
    - 也就是说read与load之间、store与write之间是可插入其他指令的，如对主内存中的变量a、b进行访问时，一种可能出现的顺序是read a、read b、load b、load a。
  - 除此之外，Java内存模型还规定了在执行上述8种基本操作时必须满足如下规则：
    - 不允许read和load、store和write操作之一单独出现，即不允许一个变量从主内存读取了但工作内存不接受，或者工作内存发起回写了但主内存不接受的情况出现。
    - 不允许一个线程丢弃它最近的assign操作，即变量在工作内存中改变了之后必须把该变化同步回主内存。
    - 不允许一个线程无原因地（没有发生过任何assign操作）把数据从线程的工作内存同步回主内存中。·一个新的变量只能在主内存中“诞生”，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量，换句话说就是对一个变量实施use、store操作之前，必须先执行assign和load操作。
    - 一个变量在同一个时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。
    - 如果对一个变量执行lock操作，那将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作以初始化变量的值。
    - 如果一个变量事先没有被lock操作锁定，那就不允许对它执行unlock操作，也不允许去unlock一个被其他线程锁定的变量。
    - 对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store、write操作）。
  - 这8种内存访问操作以及上述规则限定，再加上稍后会介绍的专门针对volatile的一些特殊规定，就已经能准确地描述出Java程序中哪些内存访问操作在并发下才是安全的。
    - 这种定义相当严谨，但也是极为烦琐，实践起来更是无比麻烦。
    - 可能部分读者阅读到这里已经对多线程开发产生恐惧感了，后来Java设计团队大概也意识到了这个问题，将Java内存模型的操作简化为read、write、lock和unlock四种，但这只是语言描述上的等价化简，Java内存模型的基础设计并未改变，即使是这四操作种，对于普通用户来说阅读使用起来仍然并不方便。
    - 不过读者对此无须过分担忧，除了进行虚拟机开发的团队外，大概没有其他开发人员会以这种方式来思考并发问题，我们只需要理解 Java 内存模型的定义即可。
    - 12.3.6节将介绍这种定义的一个等效判断原则——**先行发生原则**，用来确定一个操作在并发环境下是否安全的。

- **对于 volatile 型变量的特殊规则**

  - 关键字volatile可以说是Java虚拟机提供的最轻量级的同步机制
  - 当一个变量被定义成volatile之后，它将具备两项特性：
    - 第一项是**保证此变量对所有线程的可见性**
      - 这里的“可见性”是指当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。
      - 而普通变量并不能做到这一点，普通变量的值在线程间传递时均需要通过主内存来完成。
      - 比如，线程A修改一个普通变量的值，然后向主内存进行回写，另外一条线程B在线程A回写完成了之后再对主内存进行读取操作，新变量值才会对线程B可见。
      - 但是Java里面的运算操作符并非原子操作，这导致volatile变量的运算在并发下一样是不安全的
        - 问题就出在自增运算“race++”之中，我们用Javap反编译这段代码后会得到代码清单12-2所示，发现只有一行代码的increase()方法在Class文件中是由4条字节码指令构成（return指令不是由race++产生的，这条指令可以不计算）
        - 实事求是地说，笔者使用字节码来分析并发问题仍然是不严谨的，因为即使编译出来只有一条字节码指令，也并不意味执行这条指令就是一个原子操作。
          - 一条字节码指令在解释执行时，解释器要运行许多行代码才能实现它的语义。
          - 如果是编译执行，一条字节码指令也可能转化成若干条本地机器码指令。
        - 此处使用-XX：+PrintAssembly参数输出反汇编来分析才会更加严谨一些，但是考虑到读者阅读的方便性，并且字节码已经能很好地说明问题，所以此处使用字节码来解释。
      - 由于volatile变量只能保证可见性，在不符合以下两条规则的运算场景中，我们仍然要通过加锁（使用synchronized、java.util.concurrent中的锁或原子类）来保证原子性：
        - 运算结果并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值。
        - 变量不需要与其他的状态变量共同参与不变约束。
    - 使用volatile变量的第二个语义是**禁止指令重排序优化**
      - 普通的变量仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致。
        - 因为在同一个线程的方法执行过程中无法感知到这点，这就是Java内存模型中描述的所谓“线程内表现为串行的语义”（Within-Thread As-If-Serial Semantics）。
      - 通过对比发现，关键变化在于有 volatile 修饰的变量，赋值后（前面 `mov %eax，0x150(%esi)` 这句便是赋值操作）多执行了一个“`lock addl $0x0，(%esp)`”操作，这个操作的作用相当于一个**内存屏障（Memory Barrier或Memory Fence**，指重排序时不能把后面的指令重排序到内存屏障之前的位置，**注意不要与第3章中介绍的垃圾收集器用于捕获变量访问的内存屏障互相混淆**），只有一个处理器访问内存时，并不需要内存屏障；但如果有两个或更多处理器访问同一块内存，且其中有一个在观测另一个，就需要内存屏障来保证一致性了。
        - 这里的关键在于 lock 前缀，查询IA32手册可知，它的作用是将本处理器的缓存写入了内存，该写入动作也会引起别的处理器或者别的内核无效化（Invalidate）其缓存，这种操作相当于对缓存中的变量做了一次前面介绍Java内存模式中所说的“store和write”操作[4]。所以通过这样一个空操作，可让前面volatile变量的修改对其他处理器立即可见。
  - 我们再回头来看看Java内存模型中对volatile变量定义的特殊规则的定义。假定T表示一个线程，V和W分别表示两个volatile型变量，那么在进行read、load、use、assign、store和write操作时需要满足如下规则：
    - 只有当线程T对变量V执行的前一个动作是load的时候，线程T才能对变量V执行use动作；并且，只有当线程T对变量V执行的后一个动作是use的时候，线程T才能对变量V执行load动作。线程T对变量V的use动作可以认为是和线程T对变量V的load、read动作相关联的，必须连续且一起出现。
      - 这条规则要求在工作内存中，每次使用V前都必须先从主内存刷新最新的值，用于保证能看见其他线程对变量V所做的修改。
    - 只有当线程T对变量V执行的前一个动作是assign的时候，线程T才能对变量V执行store动作；并且，只有当线程T对变量V执行的后一个动作是store的时候，线程T才能对变量V执行assign动作。线程T对变量V的assign动作可以认为是和线程T对变量V的store、write动作相关联的，必须连续且一起出现。
      - 这条规则要求在工作内存中，每次修改V后都必须立刻同步回主内存中，用于保证其他线程可以看到自己对变量V所做的修改。
    - 假定动作A是线程T对变量V实施的use或assign动作，假定动作F是和动作A相关联的load或store动作，假定动作P是和动作F相应的对变量V的read或write动作；与此类似，假定动作B是线程T对变量W实施的use或assign动作，假定动作G是和动作B相关联的load或store动作，假定动作Q是和动作G相应的对变量W的read或write动作。如果A先于B，那么P先于Q。
      - 这条规则要求volatile修饰的变量不会被指令重排序优化，从而保证代码的执行顺序与程序的顺序相同。

- ##### 针对 long 和 double 型变量的特殊规则

  - Java内存模型要求lock、unlock、read、load、assign、use、store、write这八种操作都具有原子性，但是对于64位的数据类型（long和double），在模型中特别定义了一条宽松的规定：**允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行**，即允许虚拟机实现自行选择是否要保证64位数据类型的load、store、read和write这四个操作的原子性，这就是所谓的“**long和double的非原子性协定”（Non-Atomic Treatment of double and long Variables）**。
  - **在目前主流平台下商用的64位Java虚拟机中并不会出现非原子性访问行为**，但是对于32位的Java虚拟机，譬如比较常用的32位x86平台下的HotSpot虚拟机，对long类型的数据确实存在非原子性访问的风险。
  - 笔者的看法是，在实际开发中，除非该数据有明确可知的线程竞争，否则我们在编写代码时一般不需要因为这个原因刻意把用到的long和double变量专门声明为volatile。

- **原子性、可见性与有序性**

  - Java内存模型是围绕着在并发过程中如何处理原子性、可见性和有序性这三个特征来建立的，我们逐个来看一下哪些操作实现了这三个特性。
    - 1.**原子性（Atomicity）**
      - 由Java内存模型来直接保证的原子性变量操作包括read、load、assign、use、store和write这六个，我们大致可以认为，**基本数据类型的访问、读写都是具备原子性的**（例外就是long和double的非原子性协定，读者只要知道这件事情就可以了，无须太过在意这些几乎不会发生的例外情况）。
      - 如果应用场景需要一个**更大范围的原子性保证（经常会遇到），Java内存模型还提供了lock和unlock操作来满足这种需求**，尽管虚拟机未把lock和unlock操作直接开放给用户使用，但是却提供了**更高层次的字节码指令monitorenter和monitorexit**来隐式地使用这两个操作。
        - 这两个字节码指令反映到Java代码中就是同步块——synchronized关键字，因此在synchronized块之间的操作也具备原子性。
    - 2.**可见性（Visibility）**
      - 可见性就是指当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改。上文在讲解volatile变量的时候我们已详细讨论过这一点。
      - **Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式来实现可见性的，无论是普通变量还是volatile变量都是如此**。
        - 普通变量与volatile变量的区别是，**volatile的特殊规则保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新**。因此我们可以说volatile保证了多线程操作时变量的可见性，而普通变量则不能保证这一点。
      - 除了volatile之外，Java还有两个关键字能实现可见性，它们是 **synchronized 和 final**。
        - **同步块的可见性是由“对一个变量执行 unlock 操作之前，必须先把此变量同步回主内存中（执行 store、write 操作）”这条规则获得的**。
        - 而final关键字的可见性是指：**被 final 修饰的字段在构造器中一旦被初始化完成，并且构造器没有把“this”的引用传递出去（this引用逃逸是一件很危险的事情，其他线程有可能通过这个引用访问到“初始化了一半”的对象），那么在其他线程中就能看见final字段的值**。
    - 3.**有序性（Ordering）**
      - Java内存模型的有序性在前面讲解volatile时也比较详细地讨论过了，Java程序中天然的有序性可以总结为一句话：**如果在本线程内观察，所有的操作都是有序的**；**如果在一个线程中观察另一个线程，所有的操作都是无序的**。
        - 前半句是指“线程内似表现为串行的语义”（Within-Thread As-If-Serial Semantics），后半句是指“指令重排序”现象和“工作内存与主内存同步延迟”现象。
      - Java语言提供了 **volatile 和 synchronized** 两个关键字来保证线程之间操作的有序性
        - **volatile关键字本身就包含了禁止指令重排序的语义**
        - **而synchronized则是由“一个变量在同一个时刻只允许一条线程对其进行lock操作”这条规则获得的**，这个规则决定了持有同一个锁的两个同步块只能串行地进入。

- ##### 先行发生原则

  - 如果Java内存模型中所有的有序性都仅靠 volatile 和 synchronized 来完成，那么有很多操作都将会变得非常啰嗦，但是我们在编写Java并发代码的时候并没有察觉到这一点，这是因为Java语言中有一个**“先行发生”（Happens-Before）**的原则。
    - 这个原则非常重要，它是判断数据是否存在竞争，线程是否安全的非常有用的手段。
    - 依赖这个原则，我们可以通过几条简单规则一揽子解决并发环境下两个操作之间是否可能存在冲突的所有问题，而不需要陷入Java内存模型苦涩难懂的定义之中。
  - 现在就来看看“先行发生”原则指的是什么。先行发生是Java内存模型中定义的两项操作之间的偏序关系，比如说操作A先行发生于操作B，其实就是说在发生操作B之前，操作A产生的影响能被操作B观察到，“影响”包括修改了内存中共享变量的值、发送了消息、调用了方法等。
  - 下面是Java内存模型下一些“天然的”先行发生关系，这些先行发生关系无须任何同步器协助就已经存在，可以在编码中直接使用。如果两个操作之间的关系不在此列，并且无法从下列规则推导出来，则它们就没有顺序性保障，虚拟机可以对它们随意地进行重排序。
    - **程序次序规则（Program Order Rule）**：在一个线程内，按照控制流顺序，书写在前面的操作先行发生于书写在后面的操作。注意，这里说的是控制流顺序而不是程序代码顺序，因为要考虑分支、循环等结构。
    - **管程锁定规则（Monitor Lock Rule）**：一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。这里必须强调的是“同一个锁”，而“后面”是指时间上的先后。
    - **volatile变量规则（Volatile Variable Rule）**：对一个volatile变量的写操作先行发生于后面对这个变量的读操作，这里的“后面”同样是指时间上的先后。
    - **线程启动规则（Thread Start Rule）**：Thread对象的start()方法先行发生于此线程的每一个动作。
    - **线程终止规则（Thread Termination Rule）**：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread::join()方法是否结束、Thread::isAlive()的返回值等手段检测线程是否已经终止执行。
    - **线程中断规则（Thread Interruption Rule）**：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread::interrupted()方法检测到是否有中断发生。
    - **对象终结规则（Finalizer Rule）**：一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize()方法的开始。
    - **传递性（Transitivity）**：如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论。
  - Java语言无须任何同步手段保障就能成立的先行发生规则有且只有上面这些

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 12.3 Java 内存模型

## Java 线程的实现

- Java线程的实现
  - 但从JDK 1.3起，“主流”平台上的“主流”商用Java虚拟机的线程模型普遍都被替换为基于操作系统原生线程模型来实现，即采用 **1：1 的线程模型**。
  - **以 HotSpot 为例，它的每一个 Java 线程都是直接映射到一个操作系统原生线程来实现的**，而且中间没有额外的间接结构，所以HotSpot自己是不会去干涉线程调度的（可以设置线程优先级给操作系统提供调度建议），全权交给底下的操作系统去处理，所以何时冻结或唤醒线程、该给线程分配多少处理器执行时间、该把线程安排给哪个处理器核心去执行等，都是由操作系统完成的，也都是由操作系统全权决定的。
- 操作系统支持怎样的线程模型，在很大程度上会影响上面的 Java 虚拟机的线程是怎样映射的，这一点在不同的平台上很难达成一致，因此《Java虚拟机规范》中才不去限定 Java 线程需要使用哪种线程模型来实现。
  - 线程模型只对线程的并发规模和操作成本产生影响，对 Java 程序的编码和运行过程来说，这些差异都是完全透明的。
- Java 线程调度
  - 线程调度是指系统为线程分配处理器使用权的过程，调度主要方式有两种，分别是协同式（Cooperative Threads-Scheduling）线程调度和抢占式（Preemptive Threads-Scheduling）线程调度。
    - **Java使用的线程调度方式就是抢占式调度**。
  - 不过，**线程优先级并不是一项稳定的调节手段**，很显然因为主流虚拟机上的Java线程是被映射到系统的原生线程上来实现的，所以**线程调度最终还是由操作系统说了算**。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 12.4.1 线程的实现、12.4.2 Java 线程调度

# 多线程

## 进程通信

- 管道( pipe )：
  - 管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。
- 有名管道 (namedpipe) ：
  - 有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。
- 信号量(semophore ) ：
  - 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。
- 消息队列( messagequeue ) ：
  - 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。
- 信号 (sinal ) 
  - 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。
- 共享内存(shared memory ) ：
  - 共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。
- 套接字(socket ) ：
  - 套接口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同设备及其间的进程通信。

参考文档：

1. [面试官：说说进程间通信和线程间通信的几种方式及区别 - 知乎](https://zhuanlan.zhihu.com/p/430069448)

## 线程通信

- 锁机制：包括互斥锁、条件变量、读写锁
- 信号量机制(Semaphore)：包括无名线程信号量和命名线程信号量
- 信号机制(Signal)：类似进程间的信号处理

参考文档：

1. [面试官：说说进程间通信和线程间通信的几种方式及区别 - 知乎](https://zhuanlan.zhihu.com/p/430069448)

## 线程和进程区别

- 调度：
  - 线程作为处理器调度和分配的基本单位
  - 而进程是作为拥有资源的基本单位
- 并发性
  - 不仅进程之间可以并发执行
  - 同一个进程的多个线程之间也可以并发执行
- 拥有资源
  - 进程是拥有资源的一个独立单位，有自己独立的地址空间
  - 线程不拥有系统资源，但可以访问隶属于进程的资源，共享进程的地址空间.
- 系统开销
  - 在创建或撤消进程时，由于系统都要为之分配和回收资源，导致系统的开销明显大于创建或撤消线程时的开销。

参考文档：

1. [操作系统：进程与线程之间的区别及联系](https://zhuanlan.zhihu.com/p/505594640)

## 协程

- 内核线程的局限

  - Java目前的并发编程机制就与上述架构趋势产生了一些矛盾，1：1的内核线程模型是如今Java虚拟机线程实现的主流选择，但是这种映射到操作系统上的线程天然的缺陷是切换、调度成本高昂，系统能容纳的线程数量也很有限。
  - 传统的Java Web服务器的线程池的容量通常在几十个到两百之间，当程序员把数以百万计的请求往线程池里面灌时，系统即使能处理得过来，但其中的切换损耗也是相当可观的。

- 协程的复苏

  - **内核线程的调度成本主要来自于用户态与核心态之间的状态转换**，而这两种状态转换的开销主要来自于响应中断、保护和恢复执行现场的成本。
  - 由于最初多数的用户线程是被设计成协同式调度（Cooperative Scheduling）的，所以它有了一个别名——**“协程”（Coroutine）**。
    - 又由于这时候的协程会完整地做调用栈的保护、恢复工作，所以今天也被称为**“有栈协程”（Stackful Coroutine）**，起这样的名字是为了便于跟后来的**“无栈协程”（Stackless Coroutine）**区分开。
    - 无栈协程不是本节的主角，不过还是可以简单提一下它的典型应用，即各种语言中的await、async、yield这类关键字。
      - 无栈协程本质上是一种有限状态机，状态保存在闭包里，自然比有栈协程恢复调用栈要轻量得多，但功能也相对更有限。
  - 协程的主要优势是轻量，无论是有栈协程还是无栈协程，都要比传统内核线程要轻量得多。
  - 协程当然也有它的局限，需要在应用层面实现的内容（调用栈、调度器这些）特别多，这个缺点就不赘述了。
    - 具体到Java语言，还会有一些别的限制，譬如HotSpot这样的虚拟机，Java调用栈跟本地调用栈是做在一起的。
    - 如果在协程中调用了本地方法，还能否正常切换协程而不影响整个线程？
    - 另外，如果协程中遇传统的线程同步措施会怎样？譬如Kotlin提供的协程实现，一旦遭遇synchronize关键字，那挂起来的仍将是整个线程。

- Java 的解决方案

  - 对于有栈协程，有一种特例实现名为纤程（Fiber），这个词最早是来自微软公司，后来微软还推出过系统层面的纤程包来方便应用做现场保存、恢复和纤程调度。
    - OpenJDK在2018年创建了Loom项目，这是Java用来应对本节开篇所列场景的官方解决方案，根据目前公开的信息，如无意外，日后该项目为Java语言引入的、与现在线程模型平行的新并发编程机制中应该也会采用“纤程”这个名字，不过这显然跟微软是没有任何关系的。(注：然而 JDK 21 实际引入时叫的是“虚拟线程”)
    - 从Oracle官方对“什么是纤程”的解释里可以看出，它就是一种典型的有栈协程
  - 在新并发模型下，一段使用纤程并发的代码会被分为两部分——执行过程（Continuation）和调度器（Scheduler）。

- Java 21 的落地实现：虚拟线程

  - 和 Kotlin 的对比

    - **Java 是有栈协程，协程上下文保存在与执行处独立的栈上**，所以异步函数可以直接像正常函数那样写，程序执行到阻塞部分时 JVM 内部自动挂起；
    - **Kotlin 是无栈协程，因为干预不了 JVM，所以协程上下文必须保存在堆上**，同时得显式声明函数是可挂起的，否则不知道什么函数支持挂起（suspend 在转译为字节码时会添加 Continuation 参数）。

  - Java 协程的调度是通过 **ForkJoinPool** 来实现的，因此也是具有 **work stealing** 机制的

  - 使用指南

    - **请大方使用同步阻塞 IO**

      - 在 “一个请求一个线程” 模型中使用平台线程的成本很高，因为平台线程与操作系统线程对应（操作系统线程是一种相对稀缺的资源），阻塞了平台线程，会让它无事可做一直处于阻塞中，这样就会造成很大的资源浪费。
      - 然而，在这个模型中使用虚拟线程就很合适，**因为虚拟线程非常廉价就算被阻塞也不会造成资源浪费**。因此在虚拟线程出来后，Java 的设计者是建议我们应该以简单的同步风格编写代码并使用阻塞 IO。

    - **避免池化虚拟线程**

      - 关于虚拟线程使用方面最难理解的一件事情就是，我们不应该池化虚拟线程。虽然虚拟线程具有与平台线程相同的行为，但虚拟线程和线程池其实是两种概念。

        - 平台线程是一种稀缺资源，因为它很宝贵。越宝贵的资源就越需要管理，管理平台线程最常见的方法是使用线程池。
        - 虚拟线程是一种非常廉价的资源，每个虚拟线程不应代表某些共享的、池化的资源，而应代表单一任务。在应用程序中，我们应该直接使用虚拟线程而不是通过线程池使用它。

      - 那么我们应该创建多少个虚拟线程嘞？答案是不必在乎虚拟线程的数量，我们有多少个并发任务就可以有多少个虚拟线程。

      - 建议使用虚拟线程执行器，代码如下：

        ```java
        try (var executor = Executors.newVirtualThreadPerTaskExecutor()) {
           Future<ResultA> f1 = executor.submit(task1);
           Future<ResultB> f2 = executor.submit(task2);
           // ... use futures
        }
        ```

        - 上面代码虽然仍使用 ExecutorService，但从 **Executors.newVirtualThreadPerTaskExecutor() 方法返回的执行器不再使用线程池**。它会为每个提交的任务都创建一个新的虚拟线程。
        - 此外，ExecutorService 本身是轻量级的，我们可以像创建任何简单对象一样直接创建一个新的 ExecutorService 对象而不必考虑复用。

    - **使用信号量限制并发**

      - 使用虚拟线程时，如果要限制访问某些服务的并发请求，则应该使用专门为此目的设计的 Semaphore 类。
      - 简单地使用信号量阻塞某些虚拟线程可能看起来与将任务提交到固定数量线程池有很大不同，但事实并非如此。
        - 将任务提交到等待任务池会将它们排队处理，信号量在内部(或任何其他阻塞同步构造)构造了一个阻塞线程队列，这些任务在阻塞线程队列上也会进行排队处理。
        - 我们可以将平台线程池认作是从等待任务队列中提取任务进行处理的工作人员，然后将虚拟线程视为任务本身，在任务或者线程可以执行之前将会被阻塞，但它们在计算机中的底层表示上实际是相同的。
        - 这里想告诉大家的就是不管是线程池的任务排队，还是信号量内部的线程阻塞，它们之间是由等效性的。在虚拟线程某些需要限制并发数场景下，直接使用信号量即可。

    - **不要在线程局部变量中缓存可重用对象**

      - 虚拟线程支持线程局部变量（ThreadLocal），就像平台线程一样。通常线程局部变量用于将一些特定于上下文的信息与当前运行的代码关联起来，例如当前事务和用户 ID。
        - 对于虚拟线程来说，使用线程局部变量是完全合理的。但是如果考虑更安全、更有效的线程局部变量，可以使用 Scoped Values。
      - 线程局部变量有一种用途与虚拟线程是不太适合的，那就是缓存可重用对象。
        - 可重用对象的创建成本通常很高，通常消耗大量内存且可变，还不是线程安全的。它们被缓存在线程局部变量中，以减少它们实例化的次数以及它们在内存中的实例数量，好处是它们可以被线程上不同时间运行的多个任务重用，避免昂贵对象的重复创建。
      - 对于线程局部变量缓存可重用对象的问题，没有什么好的通用替代方案，但对于 SimpleDateFormat，我们应该将其替换为 DateTimeFormatter。DateTimeFormatter 是不可变的，因此单个实例就可以由所有线程共享

    - **避免长时间和频繁的 synchronized**

      - 当前虚拟线程实现由一个限制是，在同步块或方法内执行 synchronized 阻塞操作会导致 JDK 的虚拟线程调度程序阻塞宝贵的操作系统线程，而如果阻塞操作是在同步块或方法外完成的，则不会被阻塞。我们称这种情况为 “Pinning”。
      - 如果阻塞操作既长期又频繁，则 “Pinning” 可能会对服务器的吞吐量产生不利影响。如果阻塞操作短暂（例如内存中操作）或不频繁则可能不会产生不利影响。
      - 为了检测可能有害的 “Pinning” 实例，（JDK Flight Recorder (JFR) 在 “Pinning” 阻塞时间超过 20 毫秒时，会发出 jdk.VirtualThreadPinned 事件。
        - 或者我们可以使用系统属性 jdk.tracePinnedThreads 在线程被 “Pinning” 阻塞时发出堆栈跟踪。
        - 启动 Java 程序时添加 -Djdk.tracePinnedThreads=full 运行，会在线程被 “Pinning” 阻塞时打印完整的堆栈跟踪，突出显示本机帧和持有监视器的帧。使用 -Djdk.tracePinnedThreads=short 运行，会将输出限制为仅有问题的帧。
      - 如果这些机制检测到既长期又频繁 “Pinning” 的地方，请在这些特定地方将 synchronized 替换为 ReentrantLock。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 12.5 Java 与协程
2. [为什么我认为 Java 虚拟线程不会取代 Kotlin 协程 - 知乎](https://zhuanlan.zhihu.com/p/647257179)
3. [Java 21 虚拟线程：使用指南（一）- 博客园](https://www.cnblogs.com/waynaqua/p/17935918.html)
4. [Java 新技术：虚拟线程使用指南（二）- 博客园](https://www.cnblogs.com/waynaqua/p/17954092)

## 查看线程/进程 CPU 使用率

- 那么首先需要获取这个进程的PID: `ps -ef| grep [process name]`
- 然后查看该进程的CPU: `top -p [PID]`
  - `-p <进程ID>`：仅显示指定进程ID的信息。
- 查看这个进程的各个进程的CPU: `top -H -p [PID]`
  - `-H`：在进程信息中显示线程详细信息。
  - `-d <秒数>`：指定 top 命令的刷新时间间隔，单位为秒。

参考文档：

1. [linux 下查看某一进程的cpu使用率和这个线程中各个线程的cpu使用率 - CSDN](https://blog.csdn.net/learners_sjt/article/details/82461079)
2. [Linux top 命令 - RUNOOB](https://www.runoob.com/linux/linux-comm-top.html)

## 确定创建线程数量

- 创建多少线程合适， 要看多线程具体的应用场景。一般来说，我们可以将程序分为：**CPU 密集型程序和 I/O 密集型程序**， 而针对于 CPU 密集型程序和 I/O 密集型程序，其计算最佳线程数的方法是不同的 。
- **CPU 密集型程序**
  - 对于CPU密集型计算， 多线程本质上是提升多核CPU的利用率， 所以对于一个4核的CPU， 每个核一个线程， 理论上创建4个线程就可以了， 再多创建线程也只是增加线程切换的成本。所以， 对于CPU密集型的计算场景， 理论上“线程的量=CPU核数”就是最合适的。但是在实际工作中， 一般会将线程数量设置为“CPU核数+1”， 这样的话， 当线程因为偶尔的内存页失效或其他原因导致阻塞时， 这个额外的线程可以顶上， 从而保证CPU的利用率 。
  - **所以，在 CPU 密集型的程序中，一般可以将线程数设置为CPU核数+1。**
- **I/O 密集型程序**
  - 对于I/O密集型的程序，最佳的线程数是与程序中CPU计算和I/O操作的耗时比相关。总体来说，可以将其总结为如下的公式。
  - **单核 CPU**
    - **最佳线程数 = 1 +（I/O 耗时 / CPU 耗时）** 
    - 我们令 R=I/O 耗时 / CPU 耗时， 可以这样理解：当线程 A 执行 IO 操作时， 另外 R 个线程正好执行完各自的 CPU 计算。这样 CPU 的利用率就达到了 100%。
  - **多核 CPU**
    - 多核CPU的最佳线程数在单核CPU最佳线程数的基础上，乘以CPU核数即可，如下所示。
    - **最佳线程数 = CPU核数 \* [ 1 +（I/O 耗时 / CPU 耗时）]** 
- **总结**
  - 上述公式计算的结果为最佳理论值，实际工作中还是要通过实际压测数据来找到最佳线程数，将硬件的性能发挥到极致。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景三十四：高并发场景下创建多少线程才合适？

## 死锁必要条件、预防、避免

- 四个必要条件
  - **互斥条件**：资源是独占的且排他使用，进程互斥使用资源，即任意时刻一个资源只能给一个进程使用，其他进程若申请一个资源，而该资源被另一进程占有时，则申请者等待直到资源被占有者释放。
  - **不可剥夺条件**：进程所获得的资源在未使用完毕之前，不被其他进程强行剥夺，而只能由获得该资源的进程资源释放。
  - **请求和保持条件**：进程每次申请它所需要的一部分资源，在申请新的资源的同时，继续占用已分配到的资源。
  - **循环等待条件**：在发生死锁时必然存在一个进程等待队列{P1,P2,…,Pn},其中P1等待P2占有的资源，P2等待P3占有的资源，…，Pn等待P1占有的资源，形成一个进程等待环路，环路中每一个进程所占有的资源同时被另一个申请，也就是前一个进程占有后一个进程所深情地资源。
  - 以上给出了导致死锁的四个必要条件，只要系统发生死锁则以上四个条件至少有一个成立。事实上循环等待的成立蕴含了前三个条件的成立，似乎没有必要列出然而考虑这些条件对死锁的预防是有利的，因为可以通过破坏四个条件中的任何一个来预防死锁的发生。
- 死锁预防
  - 我们可以通过破坏死锁产生的4个必要条件来 预防死锁，由于资源互斥是资源使用的固有特性是无法改变的。
  - **破坏“不可剥夺”条件**：一个进程不能获得所需要的全部资源时便处于等待状态，等待期间他占有的资源将被隐式的释放重新加入到 系统的资源列表中，可以被其他的进程使用，而等待的进程只有重新获得自己原有的资源以及新申请的资源才可以重新启动，执行。
  - **破坏”请求与保持条件“**：第一种方法静态分配即每个进程在开始执行时就申请他所需要的全部资源。第二种是动态分配即每个进程在申请所需要的资源时他本身不占用系统资源。
  - **破坏“循环等待”条件**：采用资源有序分配其基本思想是将系统中的所有资源顺序编号，将紧缺的，稀少的采用较大的编号，在申请资源时必须按照编号的顺序进行，一个进程只有获得较小编号的进程才能申请较大编号的进程。
- 死锁避免
  - 死锁避免的基本思想：系统对进程发出的每一个系统能够满足的资源申请进行动态检查，并根据检查结果决定是否分配资源，如果分配后系统可能发生死锁，则不予分配，否则予以分配，这是一种保证系统不进入死锁状态的动态策略。
  - 银行家算法
  - 安全状态、不安全状态

参考文档：

1. [死锁的四个必要条件](https://blog.csdn.net/jyy305/article/details/70077042)

## Java 线程状态

- Java语言定义了6种线程状态，在任意一个时间点中，一个线程只能有且只有其中的一种状态，并且可以通过特定的方法在不同状态之间转换。这6种状态分别是：
  - **新建（New）**：创建后尚未启动的线程处于这种状态。
  - **运行（Runnable）**：包括操作系统线程状态中的Running和Ready，也就是处于此状态的线程有可能正在执行，也有可能正在等待着操作系统为它分配执行时间。
  - **无限期等待（Waiting）**：处于这种状态的线程不会被分配处理器执行时间，它们要等待被其他线程显式唤醒。以下方法会让线程陷入无限期的等待状态：
    - 没有设置Timeout参数的Object::wait()方法；
    - 没有设置Timeout参数的Thread::join()方法；
    - LockSupport::park()方法。
  - **限期等待（Timed Waiting）**：处于这种状态的线程也不会被分配处理器执行时间，不过无须等待被其他线程显式唤醒，在一定时间之后它们会由系统自动唤醒。以下方法会让线程进入限期等待状态：
    - Thread::sleep()方法；
    - 设置了Timeout参数的Object::wait()方法；
    - 设置了Timeout参数的Thread::join()方法；
    - LockSupport::parkNanos()方法；
    - LockSupport::parkUntil()方法。
  - **阻塞（Blocked）**：线程被阻塞了，“阻塞状态”与“等待状态”的区别是“阻塞状态”在等待着获取到一个排它锁，这个事件将在另外一个线程放弃这个锁的时候发生；而“等待状态”则是在等待一段时间，或者唤醒动作的发生。在程序等待进入同步区域的时候，线程将进入这种状态。
  - **结束（Terminated）**：已终止线程的线程状态，线程已经结束执行。

参考文档：

1. [2024年面试准备-3-JVM篇.md](./2024年面试准备-3-JVM篇.md) 12.4.3 状态转换

## 有 synchronized 为什么还提供 Lock？

- 参考死锁的必要条件

  - 如果我们的程序使用synchronized关键字发生了死锁时，synchronized关键是是**无法破坏“不可剥夺”这个死锁的条件的**。这是因为synchronized申请资源的时候， 如果申请不到， 线程直接进入阻塞状态了， 而线程进入阻塞状态， 啥都干不了， 也释放不了线程已经占有的资源。

- 解决问题

  - （1）**能够响应中断**。
    - synchronized的问题是， 持有锁A后， 如果尝试获取锁B失败， 那么线程就进入阻塞状态， 一旦发生死锁， 就没有任何机会来唤醒阻塞的线程。
    - 但如果阻塞状态的线程能够响应中断信号， 也就是说当我们给阻塞的线程发送中断信号的时候， 能够唤醒它， 那它就有机会释放曾经持有的锁A。这样就破坏了不可剥夺条件了。
  - （2）**支持超时**。
    - 如果线程在一段时间之内没有获取到锁， 不是进入阻塞状态， 而是返回一个错误，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可剥夺条件。
  - （3）**非阻塞地获取锁**。
    - 如果尝试获取锁失败， 并不进入阻塞状态， 而是直接返回， 那这个线程也有机会释放曾经持有的锁。这样也能破坏不可剥夺条件。

- 体现在 Lock 接口上，就是 Lock 接口提供的三个方法

  - ```java
    // 支持中断的API
    void lockInterruptibly() throws InterruptedException;
    // 支持超时的API
    boolean tryLock(long time, TimeUnit unit) throws InterruptedException;
    // 支持非阻塞获取锁的API
    boolean tryLock();
    ```

- 例外，Lock下面有一个ReentrantLock，而**ReentrantLock支持公平锁和非公平锁**。

  - 锁的实现在本质上都对应着一个入口等待队列， 如果一个线程没有获得锁， 就会进入等待队列， 当有线程释放锁的时候， 就需要从等待队列中唤醒一个等待的线程。
    - 如果是公平锁， 唤醒的策略就是谁等待的时间长， 就唤醒谁， 很公平；
    - 如果是非公平锁， 则不提供这个公平保证， 有可能等待时间短的线程反而先被唤醒。 
  - 而Lock是支持公平锁的，synchronized不支持公平锁。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十：Java 中提供了 synchronized，为什么还要提供 Lock 呢？

## 局部变量是线程安全的

- 每个方法在调用栈里都会有自己独立的栈帧，每个栈帧里都有对应方法需要的参数和返回地址。当调用方法时，会创建新的栈帧，并压入调用栈；当方法返回时，对应的栈帧就会被自动弹出。
- 我们可以这样说：**栈帧是在调用方法时创建，方法返回时“消亡”。**
- **局部变量存放在哪里？**
  - 局部变量的作用域在方法内部，当方法执行完，局部变量也就没用了。可以这么说，方法返回时，局部变量也就“消亡”了。此时，我们会联想到调用栈的栈帧。没错，**局部变量就是存放在调用栈里的**。
- **调用栈与线程**
  - 两个线程就可以同时用不同的参数调用相同的方法。**那么问题来了，调用栈和线程之间是什么关系呢？答案是：每个线程都有自己独立的调用栈**。
- 此时，我们再看下文中开头的问题：**Java 方法内部的局部变量是否存在并发问题？答案是不存在并发问题！因为每个线程都有自己的调用栈，局部变量保存在线程各自的调用栈里，不会共享，自然也就不存在并发问题。**
- **线程封闭**
  - 方法里的局部变量，因为不会和其他线程共享，所以不会存在并发问题。这种解决问题的技术也叫做线程封闭。官方的解释为：仅在单线程内访问数据。由于不存在共享，所以即使不设置同步，也不会出现并发问题！

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景十一：为啥局部变量是线程安全的？

## 线程池：ThreadPoolExecutor 七大参数

ThreadPoolExecutor 7 大参数

1. int **corePoolSize**：

   - 线程池中的常驻核心线程数

2. int **maximumPoolSize**

   - 线程池能够容纳同时执行的最大线程数，此值必须大于等于 1

3. long **keepAliveTime**

   - 多余的空闲线程的存活时间。当前线程池数量超过 corePoolSize 时，当空闲时间达到 keepAliveTime 值时，多余空闲线程会被销毁直到只剩下 corePoolSize 个线程为止

4. TimeUnit **unit**

   - keepAliveTime 的单位。

5. `BlockingQueue<Runnable>` **workQueue**

   - 任务队列，被提交但尚未被执行的任务。

6. ThreadFactory **threadFactory**

   - 表示生成线程池中工作线程的线程工厂，用于创建线程一般用默认的即可。

7. RejectedExecutionHandler **handler**

   - 拒绝策略，表示当队列满了并且工作线程大于等于线程池的最大线程数（maximumPoolSize）时如何来拒绝

   - 线程池的拒绝策略你谈谈
     - 是什么（1：01）
     - JDK 内置的拒绝策略
       - **AbortPolicy**（默认）：直接抛出 RejectedExecutionException 异常阻止系统正常运行。
       - **CallerRunsPolicy**：“调用者运行”一种调节机制，该策略既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量。
       - **DiscardOldestPolicy**：抛弃队列中等待最久的任务，然后把当前任务加入队列中尝试再次提交当前 任务。
       - **DiscardPolicy**：直接丢弃任务，不予任何处理也不抛出异常。如果允许任务丢失，这是最好的一种方案。
     - 以上内置拒绝策略均实现了 **RejectedExecutionHandler** 接口

参考文档：

1. [《尚硅谷Java大厂面试题第2季》笔记.md](../视频笔记/《尚硅谷Java大厂面试题第2季》笔记.md) P49、P51

## 线程池：Executors 默认创建方法

- 了解

  - **Executors.newScheduledThreadPool(int)**

    - 创建一个核心线程固定大小的线程池，用于定时执行任务。核心线程数量固定，最大线程数量 Integer.MAX_VALUE。适用于定时执行任务的场景。

    - ```java
      return new ScheduledThreadPoolExecutor(corePoolSize);
      
      public ScheduledThreadPoolExecutor(int corePoolSize) {
          // 对应 ThreadPoolExecutor
          super(corePoolSize, Integer.MAX_VALUE,
                DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS,
                new DelayedWorkQueue());
      }
      ```

  - **Executors.newSingleThreadScheduledExecutor()**

    - 创建一个单线程的定时执行线程池。只包含一个线程，用于串行定时执行任务。

    - ```java
      return new DelegatedScheduledExecutorService
                  (new ScheduledThreadPoolExecutor(1));
      ```

  - java8 新出

    - Executors.newWorkStealingPool(int parallelism)

      - 创建一个工作窃取线程池，线程数量根据CPU核心数动态调整。适用于CPU密集型的任务。

      - java8 新增，使用目前机器上可用的处理器作为它的并行级别

      - ```java
        // 不传 parallelism 的重载方法，默认取 Runtime.getRuntime().availableProcessors()
        return new ForkJoinPool
            (parallelism,
             	ForkJoinPool.defaultForkJoinWorkerThreadFactory,
             	null, true);
        ```

- 重点

  - **Executors.newFixedThreadPool(int)**（19：58）

    - 创建一个固定大小的线程池，其中包含指定数量的线程。线程数量是固定的，不会自动扩展。适用于执行固定数量的长期任务。

    - 执行长期的任务，性能好很多

    - ```java
      return new ThreadPoolExecutor(nThreads, nThreads,
                                    0L, TimeUnit.MILLISECONDS,
                                    new LinkedBlockingQueue<Runnable>());
      ```

  - **Executors.newSingleThreadExecutor()**（20：39）

    - 创建一个单线程的线程池。这个线程池中只包含一个线程，用于串行执行任务。适用于需要按顺序执行任务的场景。

    - 一个任务一个任务执行的场景

    - ```java
      return new FinalizableDelegatedExecutorService
                  (new ThreadPoolExecutor(1, 1,
                                          0L, TimeUnit.MILLISECONDS,
                                          new LinkedBlockingQueue<Runnable>()));
      ```

  - **Executors.newCachedThreadPool()**（21：21）

    - 创建一个可缓存的线程池。这个线程池的线程数量可以根据需要自动扩展，如果有可用的空闲线程，就会重用它们；如果没有可用的线程，就会创建一个新线程。适用于执行大量的短期异步任务。

    - 适用：执行很多短期异步的小程序或者负载较轻的服务器

    - ```java
      return new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                                    60L, TimeUnit.SECONDS,
                                    new SynchronousQueue<Runnable>());
      ```

- 你在工作中单一的/固定数的/可变的三种创建线程池的方法，你用哪个多？超级大坑

  - 答案是一个都不用，我们生产上只能使用自定义的

  - Executors 中 JDK 已经给你提供了，为什么不用？

    - （5：10）

      阿里巴巴 Java 开发手册

      4、【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式

      说明：Executors 返回的线程池对象的弊端如下：

      1）FixedThreadPool 和 SingleThreadPool：

      允许的**请求队列长度**为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。

      2）CachedTheadPool 和 ScheduledThreadPool：

      允许的**创建线程数量**为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。

参考文档：

1. [《尚硅谷Java大厂面试题第2季》笔记.md](../视频笔记/《尚硅谷Java大厂面试题第2季》笔记.md) P47、P52
2. [【Java 基础篇】Executors工厂类详解 - CSDN](https://blog.csdn.net/qq_21484461/article/details/133101696)（感觉细节有点问题，还是优先参考源码）

## ForkJoinPool

- 使用场景

  - **ForkJoinPool 就是设计用来解决父子任务有依赖的并行计算问题的。**  

    - 类似于快速排序、二分查找、集合运算等有父子依赖的并行计算问题，都可以用 ForkJoinPool 来解决。

    - 对于 Fibonacci 数列问题，如果用 ForkJoinPool 来实现，其实现代码为：

      ```java
      @Slf4j
      public class ForkJoinDemo {
          // 1. 运行入口
          public static void main(String[] args) {
              int n = 20;
      
              // 为了追踪子线程名称，需要重写 ForkJoinWorkerThreadFactory 的方法
              final ForkJoinPool.ForkJoinWorkerThreadFactory factory = pool -> {
                  final ForkJoinWorkerThread worker = ForkJoinPool.defaultForkJoinWorkerThreadFactory.newThread(pool);
                  worker.setName("my-thread" + worker.getPoolIndex());
                  return worker;
              };
      
              //创建分治任务线程池，可以追踪到线程名称
              ForkJoinPool forkJoinPool = new ForkJoinPool(4, factory, null, false);
      
              // 快速创建 ForkJoinPool 方法
              // ForkJoinPool forkJoinPool = new ForkJoinPool(4);
      
              //创建分治任务
              Fibonacci fibonacci = new Fibonacci(n);
      
              //调用 invoke 方法启动分治任务
              Integer result = forkJoinPool.invoke(fibonacci);
              log.info("Fibonacci {} 的结果是 {}", n, result);
          }
      }
      
      // 2. 定义拆分任务，写好拆分逻辑
      @Slf4j
      class Fibonacci extends RecursiveTask<Integer> {
          final int n;
          Fibonacci(int n) {
              this.n = n;
          }
      
          @Override
          public Integer compute() {
              //和递归类似，定义可计算的最小单元
              if (n <= 1) {
                  return n;
              }
              // 想查看子线程名称输出的可以打开下面注释
              //log.info(Thread.currentThread().getName());
      
              Fibonacci f1 = new Fibonacci(n - 1);
              // 拆分成子任务
              f1.fork();
              Fibonacci f2 = new Fibonacci(n - 2);
              // f1.join 等待子任务执行结果
              return f2.compute() + f1.join();
          }
      }
      ```

    - 如上面代码所示，我们定义了一个 Fibonacci 类，继承了 RecursiveTask 抽象类。在 Fibonacci 类中，我们定义了拆分逻辑，并调用了 join() 等待子线程执行结果。

    - 上面代码中提到的 fork () 和 join () 是 ForkJoinPool 提供的 API 接口，主要用于执行任务以及等待子线程结果。关于其详细用法，我们稍后会讲到。

  - **除了用于处理父子任务有依赖的情形，其实 ForkJoinPool 也可以用于处理需要获取子任务执行结果的场景。**  

    - 例如：我们要计算 1 到 1 亿的和，为了加快计算的速度，我们自然想到算法中的分治原理，将 1 亿个数字分成 1 万个任务，每个任务计算 1 万个数值的综合，利用 CPU 的并发计算性能缩短计算时间。
    - 对比 ThreadPoolExecutor 和 ForkJoinPool 这两者的实现，可以发现它们都有任务拆分的逻辑，以及最终合并数值的逻辑。但 ForkJoinPool 相比 ThreadPoolExecutor 来说，做了一些实现上的封装，例如：
      - 不用手动去获取子任务的结果，而是使用 join () 方法直接获取结果。
      - 将任务拆分的逻辑，封装到 RecursiveTask 实现类中，而不是裸露在外。
    - 因此对于没有父子任务依赖，但是希望获取到子任务执行结果的并行计算任务，也可以使用 ForkJoinPool 来实现。**在这种情况下，使用 ForkJoinPool 实现更多是代码实现方便，封装做得更加好。**

- 使用指南

  - 使用 ForkJoinPool 来进行并行计算，主要分为两步：
    1. 定义 RecursiveTask 或 RecursiveAction 的任务子类。
    2. 初始化线程池及计算任务，丢入线程池处理，取得处理结果。
  - **首先，我们需要定义一个 RecursiveTask 或 RecursiveAction 的子类，然后再该类的 compute () 方法中定义拆分逻辑和计算逻辑。**  这两个抽象类的区别在于：前者有返回值，后者没有返回值。
    - 对于 compute () 方法的实现，核心是想清楚：怎么拆分成子任务？什么时候结束拆分？
  - **接着，初始化 ForkJoinPool 线程池，初始化计算任务，最后将任务丢入线程池中。**

- 原理解析

  - ForkJoinPool 的设计思想是分治算法，即将任务不断拆分（fork）成更小的任务，最终再合并（join）各个任务的计算结果。通过这种方式，可以充分利用 CPU 资源，再结合工作窃取算法（worksteal）整体提高执行效率。
  - 从图中可以看出 ForkJoinPool 要先执行完子任务才能执行上一层任务。**因此 ForkJoinPool 最适合有父子任务依赖的场景，其次就是需要获取子任务执行结果的场景。比如：Fibonacci 数列、快速排序、二分查找等。**

- 源码实现

  - ForkJoinPool 的主要实现类为：ForkJoinPool 和 ForkJoinTask 抽象类。
  - ForkJoinTask 实现了 Future 接口，可以用于获取处理结果。
  - ForkJoinTask 有两个抽象子类：RecursiveAction 和 RecursiveTask 抽象类，其区别在于前者没有返回值，后者有返回值，其类图如下所示。

- 窃取算法

  - 我们知道 ForkJoinPool 的父子任务之间是有依赖关系的，那么 ForkJoinPool 是如何实现的呢？**答案是：利用不同任务队列执行。**  
    - 在 ForkJoinPool 中有一个数组形式的成员变量 `workQueue[]`，其对应一个队列数组，每个队列对应一个消费线程。丢入线程池的任务，根据特定规则进行转发。
  - 这样就有一个问题：有些队列可能任务比较多，有些队列任务比较少，这样就会导致不同线程负载不一样，整体不够高效，怎么办呢？
    - **答案是：利用窃取算法，空闲的线程从尾部去消费其他队列的任务。**
    - 一般情况下，线程获取自己队列中的任务是 LIFO（Last Input First Output 后进先出）的方式，即类似于栈的操作方式。如下图所示，首先放入队列的时候先将任务 Push 进队列的头部（top），之后消费的时候在 pop 出队列头部（top）。
    - 而当某个线程对应的队列空闲时，该线程则去队列的底部（base）窃取（poll）任务到自己的队列，然后进行消费。**那问题来了：为什么不从头部（top）获取任务，而要从底部（base）获取任务呢？**  那是为了避免冲突！如果两个线程同时从顶部获取任务，那就会有多线程的冲突问题，就需要加锁操作，从而降低了执行效率。

参考文档：

1. [深入理解 ForkJoinPool：入门、使用、原理 - 掘金](https://juejin.cn/post/7150836399234236430)

# Spring

## 核心组件

- Data Access / Integration
  - JDBC
  - ORM
  - OXM
  - JMS
  - Transactions
- Web（MVC / Remoting）
  - Web
  - Servlet
  - Portlet
  - Struts
- AOP
- Aspects
- Instrumentation
- Core Container
  - Beans
  - Core
  - Context
  - Expression Language
- Test

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring 核心组件

## 常用模块

- 核心容器
  - 核心容器提供 Spring 框架的基本功能。核心容器的主要组件是 BeanFactory，它是工厂模式的实现。BeanFactory 使用控制反转（IOC）模式将应用程序的配置和依赖性规范与实际的应用程序代码分开。
- Spring 上下文
  - Spring 上下文是一个配置文件，向 Spring 框架提供上下文信息。Spring 上下文包括企业服务，例如 JNDI、EJB、电子邮件、国际化、校验和调度功能。
- Spring AOP
  - 通过配置管理特性，Spring AOP 模块直接将面向切面的编程功能集成到了 Spring 框架中。可以将一些通用任务，如安全、事务、日志等集中进行管理，提高了复用性和管理的便捷性。
- Spring DAO
  - 为 JDBC DAO 抽象层提供了有意义的异常层次结构，可用该结构来管理异常处理和不同数据库供应商抛出的错误消息。异常层次结构简化了错误处理，并且极大地降低了需要编写的异常代码数量（例如打开和关闭连接）。Spring DAO 的面向 JDBC 的异常遵从通用的 DAO 异常层次结构
- Spring ORM
  - Spring 框架插入了若干个 ORM 框架，从而提供了 ORM 的对象关系工具，其中包括 JDO、Hibernate 和 iBatis SQL Map。所有这些都遵从 Spring 的通用事务和 DAO 异常层次结构。
- Spring Web 模块
  - Web 上下文模块建立在应用程序上下文模块之上，为基于 Web 的应用程序提供了上下文。所以，Spring 框架支持与 Jakarta Structs 的集成。Web 模块还简化了处理多部分请求以及将请求参数绑定到域对象的工作。
- Spring MVC 框架
  - MVC 框架是一个全功能的构建 Web 应用程序的 MVC 实现。通过策略接口，MVC 框架便成为高度可配置的，MVC 容纳了大量视图技术，其中包括 JSP、Velocity、Tiles、iText 和 POI。

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring 常用模块

## 常用注解

- 1
  - @Controller
  - @RestController
    - 相当于 @Controller 和 @ResponseBody 的组合效果
  - @Component
  - @Repository
  - @Service
- 2
  - @ResponseBody
    - 异步请求
    - 该注解用于将 Controller 的方法返回的对象，通过适当的 HttpMessageConverter 转换为指定模式后，写入到 Response 对象的 body 数据区
    - 返回的数据不是 html 标签的页面，而是其他某种格式的数据（如 json、xml 等）时使用
  - @RequestMapping
    - 一个用来处理请求地址的注解，可用于类或方法上。用于类上，表示类中的所有响应请求的方法都是以该地址作为父路径
  - @Autowired
    - 它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。通过 @Autorwired 的使用来消除 set、get 方法
  - @PathVariable
    - 用于将请求 URL 中的模板变量映射到功能处理方法的参数上，即取出 url 模板中的变量做为参数
  - @RequestParam
    - 主要用于在 SpringMVC 后台控制层获取参数，类似一种是 request.getParameter("name")
  - @RequestHeader
    - 可以把 Request 请求 header 部分的值绑定到方法的参数上
- 3
  - @ModelAttribute
  - @SessionAttribute
    - 即将值放到 session 作用域中，写在 class 上面
  - @Valid
    - 实体数据校验，可以结合 hibernate validator 一起使用
  - @CookieValue
    - 用来获取 Cookie 中的值

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring 常用注解

## 依赖注入

- 构造器注入
  - 在构造器注入中，依赖关系通过类的构造函数传递。这意味着在创建对象时，依赖的对象实例会作为构造函数的参数传递进来。
  - 优点：
    1. 对象的依赖关系在创建时就被确定，对象一旦创建就不可变，有助于保持对象的一致性和可靠性。
    2. 在构造函数中明确声明依赖，可以使类的使用更加清晰，减少了后续对依赖的猜测。
- setter 注入
  - 在Setter注入中，依赖通过类的setter方法进行注入。这意味着你可以在对象创建后随时改变依赖关系。
  - 优点：
    1. 灵活性高，可以在运行时动态更改依赖关系。
    2. 允许逐步构建对象，不需要一次性提供所有依赖。
- 选择构造器注入还是Setter注入取决于以下因素：
  1. 不变性需求： 如果对象的依赖关系在创建后不应该更改，构造器注入是一个好的选择。
  2. 灵活性需求： 如果对象的依赖关系可能在运行时更改，Setter注入更为合适。
  3. 清晰性： 构造器注入通常更容易理解，因为依赖关系在对象创建时就被确定。
  4. 依赖数量： 如果类有大量的依赖，构造器注入可能更清晰，而不是在构造函数中添加大量的参数。
- 在实践中，有时也可以使用构造器注入和Setter注入的组合，以满足不同的需求。
- 总结：以上论点就是：
  - **构造器注入提倡不可变性：** 通过构造器注入对象，实现了对象初始化后的不可变性，同时确保所需依赖不为空。这有助于保持对象状态的稳定性。
  - **构造器注入促使代码质量提升：** 通过构造器注入，可以清晰地看到类的依赖关系，大量构造器参数说明当前类耦合过多、职责过多，从而促使编码者考虑是否需要重构，以提高代码质量和可维护性。
  - **Setter注入适用于可选依赖：** Setter注入主要用于可选依赖，这些依赖可以在类内部被合理默认赋值。然而，需要注意的是，Setter注入的对象需要进行非空检查，因为它们具有可变性。
  - **Setter注入支持对象的动态重配置：** 通过Setter注入，对象可以在运行时进行重新配置或重新注入。这使得Setter注入在JMX MBeans等需要动态管理的场景下变得特别有用。
- 顺便吐槽一下，网上有各种错误的关于 Spring Bean 依赖注入方式有多少种的回答，数字可以从三种开始一直往上加……
  - 但其实就只有我们提到的：**构造器**和 **setter** 两种。具体可以参考 Spring 官方文档：[**1.4.1. Dependency Injection**: https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#beans-factory-collaborators](https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#beans-factory-collaborators) 文档中可以看到：“DI exists in two major variants: [Constructor-based dependency injection](https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#beans-constructor-injection) and [Setter-based dependency injection](https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#beans-setter-injection).”，即“DI 存在两种：基于构造函数的依赖注入和基于 Setter 的依赖注入。”
  - 网上大多数错误的文章是把 Bean 相关的一些 Spring 使用方式单独作为一种来和这两种注入方式并列，个人认为其实并不合适。
    - 一种错误是把 **xml、注解**实现这些具体的**配置写法**作为单独的一种分离出来，这种错误较为明显。
    - 还有一种错误是误把 **`@Autowired` 或工厂方式**作为单独的种类
      - 其实前者是把**自动装配功能**错误理解成了注入方式的一种
      - 而后者是将 **Bean 的生成**与注入本身混淆了。
  - 个人认为以官方文档为准即可，分享一下相关思考供大家参考。

参考文档

1. [Spring Framework中的依赖注入：构造器注入 vs. Setter注入  - 腾讯云](https://cloud.tencent.com/developer/article/2358469)
2. [函数式编程和设计模式（Cake模式依赖注入）.md](../个人创作/技术文章/函数式编程和设计模式（Cake模式依赖注入）.md) 1 依赖注入

## 自动装配

- xml
  - autowire="byName"(按名称自动装配)
  - autowire="byType" (按类型自动装配)
- 使用注解
  1. @Autowired（按类型自动转配的，不支持id匹配）
     - @Qualifier（不能单独使用，加上@Qualifier则可以根据byName的方式自动装配）
  2. @Resource（先进行byName查找，失败；再进行byType查找）
- @Autowired与@Resource异同
  - 1、@Autowired与@Resource都可以用来装配bean。都可以写在字段上，或写在setter方法上。
  - 2、@Autowired默认按类型装配（属于spring规范），默认情况下必须要求依赖对象必须存在，如果要允许 null 值，可以设置它的required属性为false，如：@Autowired(required=false)，如果我们想使用名称装配可以结合@Qualifier注解进行使用
  - 3、@Resource（属于J2EE复返），默认按照名称进行装配，名称可以通过name属性进行指定。如果没有指定name属性，当注解写在字段上时，默认取字段名进行按照名称查找，如果注解写在setter方法上默认取属性名进行装配。当找不到与名称匹配的bean时才按照类型进行装配。但是需要注意的是，**如果name属性一旦指定，就只会按照名称进行装配**。
  - 它们的作用相同都是用注解方式注入对象，但执行顺序不同。@Autowired先byType，@Resource先byName。

参考文档：

1. [5.自动装配：autowire=“byName“ or “byType“ + 使用注解【@Autowired 、@Qualifier、 @Resource】- CSDN](https://blog.csdn.net/weixin_42214698/article/details/122781230)

## xml bean 标签 autowire 属性

| 模式        | 说明                                                         |
| ----------- | ------------------------------------------------------------ |
| no          | (默认)不采用autowire机制。当我们需要使用依赖注入，只能用 ref |
| byName      | 通过属性的名称自动装配（注入）。Spring会在容器中查找名称与bean属性名称一致的bean，并自动注入到bean属性中。当然bean的属性需要有setter方法。例如：bean A有个属性master，master的setter方法就是setMaster，A设置了autowire=“byName”，那么Spring就会在容器中查找名为master的bean通过setMaster方法注入到A中。 |
| byType      | 通过类型自动装配（注入）。Spring会在容器中查找类（Class）与bean属性类一致的bean，并自动注入到bean属性中，如果容器中包含多个这个类型的bean，Spring将抛出异常。如果没有找到这个类型的bean，那么注入动作将不会执行。 |
| constructor | 类似于byType，但是是通过构造函数的参数类型来匹配。假设bean A有构造函数A(B b, C c)，那么Spring会在容器中查找类型为B和C的bean通过构造函数A(B b, C c)注入到A中。与byType一样，如果存在多个bean类型为B或者C，则会抛出异常。但时与byType不同的是，如果在容器中找不到匹配的类的bean，将抛出异常，因为Spring无法调用构造函数实例化这个bean。 |
| autodetect  | 如果发现默认的构造方法，则用 constructor 模式，否则用 byType 模式 |
| default     | 采用父级标签的配置。（即beans的default-autowire属性）        |

参考文档：

1. [【Spring】Spring配置文件bean标签的autowire属性 - CSDN](https://blog.csdn.net/qq_30081043/article/details/107531417)

## IOC 容器实现（FactoryBean、ApplicationContext、WebApplication）

- 概念

  - Spring 通过一个配置文件描述 Bean 及 Bean 之间的依赖关系，利用 Java 语言的反射功能实例化 Bean 并建立 Bean 之间的依赖关系。
  - Spring 的 IoC 容器在完成这些底层工作的基础上，还提供了 Bean 实例缓存、生命周期管理、 Bean 实例代理、事件发布、资源装载等高级服务。

- Spring 容器高层视图

  - Spring 启动时读取应用程序提供的 Bean 配置信息，并在 Spring 容器中生成一份相应的 Bean 配置注册表，然后根据这张注册表实例化 Bean，装配好 Bean 之间的依赖关系，为上层应用提供准备就绪的运行环境。其中 Bean 缓存池为 HashMap 实现

- IOC 容器实现

  - **BeanFactory -** 框架基础设施

    - BeanFactory 是 Spring 框架的基础设施，面向 Spring 本身；ApplicationContext 面向使用 Spring 框架的开发者，几乎所有的应用场合我们都直接使用 ApplicationContext 而非底层的 BeanFactory。

    - ```mermaid
      classDiagram
      	DefaultListableBeanFactory <|-- XmlBeanFactory
      	BeanDefinitionRegistry <|.. DefaultListableBeanFactory
      	ConfigurableListableBeanFactory <|.. DefaultListableBeanFactory
      	ListableBeanFactory <|-- ConfigurableListableBeanFactory
      	BeanFactory <|-- ListableBeanFactory
      	ConfigurableBeanFactory <|-- ConfigurableListableBeanFactory
      	HierarchicalBeanFactory <|-- ConfigurableBeanFactory
      	BeanFactory <|-- HierarchicalBeanFactory
      	AbstractAutowireCapableBeanFactory <|-- DefaultListableBeanFactory
      	AbstractBeanFactory <|-- AbstractAutowireCapableBeanFactory
      	ConfigurableBeanFactory <|.. AbstractBeanFactory
      	DefaultSingletonBeanRegistry <|-- AbstractBeanFactory
      	SingletonBeanRegistry <|.. DefaultSingletonBeanRegistry
      	AutowireCapableBeanFactory <|.. AbstractAutowireCapableBeanFactory
      	BeanFactory <|-- AutowireCapableBeanFactory
      ```

    - 详解

      1. *BeanDefinitionRegistry* 注册表
         - Spring 配置文件中每一个节点元素在 Spring 容器里都通过一个 BeanDefinition 对象表示，它描述了 Bean 的配置信息。而 BeanDefinitionRegistry 接口提供了向容器手工注册 BeanDefinition 对象的方法。
      2. *BeanFactory* 顶层接口
         - 位于类结构树的顶端 ，它最主要的方法就是 getBean(String beanName)，该方法从容器中返回特定名称的 Bean，BeanFactory 的功能通过其他的接口得到不断扩展
      3. *ListableBeanFactory*
         - 该接口定义了访问容器中 Bean 基本信息的若干方法，如查看 Bean 的个数、获取某一类型 Bean 的配置名、查看容器中是否包括某一 Bean 等方法；
      4. *HierarchicalBeanFactory* 父子级联
         - 父子级联 IoC 容器的接口，子容器可以通过接口方法访问父容器； 通过 HierarchicalBeanFactory 接口， Spring 的 IoC 容器可以建立父子层级关联的容器体系，子容器可以访问父容器中的 Bean，但父容器不能访问子容器的 Bean。
         - Spring 使用父子容器实现了很多功能，比如在 Spring MVC 中，展现层 Bean 位于一个子容器中，而业务层和持久层的 Bean 位于父容器中。这样，展现层 Bean 就可以引用业务层和持久层的 Bean，而业务层和持久层的 Bean 则看不到展现层的 Bean。
      5. *ConfigurableBeanFactory*
         - 是一个重要的接口，增强了 IoC 容器的可定制性，它定义了设置类装载器、属性编辑器、容器初始化后置处理器等方法；
      6. *AutowireCapableBeanFactory* 自动装配
         - 定义了将容器中的 Bean 按某种规则（如按名字匹配、按类型匹配等）进行自动装配的方法；
      7. *SingletonBeanRegistry* 运行期间注册单例 *Bean*
         -  定义了允许在运行期间向容器注册单实例 Bean 的方法；对于单实例（ singleton）的 Bean 来说，BeanFactory 会缓存 Bean 实例，所以第二次使用 getBean() 获取 Bean 时将直接从 IoC 容器的缓存中获取 Bean 实例。Spring 在 DefaultSingletonBeanRegistry 类中提供了一个用于缓存单实例 Bean 的缓存器，它是一个用 HashMap 实现的缓存器，单实例的 Bean 以 beanName 为键保存在这个 HashMap 中。
      8. 依赖日志框架
         - 在初始化 BeanFactory 时，必须为其提供一种日志框架，比如使用 Log4J， 即在类路径下提供 Log4J 配置文件，这样启动 Spring 容器才不会报错。

  - **ApplicationContext** 面向开发应用

    - ApplicationContext 由 BeanFactory 派生而来，提供了更多面向实际应用的功能。

    - ApplicationContext 继承了 HierarchicalBeanFactory 和 ListableBeanFactory 接口，在此基础上，还通过多个其他的接口扩展了 BeanFactory 的功能：

      - ```mermaid
        classDiagram
        	AbstractXmlApplicationContext <|-- FileSystemXmlApplicationContext
        	AbstractXmlApplicationContext <|-- ClassPathXmlApplicationContext
        	AbstractRefreshableConfigApplicationContext <|-- AbstractXmlApplicationContext
        	AbstractRefreshableApplicationContext <|-- AbstractRefreshableConfigApplicationContext
        	AbstractApplicationContext <|-- AbstractRefreshableApplicationContext
        	ConfigurableApplicationContext <|.. AbstractApplicationContext
        	ApplicationContext <|-- ConfigurableApplicationContext
        	Lifecycle <|-- ConfigurableApplicationContext
        	ResourcePatternResolver <|-- ApplicationContext
        	ResourceLoader <|-- ResourcePatternResolver
        	MessageSource <|-- ApplicationContext
        	ApplicationEventPublisher <|-- ApplicationContext
        	HierachicalBeanFactory <|-- ApplicationContext
        	ListableBeanFactory <|-- ApplicationContext
        	BeanFactory <|-- HierachicalBeanFactory
        	BeanFactory <|-- ListableBeanFactory
        ```

    - 详解

      1. ClassPathXmlApplicationContext：默认从类路径加载配置文件
      2. FileSystemXmlApplicationContext：默认从文件系统中装载配置文件
      3. ApplicationEventPublisher：让容器拥有发布应用上下文事件的功能，包括容器启动事件、关闭事件等。
      4. MessageSource：为应用提供 i18n 国际化消息访问的功能；
      5. ResourcePatternResolver ： 所 有 ApplicationContext 实现类都实现了类似于 PathMatchingResourcePatternResolver 的功能，可以通过带前缀的 Ant 风格的资源文件路径装载 Spring 的配置文件。
      6. LifeCycle：该接口是 Spring 2.0 加入的，该接口提供了 start()和 stop()两个方法，主要用于控制异步处理过程。在具体使用时，该接口同时被 ApplicationContext 实现及具体 Bean 实现，ApplicationContext 会将 start/stop 的信息传递给容器中所有实现了该接口的 Bean，以达到管理和控制 JMX、任务调度等目的。
      7. ConfigurableApplicationContext 扩展于 ApplicationContext，它新增加了两个主要的方法：refresh()和 close()，让 ApplicationContext 具有启动、刷新和关闭应用上下文的能力。在应用上下文关闭的情况下调用 refresh()即可启动应用上下文，在已经启动的状态下，调用 refresh()则清除缓存并重新装载配置信息，而调用 close()则可关闭应用上下文。

  - **WebApplication** 体系架构

    - WebApplicationContext 是专门为 Web 应用准备的，它允许从相对于 Web 根目录的路径中装载配置文件完成初始化工作。从 WebApplicationContext 中可以获得 ServletContext 的引用，整个 Web 应用上下文对象将作为属性放置到 ServletContext 中，以便 Web 应用环境可以访问 Spring 应用上下文。

    - 在非 Web 应用的环境下，Bean 只有 singleton 和 prototype 两种作用域。WebApplicationContext 为 Bean 添加了三个新的作用域：request、session 和 global session。

    - ```mermaid
      classDiagram
      	AbstractRefreshableConfigApplicationContext <|-- AbstractXmlApplicationContext
      	AbstractRefreshableApplicationContext <|-- AbstractRefreshableConfigApplicationContext
      	AbstractApplicationContext <|-- AbstractRefreshableApplicationContext
      	ConfigurableApplicationContext <|.. AbstractApplicationContext
      	ApplicationContext <|-- ConfigurableApplicationContext
      	AbstractRefreshableWebApplicationContext <|-- XmlWebApplicationContext
      	AbstractRefreshableWebApplicationContext <|-- AnnotationWebApplicationContext
      	AbstractRefreshableWebApplicationContext <|-- GroovyWebApplicationContext
      	AbstractRefreshableConfigApplicationContext <|-- AbstractRefreshableWebApplicationContext
      	ConfigurableWebApplicationContext <|.. AbstractRefreshableWebApplicationContext
      	WebApplicationContext <|-- ConfigurableWebApplicationContext
      	ApplicationContext <|-- WebApplicationContext
      ```

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring IOC 原理
2. [【Spring】Bean的生命周期.md](./来自BlogBackup的面试资料/【Spring】Bean 的生命周期.md)

## bean 作用域

- **Spring Bean** 作用域
- Spring 3 中为 Bean 定义了 5 种作用域，分别为 **singleton（单例）、prototype（原型）、 request、session 和 global session**
- 5 种作用域说明如下：
  - **singleton**：单例模式（多线程下不安全）
    -  singleton：单例模式，Spring IoC 容器中只会存在一个共享的 Bean 实例，无论有多少个 Bean 引用它，始终指向同一对象。该模式在多线程下是不安全的。Singleton 作用域是 Spring 中的缺省作用域，也可以显示的将 Bean 定义为 singleton 模式
  - **prototype:**原型模式每次使用时创建
    - prototype:原型模式，每次通过 Spring 容器获取 prototype 定义的 bean 时，容器都将创建一个新的 Bean 实例，每个 Bean 实例都有自己的属性和状态，而 singleton 全局只有一个对象。根据经验，对有状态的bean使用prototype作用域，而对无状态的bean使用singleton 作用域。
  - **Request**：一次**request** 一个实例
    - request：在一次 Http 请求中，容器会返回该 Bean 的同一实例。而对不同的 Http 请求则会产生新的 Bean，而且该 bean 仅在当前 Http Request 内有效,当前 Http 请求结束，该 bean 实例也将会被销毁。
  - **session**
    - session：在一次 Http Session 中，容器会返回该 Bean 的同一实例。而对不同的 Session 请求则会创建新的实例，该 bean 实例仅在当前 Session 内有效。同 Http 请求相同，每一次 session 请求创建新的实例，而不同的实例之间不共享属性，且实例仅在自己的 session 请求内有效，请求结束，则实例将被销毁。
  - **global Session**
    - global Session：在一个全局的 Http Session 中，容器会返回该 Bean 的同一个实例，仅在使用 portlet context 时有效。

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring IOC 原理

## bean 生命周期

1. 实例化
   - 实例化一个 Bean，也就是我们常说的 new。
   - 在实例化**前后**可以执行 **InstantiationAwareBeanPostProcessor** 的 postProcessBeforeInstantiation()/postProcessAfterInstantiation()
2. 属性赋值
   - Bean 属性赋值（IOC 依赖注入）
     - 按照 Spring 上下文对实例化的 Bean 进行配置，也就是 IOC 注入。
   - **BeanNameAware**
     - 如果这个 Bean 已经实现了 BeanNameAware 接口，会调用它实现的 setBeanName(String) 方法，此处传递的就是 Spring 配置文件中 Bean 的 id 值
   - BeanClassLoaderAware
   - **BeanFactoryAware**
     - 如果这个 Bean 已经实现了 BeanFactoryAware 接口，会调用它实现的 setBeanFactory，setBeanFactory(BeanFactory)传递的是 Spring 工厂自身（可以用这个方式来获取其它 Bean，只需在 Spring 配置文件中配置一个普通的 Bean 就可以）。
   - EnvironmentAware
     - 设置environment在组件使用时调用
   - EmbeddedValueResolverAware
     - 设置StringValueResolver 用来解决嵌入式的值域问题
   - ResourceLoaderAware
   - ApplicationEventPublisherAware
   - MessageSourceAware
   - **ApplicationContextAware**
     - 如果这个 Bean 已经实现了 ApplicationContextAware 接口，会调用setApplicationContext(ApplicationContext)方法，传入 Spring 上下文（同样这个方式也可以实现步骤"BeanFactoryAware 实现" 的内容，但比它更好，因为 ApplicationContext 是 BeanFactory 的子接口，有更多的实现方法）
   - ServletContextAware
     - 运行时设置ServletContext，在普通bean初始化后调用，在InitializingBean.afterPropertiesSet之前调用，在 ApplicationContextAware 之后调用
     - 注：是在WebApplicationContext 运行时
3. 初始化
   - **InitializingBean 接口**
     - afterPropertiesSet() 方法，先于 init-method 执行
   - **init-method**
     - 如果 Bean 在 Spring 配置文件中配置了 init-method 属性会自动调用其配置的初始化方法。
   - **BeanPostProcessor 接口**
     - 如果这个 Bean 关联了 BeanPostProcessor 接口，在初始化**前后**将会调用 postProcessBeforeInitialization()/postProcessAfterInitialization() 方法。
   - 注：以上工作完成以后就可以应用这个 Bean 了，那这个 Bean 是一个 Singleton 的，所以一般情况下我们调用同一个 id 的 Bean 会是在内容地址相同的实例，当然在 Spring 配置文件中也可以配置非Singleton。
4. Destroy 过期自动清理阶段
   - 单例 bean
     - **DisposableBean 接口**
       - 当 Bean 不再需要时，会经过清理阶段，如果 Bean 实现了 **DisposableBean 这个接口**，会调用那个其实现的 destroy()方法；
     - **destroy-method** 自配置清理
       - 最后，如果这个 Bean 的 Spring 配置中配置了 destroy-method 属性，会自动调用其配置的销毁方法。
       -  bean 标签有两个重要的属性（init-method 和 destroy-method）。用它们你可以自己定制初始化和注销方法。它们也有相应的注解（@PostConstruct 和@PreDestroy）。
   - 多例 bean
     - 直接返回 Bean 给用户，剩下的生命周期由用户控制

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring IOC 原理（讲的一坨大便，不推荐看）
2. [一文读懂 Spring Bean 的生命周期 - CSDN](https://blog.csdn.net/riemann_/article/details/118500805)
3. [Bean的生命周期（不要背了记思想）- 腾讯云](https://cloud.tencent.com/developer/article/2184201)

## 循环依赖

三层缓存

- 加入 singletonFactories 三级缓存的前提是**执行了构造器**，所以构造器的循环依赖无法解决。
- Spring是**不支持**基于**多例Bean**，也就是原型模式下的Bean的setter方法的循环依赖。
- Spring默认是**不支持基于代理对象的setter方法的循环依赖**
- @DependsOn注解主要用于指定Bean的实例化顺序，Spring默认是**不支持基于@DependsOn注解的循环依赖**。

三层缓存：

- singletonObjects
- earlySingletonObjects
- singletonFactories

```java
public class DefaultSingletonBeanRegistry extends SimpleAliasRegistry implements SingletonBeanRegistry {
	...
	// 从上至下 分表代表这“三级缓存”
	private final Map<String, Object> singletonObjects = new ConcurrentHashMap<>(256); //一级缓存，存放完全实例化且属性赋值完成的 Bean ，可以直接使用
	private final Map<String, Object> earlySingletonObjects = new HashMap<>(16); // 二级缓存，存放早期 Bean 的引用，尚未装配属性的 Bean
	private final Map<String, ObjectFactory<?>> singletonFactories = new HashMap<>(16); // 三级缓存，存放实例化完成的 Bean 工厂
    ...
	
	/** Names of beans that are currently in creation. */
	// 这个缓存也十分重要：它表示bean创建过程中都会在里面呆着~
	// 它在Bean开始创建时放值，创建完成时会将其移出~
	private final Set<String> singletonsCurrentlyInCreation = Collections.newSetFromMap(new ConcurrentHashMap<>(16));

	/** Names of beans that have already been created at least once. */
	// 当这个Bean被创建完成后，会标记为这个 注意：这里是set集合 不会重复
	// 至少被创建了一次的  都会放进这里~~~~
	private final Set<String> alreadyCreated = Collections.newSetFromMap(new ConcurrentHashMap<>(256));
    ...
	@Override
	@Nullable
	public Object getSingleton(String beanName) {
		return getSingleton(beanName, true);
	}
	@Nullable
	protected Object getSingleton(String beanName, boolean allowEarlyReference) {
		//1.先从一级缓存中获取，获取到直接返回
		Object singletonObject = this.singletonObjects.get(beanName);
		//2.如果获取不到或对象正在创建，就到二级缓存中去获取，获取到直接返回
		if (singletonObject == null && isSingletonCurrentlyInCreation(beanName)) {
			synchronized (this.singletonObjects) {
				singletonObject = this.earlySingletonObjects.get(beanName);
				//3.如果仍获取不到，且允许 singletonFactories(allowEarlyCurrentlyInCreation()）通过 getObject()获取。
				//就到三级缓存中用 getObject() 获取。
				//获取到就从 singletonFactories中移出，且放进 earlySingletonObjects。
				//（即从三级缓存移动到二级缓存）
				if (singletonObject == null && allowEarlyReference) {
					ObjectFactory<?> singletonFactory = this.singletonFactories.get(beanName);
					if (singletonFactory != null) {
						singletonObject = singletonFactory.getObject();
						this.earlySingletonObjects.put(beanName, singletonObject);
						this.singletonFactories.remove(beanName);
					}
				}
			}
		}
		return singletonObject;
	}
	...
	public boolean isSingletonCurrentlyInCreation(String beanName) {
		return this.singletonsCurrentlyInCreation.contains(beanName);
	}
	protected boolean isActuallyInCreation(String beanName) {
		return isSingletonCurrentlyInCreation(beanName);
	}
	...
}
```

getBean() -> doGetBean() -> createBean() -> doCreateBean() -> 返回 Bean

创建 Bean 的三个核心方法

- createBeanInstance()：例化，即调用对象的构造方法实例化对象
- populateBean()：填充属性，主要对 bean 的依赖属性注入（@Autowired）
- initializeBean()：回到一些如initMethod，InitalizingBean等方法



为什么是三层不是两层：考虑 AOP 代理对象

1. 假设去掉三级缓存：在实例化阶段就得执行后置处理器，判断有 AnnotationAwareAspectJAutoProxyCreator 并创建代理对象。
2. 假设去掉二级缓存：如果去掉了二级缓存，则需要直接在 `singletonFactory.getObject()` 阶段初始化完毕，并放到一级缓存中。那有这么一种场景，B 和 C 都依赖了 A。而多次调用 `singletonFactory.getObject()` 返回的代理对象是不同的，就会导致 B 和 C 依赖了不同的 A。



**参考文档**

1. [彻底搞懂Spring之三级缓存解决循环依赖问题 - 知乎](https://zhuanlan.zhihu.com/p/610322151)
2. [万字长文带你彻底吃透Spring循环依赖，堪称全网最全（文末福利）- 腾讯云](https://cloud.tencent.com/developer/article/2367323)

## AOP 应用场景

1. Authentication 权限
2. Caching 缓存
3. Context passing 内容传递
4. Error handling 错误处理
5. Lazy loading 懒加载
6. Debugging 调试
7. logging, tracing, profiling and monitoring 记录跟踪 优化 校准
8. Performance optimization 性能优化
9. Persistence 持久化
10. Resource pooling 资源池
11. Synchronization 同步
12. Transactions 事务

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring AOP 原理

## AOP 核心概念

1. 切面（aspect）：类是对物体特征的抽象，切面就是对横切关注点的抽象
2. 横切关注点：对哪些方法进行拦截，拦截后怎么处理，这些关注点称之为横切关注点。
3. 连接点（joinpoint）：被拦截到的点，因为 Spring 只支持方法类型的连接点，所以在 Spring 中连接点指的就是被拦截到的方法，实际上连接点还可以是字段或者构造器。
4. 切入点（pointcut）：对连接点进行拦截的定义，即匹配连接点的断言。通知和一个切入点表达式关联，并在满足这个切入点的连接点上运行（例如，当执行某个特定名称的方法时），切入点表达式如何和连接点匹配是 AOP 的核心：Spring 缺省使用 AspectJ 切入点语法
5. 通知（advice）：所谓通知指的就是指拦截到连接点之后要执行的代码，通知分为前置、后置、异常、最终、环绕通知五类。许多 AOP 框架（包括 Spring）都是以拦截器做通知模型，并维护一个以连接点为中心的拦截器链
   1. 前置通知（Before advice）：在某连接点之前执行的通知，但这个通知不能阻止连接点之前的执行流程（除非它抛出一个异常）
   2. 后置通知（After returning advice）：在某连接点正常完成后执行的通知：例如，一个方法没有抛出任何异常，正常返回
   3. 异常通知（After throwing advice）：在方法抛出异常退出时执行的通知
   4. 最终通知（After (finally) advice）：当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）
   5. 环绕通知（Around advice）：包围一个连接点的通知，如方法调用。这是最强大的一种通知类型。环绕通知可以在方法调用前后完成自定义的行为。它也会选择是否继续执行连接点或直接返回它自己的返回值或抛出异常来结束执行。
6. 目标对象（Target Object）：代理的目标对象，即被一个或多个切面所通知的对象。也被称作被通知（adviced）对象。既然 Spring AOP 是通过运行时代理实现的，这个对象永远是一个被代理（proxied）对象
7. AOP 代理（AOP Proxy）：AOP 框架创建的对象，用来实现切面契约（例如通知方法执行等等）。在 Spring 中，AOP 代理可以是 JDK 动态代理或者 CGLIB 代理。
8. 织入（weave）：将切面应用到目标对象并导致代理对象创建的过程。把切面连接到其他的应用程序类型或者对象上，并创建一个被通知的对象。这些可以在编译时（例如使用 AspectJ 编译器），类加载时和运行时完成。Spring 和其他纯 Java AOP 框架一样，在运行时完成织入。
9. 引入（introduction）：在不修改代码的前提下，引入可以在运行期为类动态地添加一些方法或字段。用来给一个类型声明额外的方法或属性（也被称为连接类型声明（inter-type declaration））。Spring 允许引入新的接口（以及一个对应的实现）到任何被代理的对象。例如，你可以使用引入来使一个 bean 实现 JsModified 接口，以便简化缓存机制。

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring AOP 原理

## AOP 两种代理方式

- Spring 提供了两种方式来生成代理对象: JDKProxy 和 Cglib
  - 具体使用哪种方式生成由 AopProxyFactory 根据 AdvisedSupport 对象的配置来决定。
  - 默认的策略是如果目标类是接口，则使用 JDK 动态代理技术，否则使用 Cglib 来生成代理。
- **JDK**动态接口代理
  - JDK 动态代理主要涉及到 java.lang.reflect 包中的两个类：Proxy 和 InvocationHandler
  - InvocationHandler是一个接口，通过实现该接口定义横切逻辑，并通过反射机制调用目标类的代码，动态将横切逻辑和业务逻辑编制在一起。
  - Proxy 利用 InvocationHandler 动态创建一个符合某一接口的实例，生成目标类的代理对象。
- **CGLib** 动态代理
  - CGLib 全称为 Code Generation Library，是一个强大的高性能，高质量的代码生成类库，可以在运行期扩展 Java 类与实现 Java 接口，CGLib 封装了 asm，可以再运行期动态生成新的 class。
  - 和 JDK 动态代理相比较：JDK 创建代理有一个限制，就是只能为接口创建代理实例，而对于没有通过接口定义业务方法的类，则可以通过 CGLib 创建动态代理。

参考文档：

1. 《Java岗面试核心MCA版.pdf》Spring AOP 原理

## 事务隔离级别

- Isolation 属性一共支持五种事务设置，具体介绍如下：
  - DEFAULT 使用数据库设置的隔离级别 ( 默认 ) ，由 DBA 默认的设置来决定隔离级别 .
  - READ_UNCOMMITTED 会出现脏读、不可重复读、幻读 ( 隔离级别最低，并发性能高 )
  - READ_COMMITTED 会出现不可重复读、幻读问题（锁定正在读取的行）
  - REPEATABLE_READ 会出幻读（锁定所读取的所有行）
  - SERIALIZABLE 保证所有的情况不会发生（锁表）

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十五：数据库中事务的隔离级别有哪些？各自有什么特点？

## 事务传播行为

- **事务传播行为种类**
  - Spring在TransactionDefinition接口中规定了7种类型的事务传播行为，它们规定了事务方法和事务方法发生嵌套调用时事务如何进行传播：
  - **PROPAGATION_REQUIRED**
    - 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。
  - **PROPAGATION_SUPPORTS**
    - 支持当前事务，如果当前没有事务，就以非事务方式执行。
  - **PROPAGATION_MANDATORY**
    - 使用当前的事务，如果当前没有事务，就抛出异常。
  - **PROPAGATION_REQUIRES_NEW**
    - 新建事务，如果当前存在事务，把当前事务挂起。
  - **PROPAGATION_NOT_SUPPORTED**
    - 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。
  - **PROPAGATION_NEVER**
    - 以非事务方式执行，如果当前存在事务，则抛出异常。
  - **PROPAGATION_NESTED**
    - 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十五：数据库中事务的隔离级别有哪些？各自有什么特点？

# MySQL

## 实现无数据插入，有数据则更新

- 向MySQL中插入数据，存在就更新，不存在则插入。本质上数据表中还是**需要存在唯一键，也就是唯一索引的**。往往在面试中，面试官都会默许存在这些前置条件。

- 这里，有两种方法可以实现这个效果。

  - 一种方法是结合 INSERT 语句和 **ON DUPLICATE KEY UPDATE 语句**实现

    - 如果指定了ON DUPLICATE KEY UPDATE，并且插入行后会导致在一个UNIQUE索引或PRIMARY KEY中出现重复值，则执行UPDATE。

    - ```mysql
      INSERT INTO table (a,b,c) VALUES (1,2,3) ON DUPLICATE KEY UPDATE c=c+1;
      ```

    - 如果行作为新记录被插入，则受影响行的值为1；如果原有的记录被更新，则受影响行的值为2。

  - 另一种方法是通过 REPLACE 语句实现。

    - 使用 REPLACE 的最大好处就是可以将 DELETE 和 INSERT 合二为一，形成一个原子操作。

    - 这样就可以不必考虑在同时使用 DELETE 和 INSERT 时添加事务等复杂操作了。在使用 REPLACE 时，表中必须有唯一索引，而且这个索引所在的字段不能允许空值，否则 REPLACE 就和 INSERT 完全一样的。

    - 语法和 INSERT 非常的相似，如下面的REPLACE语句是插入或更新一条记录

    - ```mysql
      REPLACE INTO users (id,name,age) VALUES(1, 'binghe', 18);
      ```

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十四：MySQL 如何实现无数据插入，有数据更新？

## B Tree

- B 树的英文是 Balance Tree，也就是多路平衡查找树。简写为 B-Tree（注意横杠表示这两个单词连起来的意思，不是减号）。它的高度远小于平衡二叉树的高度。
- B 树作为多路平衡查找树，它的每一个节点最多可以包括 M 个子节点，M 称为 B 树的阶。每个磁盘块中包括了关键字和子节点的指针。如果一个磁盘块中包括了 x 个关键字，那么指针数就是 x + 1。
- 一个 M 阶的 B 树（M > 2）有以下的特性：
  1. 根节点的儿子数的范围是 [2, M]
  2. **每个中间节点包含 k - 1 个关键字和 k 个孩子**，孩子的数量 = 关键字的数量 + 1，k 的取值范围为 [ceil(M/2), M]。
  3. 叶子节点包括 k-1 个关键字（叶子节点没有孩子），k 的取值范围为 [ceil(M/2), M]。
  4. 假设中间节点的关键字为: Key[1], Key[2], ..., Key[k-1], 且关键字按照升序排序，即 Key[i] < Key[i + 1]。此时 k-1 个关键字相当于划分了 k 个范围，也就是对应着 k 个指针，即为：P[1], P[2], ..., P[k]，其中 P[1] 指向关键字小于 Key[1] 的子树，P[i] 指向关键字属于(Key[i-1], Key[i]) 的子树，P[k] 指向关键字大于 Key[k-1] 的子树。
  5. **所有叶子节点位于同一层**。

- 小结：
  1. B 树在插入和删除时如果导致树不平衡，就通过自动调节节点位置来保持树的自平衡
  2. 关键字集合分布在整个树中，即叶子节点和非叶子节点都存放数据。搜索有可能在非叶子节点结束。
  3. 其搜索性能等价于在关键字全集内做一次二分查找

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P120

## B+ Tree

- B+ 树也是一种多路搜索树，基于 B 树做出了改进，主流的 DBMS 都支持 B+ 树的索引方式，比如 MySQL。相比于 B-Tree，B+Tree 适合文件索引系统。

- **B+ 树和 B 树的差异**在于以下几点：

  1. **有 k 个孩子的节点就有 k 个关键字**。也就是孩子数量 = 关键字数，而 B 树中，孩子数量 = 关键字数 + 1。
  2. 非叶子节点的关键字也会同时存在于子节点中，并且是在子节点中所有关键字的最大（或最小）。
  3. 非叶子节点仅用于索引，不保存数据记录，**跟记录有关的信息都放在叶子节点中**。而 B 树中，非叶子节点既保存索引，也保存数据记录。
  4. 所有关键字都在叶子节点出现，**叶子节点构成一个有序链表**，而且叶子节点本身按照关键字的大小从小到大顺序链接。

- B+ 树和 B 树有个根本的差异在于，B+ 树中间节点并不直接存储数据。这样的好处都有什么呢？

  - 首先，**B+ 树查询效率更稳定**，因为 B+ 树每次只有访问到叶子节点才能找到对应的数据，而在 B 树中，非叶子节点也会存储数据，这样就会造成查询效率不稳定的情况，有时候访问到了非叶子节点就可以找到关键字，而有时需要访问到叶子节点才能找到关键字。
  - 其次，**B+ 树的查询效率更高**。这是因为通常 B+ 树比 B 树更矮胖（阶数更大，深度更低），查询所需要的磁盘 I/O 也会更少。同样的磁盘页大小，B+ 树可以存储更多的节点关键字。
  - 不仅是对单个关键字的查询上，**在查询范围上，B+ 树的效率也比 B 树高**。这是因为所有关键字都出现在 B+ 树的叶子节点中，叶子节点之间会有指针，数据又是递增的，这使得我们范围查找可以通过指针连接查找。而在 B 树中则需要通过中序遍历才能完成查询范围的查找，效率要低很多。

- **思考题：为了减少 IO，索引树会一次性加载吗？**

  1. 数据库索引是存储在磁盘上的，如果数据量很大，必然导致索引的大小也会很大，超过几个 G
  2. 当我们利用索引查询的时候，是不可能将全部几个 G 的索引都加载进内存的，我们能做的只能是：逐一加载每个磁盘页，因为磁盘页对应着索引树的节点。

- **思考题：B+ 树的存储能力如何？为何说一般查找行记录，最多只需 1~3 次磁盘 IO**

  - InnoDB 存储引擎中页的大小为 16KB，一般表的主键类型为 INT（占用 4 个字节）或 BIGINT（占用 8 个字节），指针类型也一般为 4 或 8 个字节，也就是说一个页（B+Tree 中的一个节点）中大概存储 16KB/(8B+8B)=1K 个键值（因为是估值。为方便计算，这里 K 取值为 10^3。也就是说一个深度为 3 的 B+Tree 索引可以维护 10^3 * 10^3 * 10^3 = 10 亿条记录。这里假定一个数据页也存储 10^3 条行记录数据了）
  - 实际情况中每个节点可能不能填充满，因此在数据库中，B+Tree 的高度一般都在 2~4 层。MySQL 的 InnoDB 存储引擎在设计时是将根节点常驻内存的，也就是说查找某个键值的行记录时最多只需要 1 ~ 3 次磁盘 I/O 操作。

- **思考题：为什么说 B+ 树比 B树更适合实际应用中操作系统的文件索引和数据库索引**

  1. B+ 树的磁盘读写代价更低
     - B+ 树的内部节点并没有指向关键字具体信息的指针。因此其内部节点相对于 B 树更小。如果把同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说 IO 读写次数也就降低了。

  2. B+ 树的查询效率更加稳定

     - 由于非终结点并不是最终指向文件内容的节点，而只是叶子节点中关键字的索引。所以任何关键字的查找必须走一条从根节点到叶子节点的路。所有关键字查询的路径长度相同，导致每个数据的查询效率相当。

- **思考题：Hash 索引和 B+ 树索引的区别**

  我们之前讲到过 B+ 树索引的结构，Hash 索引结构和 B+ 树的不同，因此在索引使用上也会有差别。

  1. Hash 索引不能进行范围查询
  2. Hash 索引不支持联合索引的最左侧原则（即联合索引的部分索引无法使用），而 B+ 树可以。
  3. Hash 索引不支持 ORDER BY 排序
  4. InnoDB 不支持哈希索引

- **思考题：Hash 索引与 B+ 树索引是在建索引的时候手动指定吗？**

  - 针对 InnoDB 和 MyISAM 存储引擎，都会默认采用 B+ 树索引，无法使用 Hash 索引。InnoDB 提供的自适应 Hash 是不需要手动指定的。如果是 Memory/Heap 和 NDB 存储引擎，是可以进行选择 Hash 索引的。

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P120

## 索引类型

- 索引按照**物理实现**方式，索引可以分为 2 种：**聚簇（聚集）**和**非聚簇（非聚集）**索引。
  - 我们也把非聚簇索引称为**二级索引**或者**辅助索引**。
  - 以多个列的大小为排序规则建立的 B+ 树称为**联合索引**，本质上也是一个二级索引。
- 按照**功能逻辑**区分，MySQL目前主要有以下索引类型：
  - **主键索引**
    - 数据列不允许重复，不允许为 NULL，一个表只能有一个主键。
    - `ALTER TABLE table_name ADD PRIMARY KEY (column);`
  - **普通索引**
    - MySQL中基本索引类型，没有什么限制，允许在定义索引的列中插入重复值和NULL值。一个表允许多个列创建普通索引。
    - `ALTER TABLE table_name ADD INDEX index_name (column);`
  - **唯一索引**
    - 索引列中的值必须是唯一的，但是允许NULL值。建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。一个表允许多个列创建唯一索引。
    - `ALTER TABLE table_name ADD UNIQUE (column);`
  - **全文索引**
    - 主要是为了快速检索大文本数据中的关键字的信息。字段长度比较大时，如果创建普通索引，在进行like模糊查询时效率比较低，这时可以创建全文索引，基于倒排索引，类似于搜索引擎。MyISAM存储引擎支持全文索引，InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。
    - `ALTER TABLE table_name ADD FULLTEXT (column);`
  - **前缀索引**
    - 在文本类型如BLOB、TEXT或者很长的VARCHAR列上创建索引时，可以使用前缀索引，数据量相比普通索引更小，可以指定索引列的长度，但是数值类型不能指定。
    - `ALTER TABLE table_name ADD KEY(column_name(prefix_length));`
  - **组合索引**
    - 指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀原则。
    - 主键索引、普通索引、唯一索引等都可以使用多个字段形成组合索引。例如，`ALTER TABLE table_name ADD INDEX index_name ( column1, column2, column3 );`
  - **空间索引**
    - MySQL在5.7之后的版本支持了空间索引，而且支持OpenGIS几何数据模型。MySQL在空间索引这方面遵循OpenGIS几何数据模型规则。

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P118
2. [MySQL索引的概念以及七种索引类型介绍 - CSDN](https://blog.csdn.net/weixin_43767015/article/details/119109385)

## 索引失效

- **最佳左前缀法则**
  - 在 MySQL 建立联合索引时会遵守最佳左前缀匹配原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。
  - 如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。
  - 对于多列索引，**过滤条件要使用索引必须按照索引建立时的顺序，依次满足，一旦跳过某个字段，索引后面的字段都无法被使用**。如果查询条件中没有使用这些字段中第 1 个字段时，多列（或联合）索引不会被使用。
- **主键**插入顺序
  - 对于一个使用 `InnoDB` 存储引擎的表来说，在我们没有显式的创建索引时，表中的数据实际上都是存储在聚簇索引的叶子节点的。
  - 而记录又是存储在数据页中的，数据页和记录又是按照记录主键值从小到大的顺序进行排序，所以如果我们插入的记录的主键值是依次增大的话，那我们每插满一个数据页就换到下一个数据页继续插，而如果我们插入的主键值忽大忽小的话，就比较麻烦了。
  - 所以我们建议：让主键具有 `AUTO_INCREMENT`，让存储引擎自己为表生成主键，而不是我们手动插入。
    - 数据页已经满了，再插进来怎么办呢？我们需要把当前页面分裂成两个页面，把本页中的一些记录移动到新创建的这个页中。**页面分裂**和**记录移位**意味着什么？意味着：性能损耗！所以如果我们想尽量避免这样无谓的性能损耗，最好让插入的记录的主键值依次递增，这样就不会发生这样的性能损耗了。
- **计算、函数、类型转换（自动或手动）**导致索引失效
- **类型转换**导致索引失效
- **范围条件**右边的列索引失效
  - 范围右边的列不能使用。比如：（<）（<=）（>）（>=）和 between 等
  - 将范围查询条件放置语句最后
- **不等于**（!= 或者 <>）索引失效
- is null 可以使用索引，**is not null** 无法使用索引
- **like 以通配符 % 开头**索引失效
- **OR 前后存在非索引的列**，索引失效
  - 在 WHERE 子句中，如果在 OR 前的条件列进行了索引，而在 OR 后的条件列没有进行索引，那么索引会失效。也就是说，**OR 前后的两个条件中的列都是索引时，查询中才使用索引**。
  - 前后都是索引列时可以使用索引合并：index_merge
- 数据库和表的字符集统一使用 utf8mb4
  - 统一使用 utf8mb4（5.5.3 版本以上支持）兼容性更好，统一字符集可以避免由于字符集转换产生的乱码。**不同的字符集进行比较前需要进行转换会造成索引失效**。

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P141 ~ 142

## 覆盖索引

- 什么是覆盖索引？
  - **理解方式一：**索引是高效找到行的一个方法，但是一般数据库也能使用索引找到一个列的数据，因此它不必读取整个行。毕竟索引叶子节点存储了它们索引的数据；当能通过读取索引就可以得到想要的数据，那就不需要读取行了。**一个索引包含了满足查询结果的数据就叫做覆盖索引**。
  - **理解方式二**：非聚簇符合索引的一种形式，它包括在查询里的 SELECT、JOIN 和 WHERE 子句用到的所有列（即建索引的字段正好是覆盖查询条件中所涉及的字段）。
  - 简单说就是，`索引列+主键`包含 SELECT 到 FROM 之间查询的列。
- 覆盖索引的利弊
  - **好处：**
    - **1、避免 InnoDB 表进行索引的二次查询（回表）**
      - InnoDB 是以聚集索引的顺序来存储的，对于 InnoDB 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据，在查找到相应的键值后，还需通过主键进行二次查询才能获取我们真实所需要的数据。
      - 在覆盖索引中，二级索引的键值中可以获取所要的数据，避免了对主键的二次查询，减少了 IO 操作，提升了查询效率。
    - **2、可以把随机 IO 变成顺序 IO 加快查询效率**
      - 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此引用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。
    - **由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。**
  - **弊端：**
    - 索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这是业务 DBA，或者称为业务数据架构师的工作。

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P147

## 索引下推

- Index Condition Pushdown（ICP）是 MySQL 5.6 中新特性，是一种在存储引擎层使用索引过滤数据的优化方式。

  - 如果没有 ICP，存储引擎会遍历索引以定位基表中的行，并将它们返回给 MySQL 服务器，由 MySQL 服务器评估 `WHERE` 后面的条件是否保留行。
  - 启用 ICP 后，如果部分 `WHERE` 条件可以仅使用索引中的列进行筛选，则 MySQL 服务器会把这部分 `WHERE` 条件放到存储引擎筛选。然后存储引擎通过使用索引条目来筛选数据，并且只有在满足这一条件时才从表中读取行。
    - 好处：ICP 可以减少存储引擎必须访问基表的次数和 MySQL 服务器必须访问存储引擎的次数。
    - 但是，ICP 的加速效果取决于存储引擎内通过 ICP 筛选掉的数据的比例。

- ICP 的开启/关闭

  - 默认情况下启用索引条件下推。可以通过设置系统变量 `optimizer_switch` 控制：`index_condition_pushdown`

    ```mysql
    # 打开索引下推
    SET optimizer_switch = 'index_condition_pushdown=on';
    # 关闭索引下推
    SET optimizer_switch = 'index_condition_pushdown=off';
    ```

  - 当使用索引条件下推时，`EXPLAIN` 语句输出结果中 `Extra` 列内容显示为 `Using index condition`。

- ICP 使用案例

  - 建表

  - 插入数据

  - 为该表定义联合索引 zip_last_first(zipcode, lastname, firstname)。如果我们知道了一个人的邮编，但是不确定这个人的姓氏，我们可以进行如下检索：

    ```mysql
    SELECT * FROM people
    WHERE zipcode='000001'
    AND lastname LIKE '%张%'
    AND address LIKE '%北京市%';
    ```

    执行查看 SQL 的查询计划，`Extra` 中显示了 `Using index condition`，这表示使用了索引下推。另外， Using where 表示条件中包含需要过滤的非索引列的数据，即 address LIKE '%北京市%' 这个条件并不是索引列，需要在服务端过滤掉。

    如果不想出现 Using where，把 address LIKE '%北京市%' 去掉即可

    这个表中存在两个索引，分别是：

    - 主键索引
    - 二级索引 zip_last_first

    下面我们关闭 ICP 查看执行计划

    ```mysql
    SET optimizer_switch = 'index_condition_pushdown=off';
    ```

    查看执行计划，已经没有了 Using index condition，表示没有使用 ICP

    ```mysql
    EXPLAIN SELECT * FROM people
    WHERE zipcode='000001'
    AND lastname LIKE '%张%'
    AND address LIKE '%北京市%';
    ```

- ICP 的使用条件

  1. 如果表访问的类型为 range、ref、eq_ref 和 ref_or_null 可以使用 ICP
  2. ICP 可以用于 `InnoDB` 和 `MyISAM` 表，包括分区表 `InnoDB` 和 `MyISAM` 表
  3. 对于 `InnoDB` 表，ICP 仅用于二级索引。ICP 的目标是减少全行读取次数，从而减少 I/O 操作。
  4. 当 SQL 使用覆盖索引时，不支持 ICP。因为这种情况下使用 ICP 不会减少 I/O。
  5. 相关子查询的条件不能使用 ICP

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P148

## ACID

- **原子性（atomicity）**

  原子性是指事务是一个不可分割的工作单位，要么全部提交，要么全部失败回滚。即要么转账成功，要么转账失败，是不存在中间的状态。如果无法保证原子性会怎么样？就会出现数据不一致的情形，A 账户减去 100 元，而 B 账户增加 100 元操作失败，系统将无故丢失 100 元。

- **一致性（consistency）**

  根据定义，一致性是指事务执行前后，数据从一个**合法性状态**变换到另外一个**合法性状态**。这种状态是**语义上**的而不是语法上的，跟具体的业务有关。

  那么什么是合法的数据状态呢？满足**预定的约束**的状态就叫做合法的状态。通俗一点，这状态是由你自己来定义的（比如满足现实世界中的约束）。满足这个状态，数据就是一致的，不满足这个状态，数据就是不一致的！如果事务中某个操作失败了，系统就会自动撤销当前正在执行的事务，返回到事务操作之前的状态。

- **隔离性（isolation）**

  事务的隔离性是指一个事务的执行**不能被其他事务干扰**，即一个事务内部的操作及使用的数据对**并发**的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。

- **持久性（durability）**

  持久性是指一个事务一旦被提交，它对数据库中数据的改变就是**永久性的**，接下来的其他操作和数据库故障不应该对其有任何影响。

  持久性是通过**事务日志**来保证的。日志包括了**重做日志**和**回滚日志**。当我们通过事务对数据进行修改的时候，首先会将数据库的变化信息记录到重做日志中，然后再对数据库中对应的行进行修改。这样做的好处是，即使数据库系统崩溃，数据库重启后也能找到没有更新到数据库系统中的重做日志，重新执行，从而使事务具有持久性。

> 总结
>
> ACID 是事务的四大特性，在这四个特性中，原子性是基础，隔离性是手段，一致性是约束条件，而持久性是我们的目的。
>
> 数据库事务，其实就是数据库设计者为了方便起见，把需要保证**原子性**、**隔离性**、**一致性**和**持久性**的一个或多个数据库操作称为一个事务

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P161

## 事务隔离级别

- 数据库提供了四种事务隔离级别, 不同的隔离级别采用不同的锁类开来实现。
  - 在四种隔离级别中, Serializable 的级别最高, Read Uncommited级别最低。
  - 大多数数据库的默认隔离级别为: Read Commited,如 Sql Server , Oracle。
  - 少数数据库默认的隔离级别为 Repeatable Read, 如 MySQL InnoDB 存储引擎 。
  - 即使是最低的级别,也不会出现 第一类 丢失 更新问题。
- 事务并发问题
  - 1.脏读(事务没提交，提前读取)
    - 脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。
  - 2.不可重复读(两次读的不一致)
    - 指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。
    - 例如，一个编辑人员两次读取同一文档，但在两次读取之间，作者重写了该文档。当编辑人员第二次读取文档时，文档已更改。原始读取不可重复。如果只有在作者全部完成编写后编辑人员才可以读取文档，则可以避免该问题。
  - 3.幻读
    - 指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好像发生了幻觉一样。
    - 例如，一个编辑人员更改作者提交的文档，但当生产部门将其更改内容合并到该文档的主复本时，发现作者已将未编辑的新材料添加到该文档中。如果在编辑人员和生产部门完成对原始文档的处理之前，任何人都不能将新材料添加到文档中，则可以避免该问题。
  - 4.第一类更新丢失(回滚丢失)
    - 当2个事务更新相同的数据源，如果第一个事务被提交，而另外一个事务却被撤销，那么会连同第一个事务所做的更新也被撤销。也就是说第一个事务做的更新丢失了。
  - 5.第二类更新丢失(覆盖丢失)
    - 第二类更新丢失实在实际应用中经常遇到的并发问题，他和不可重复读本质上是同一类并发问题，通常他被看做不可重复读的特例：当2个或这个多个事务查询同样的记录然后各自基于最初的查询结果更新该行时，会造成第二类丢失更新。因为每个事务都不知道不知道其他事务的存在，最后一个事务对记录做的修改将覆盖其他事务对该记录做的已提交的更新。
- **不可重复读和幻读的区别**
  - 不可重复读的重点是修改：同样的条件，你读取过的数据，再次读取出来发现值不一样了 。
  - 幻读的重点在于新增或者删除：同样的条件, 第1次和第 2 次读出来的记录数不一样。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十五：数据库中事务的隔离级别有哪些？各自有什么特点？

## 慢查询

- 默认情况下，MySQL 数据库没有开启慢查询日志，需要我们手动来设置这个参数。

  - 如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。

- 开启慢查询日志步骤

  - 开启 slow_query_log

    在使用前，我们需要先看下慢查询是否已经开启，使用下面这条命令即可：

    ```mysql
    show variables like '%slow_query_log%';
    ```

    我们能看到 `slow_query_log=OFF`，我们可以把慢查询日志打开，注意设置变量值的时候需要使用 global，否则会报错：

    ```mysql
    set global slow_query_log='ON';
    ```

    然后我们再来查看下慢查询日志是否开启，以及慢查询日志文件的位置（slow_query_log_file）：

    你能看到这时慢查询分析已经开启，同时文件保存在 `/var/lib/mysql/atguigu02-slow.log` 文件中。

  - 修改 long_query_time 阈值

    接下来我们来看下慢查询的时间阈值设置，使用如下命令：

    ```mysql
    show variables like '%long_query_time%';
    ```

    这里如果我们想把时间缩短，比如设置为 1 秒，可以这样设置：

    ```mysql
    #测试发现：设置 global 的方式对当前 session 的 long_query_time 失效，对新连接的客户端有效，所以可以一并执行下述语句
    set global long_query_time = 1;
    show global variables like '%long_query_time%';
    
    set long_query_time = 1;
    show variables like '%long_query_time%';
    ```

    **补充：配置文件中一并设置参数**

    如下的方式相较于前面的命令行方式，可以看作是永久设置的方式。

    修改 `my.cnf` 文件，[mysqld]下增加或修改参数 `long_query_time`、`slow_query_log` 和 `slow_query_log_file` 后，然后重启 MySQL 服务器。

    ```properties
    [mysqld]
    slow_query_log=ON # 开启慢查询日志的开关
    slow_query_log_file=/var/lib/mysql/atguigu-slow.log #慢查询日志的目录和文件名信息
    long_query_time=3 #设置慢查询的阈值为 3 秒，超出此设定值的 SQL 即被记录到慢查询日志
    log_output=FILE
    ```

    如果不指定存储路径，慢查询日志将默认存储到 MySQL 数据库的数据文件夹下。如果不指定文件名，默认文件名为 hostname-slow.log

- 查看慢查询数目

  - 查询当前系统中有多少条慢查询记录：

    ```mysql
    SHOW GLOBAL STATUS LIKE `%Slow_queries%`;
    ```

- 慢查询日志分析工具：mysqldumpslow

  - 在生产环境中，如果要手工分析日志，查找、分析 SQL，显然是个体力活，MySQL 提供了日志分析工具 `mysqldumpslow`。

  - 查看 mysqldumpslow 的帮助信息

    ```mysql
    mysqldumpslow --help
    ```

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P135

## EXPLAIN 执行计划

- 版本情况

  - MySQL 5.6.3 以前只能 `EXPLAIN SELECT`；MySQL 5.6.3 以后就可以 `EXPLAIN SELECT, UPDATE, DELETE`
  - 在 5.7 以前的版本中，想要显示 `partitions` 需要使用 `explain partitions` 命令；想要显示 `filtered` 需要使用 `explain extended` 命令。在 5.7 版本后，默认 explain 直接显示 partitions 和 filtered 中的信息。

- EXPLAIN 语句输出的各个列的作用如下：

  | 列名          | 描述                                                      |
  | ------------- | --------------------------------------------------------- |
  | id            | 在一个大的查询语句中每个 SELECT 关键字都对应一个唯一的 id |
  | select_type   | SELECT 关键字对应的那个查询的类型                         |
  | table         | 表名                                                      |
  | partitions    | 匹配的分区信息                                            |
  | type          | 针对单表的访问方法                                        |
  | possible_keys | 可能用到的索引                                            |
  | key           | 实际上使用的索引                                          |
  | key_len       | 实际使用到的索引长度                                      |
  | ref           | 当使用索引列等值查询时，与索引列进行等值匹配的对象信息    |
  | rows          | 预估的需要读取的记录条数                                  |
  | filtered      | 某个表经过搜索条件过滤后剩余记录条数的百分比              |
  | Extra         | 一些额外信息                                              |

- 各列作用

  - table

    - MySQL 规定 EXPLAIN 语句输出的每条记录都对应着某个单表的访问方法，该条记录的 table 列代表着该表的表名（有时不是真实的表名字，可能是简称）。

  - id

    - **查询语句中每出现一个 `SELECT` 关键字，MySQL 就会为它分配一个唯一的 `id` 值**。

    - 这个 `id` 值就是 `EXPLAIN` 语句的第一个列

    - 这里需要大家记住的是，**在连接查询的执行计划中，每个表都会对应一条记录，这些记录的 id 列的值是相同的**，出现在前边的表表示驱动表，出现在后边的表表示被驱动表。

    - 对于包含子查询的查询语句来说，就可能涉及多个 `SELECT` 关键字，所以在**包含子查询的查询语句的执行计划中，每个 `SELECT` 关键字都会对应一个唯一的 `id` 值**

    - 但是这里大家需要特别注意，**查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询。**所以如果我们想知道查询优化器对某个包含子查询的语句是否进行了重写，直接查看执行计划就好了

    - UNION 子句、UNION ALL 的情况：

      - 对于包含 `UNION` 子句的查询语句来说，每个 `SELECT` 关键字对应一个 `id` 值也是没错的，不过还是有点儿特别的东西，比方说下边这个查询：

        ```mysql
        EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2;
        ```

        这个语句的执行计划的第三条记录是什么？为何 `id` 值是 `NULL`，而且 table 列也很奇怪？`UNION`！它会把多个查询的结果集并起来并对结果集中的记录进行去重，怎么去重呢？MySQL 使用的是内部的临时表。正如上边的查询计划中所示，UNION 子句是为了把 id 为 1 的查询和 id 为 2 的查询的结果集合并起来并去重，所以在内部创建了一个名为 `<union1, 2>` 的临时表（就是执行计划第三条记录的 table 列的名称），id 为 `NULL` 表明这个临时表是为了合并两个查询的结果集而创建的。

      - 跟 UNION 对比起来，`UNION ALL` 就不需要为最终的结果集进行去重，它只是单纯的把多个查询的结果集中的记录合并成一个并返回给用户，所以也就不需要使用临时表。所以在包含 `UNION ALL` 子句的查询的执行计划中，就没有那个 id 为 NULL 的记录

    - 小结：

      - id 如果相同，可以认为是一组，从上往下顺序执行
      - 在所有组中，id 值越大，优先级越高，越先执行
      - 关注点：id 号每个号码，表示一趟独立的查询，一个 sql 的查询趟数越少越好

  - select_type

    - | 名称                 | 描述                                                         |
      | -------------------- | ------------------------------------------------------------ |
      | SIMPLE               | Simple SELECT (not using UNION or subqueries)<br />查询语句中不包含 `UNION` 或者子查询的查询都算作是 `SIMPLE` 类型<br />当然，连接查询也算是 `SIMPLE` 类型 |
      | PRIMARY              | Outermost SELECT<br />对于包含 `UNION`、`UNION ALL` 或者子查询的大查询组成的，其中最左边的那个查询的 `select_type` 值就是 `PRIMARY` |
      | UNION                | Second or later SELECT statement in a UNION<br />对于包含 `UNION` 或者 `UNION ALL` 的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的 `select_type` 值就是 `UNION` |
      | UNION RESULT         | Result of a UNION<br />`MySQL` 选择使用临时表来完成 `UNION` 查询的去重工作，针对该临时表的查询的 `select_type` 就是 `UNION RESULT` |
      | SUBQUERY             | First SELECT in subquery<br />如果包含子查询的查询语句不能够转为对应的 `semi-join` 的形式，并且该子查询是不相关子查询，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个 `SELECT` 关键字代表的那个查询的 `select_type` 就是 `SUBQUERY` |
      | DEPENDENT SUBQUERY   | First SELECT in subquery, dependent on outer query           |
      | DEPENDENT UNION      | Second or later SELECT statement in a UNION, dependent on outer query |
      | DERIVED              | Derived table                                                |
      | MATERIALIZED         | Materialized subquery<br />当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询对应的 `select_type` 属性就是 `MATERIALIZED` |
      | UNCACHEABLE SUBQUERY | A subquery for which the result cannot be cached and must be re-evaluated for each row of the outer query |
      | UNCACHEABLE UNION    | The second or later select in a UNION that belongs to an uncacheable subquery (see UNCACHEABLE SUBQUERY) |

  - partitions

    - 代表分区表中的命中情况，非分区表，该项为 `NULL`。

  - type

    - 执行计划的一条记录就代表着 MySQL 对某个表的执行查询时的访问方法，又称“访问类型”，其中的 `type` 列就表明了这个访问方法是啥，是较为重要的一个指标。

    - 完整的访问方法如下：`system`，`count`，`eq_ref`，`ref`，`fulltext`，`ref_or_null`，`index_merge`，`unique_subquery`，`index_subquery`，`range`，`index`，`ALL`。

    - 详细解释

      - system

        - 当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如 MyISAM、Memory，那么对该表的访问方法就是 `system`。
          - 测试：可以把表改成使用 InnoDB 存储引擎，试试看执行计划的 `type` 列是什么？ALL

      - const

        - 当我们根据**主键**或者**唯一**二级索引列与常数进行等值匹配时，对单表的访问方法就是 `const`

      - eq_ref

        - 在**连接查询**时，如果**被驱动表**是通过**主键**或者**唯一**二级索引列等值匹配的方式进行访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较）则对该被驱动表的访问方法就是 `eq_ref`

      - ref

        - 当通过**普通的二级索引**列与常量进行等值匹配时来查询某个表，那么对该表的访问方法就可能是 `ref`

      - fulltext

        - 全文索引

      - ref_or_null

        - 当对**普通二级索引**进行等值匹配查询，该索引列的值也可以是 `NULL` 值时，那么对该表的访问方法就可能是 `ref_or_null`

      - index_merge

        - 一般情况下对于某个表的查询只能使用到一个索引，但单表访问方法时在某些场景下可以使用 **`Intersection`、`Union`、`Sort-Union`** 这三种索引合并的方式来执行查询。

        - 例子：

          ```mysql
          EXPLAIN SELECT * FROM s1 WHERE key1 = 'a' OR key3 = 'a';
          ```

      - unique_subquery

        - 类似于两表连接中的被驱动表的 `eq_ref` 访问方法，`unique_subquery` 是针对在一些**包含 `IN` 子查询**的查询语句中，如果查询优化器决定将 `IN` 子查询转换为 `EXISTS` 子查询，而且子查询可以使用到**主键**进行**等值匹配**的话，那么该子查询执行计划的 `type` 列的值就是 `unique_subquery`

      - index_subquery

        - `index_subquery` 与 `unique_subquery` 类似，只不过访问子查询中的表时使用的是**普通的索引**

      - range

        - 如果**使用索引**获取某些**特定范围**的记录，那么就可能使用到 `range` 访问方法

      - index

        - 当我们可以使用索引覆盖，但**需要扫描全部的索引记录**时，该表的访问方法就是 `index`

      - ALL

        - 最熟悉的全表扫描

    - **小结：**

      - 结果值从最好到最坏依次是：

        **system > const > eq_ref > ref** > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > **range > index > ALL**

        其中比较重要的几个提取出来（见粗体）。

      - SQL 性能优化的目标：至少要达到 range 级别，要求是 ref 级别，最好是 const 级别。（阿里巴巴开发手册要求）

  - possible_keys

    - `possible_keys` 列表示在某个查询语句中，对某个表执行单表查询时可能用到的索引有哪些。一般查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用。

  - keys

    - `key` 列表示实际用到的索引有哪些，如果为 NULL，则没有使用索引。

  - key_len

    - 实际使用到的索引长度（即：字节数）
    - 帮你检查是否充分的利用上了索引，值越大越好，主要针对于联合索引，有一定的参考意义。

  - ref

    - 显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量。哪些列或常量被用于查找索引列上的值。
    - 当使用索引列等值匹配的条件去执行查询时，也就是在访问方法是 `const`、`eq_ref`、`ref`、`ref_or_null`、`unique_subquery`、`index_subquery` 其中之一时，`ref` 列展示的就是与索引列作等值匹配的结构是什么，比如只是一个常数或者是某个列。

  - rows

    - 预估的需要读取的记录条数
    - 值越小越好

  - filtered

    - 某个表经过搜索条件过滤后剩余记录条数的**百分比**

  - extra

    - 顾名思义，`Extra` 列是用来说明一些额外信息的，包含不适合在其他列中显示但十分重要的额外信息。

    - `No tables used`

      - 当查询语句的没有 `FROM` 子句时将会提示该额外信息

    - `Impossible WHERE`

      - 查询语句的 `WHERE` 子句永远为 `FALSE` 时将会提示该额外信息

    - `Using where`

      - 不用读取表中所有信息，仅通过索引就可以获取所需数据，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示 mysql 服务器将在存储引擎检索行后再进行过滤。表明使用了 where 过滤。
      - 当我们使用全表扫描来执行对某个表的查询，并且该语句的 `WHERE` 子句中有针对该表的搜索条件时，在 `Extra` 列中会提示上述额外信息。

    - `No matching min/max row`

      - 当查询列表处有 `MIN` 或者 `MAX` 聚集函数，但是并没有符合 `WHERE` 子句中的搜索条件的记录时，将会提示该额外信息

    - `Using index`

      - 当我们的查询列表以及搜索条件中只包含属于某个索引的列，也就是在**可以使用索引覆盖**的情况下，在 `Extra` 列将会提示该额外信息。

    - `Using index condition`

      - 如果在查询语句的执行过程中将要使用索引条件下推这个特性，在 Extra 列中将会显示 `Using index condition`

      - **索引条件下推**

        - 有些搜索条件中虽然出现了索引列，但却不能使用到索引

        - 案例：

          - ```mysql
            SELECT * FROM s1 WHERE key1 > 'z' AND key1 LIKE '%a';
            ```

          - 先根据 `key1 > 'z'` 这个条件，定位到二级索引 `idx_key1` 中对应的二级索引记录，先不着急回表，而是先检测一下该记录是否满足 `key1 LIKE '%a'` 这个条件，如果这个条件不满足，则该二级索引记录压根儿就没必要回表。

        - 回表操作其实是一个随机 IO，比较耗时，所以上述修改虽然只改进了一点点，但是可以省去好多回表操作的成本。

    - `Using join buffer (Block Nested Loop)`

      - 在连接查询执行过程中，当被驱动表不能有效的利用索引加快访问速度，MySQL 一般会为其分配一块名叫 `join buffer` 的内存块来加快查询速度，也就是我们所讲的基于块的嵌套循环算法

    - `Not exists`

      - 当我们使用左（外）连接时，如果 `WHERE` 子句中包含要求被驱动表的某个列等于 `NULL` 值的搜索条件，而且那个列又是不允许存储 `NULL` 值的，那么在该表的执行计划的 Extra 列就会提示 `Not exists` 额外信息

    - `Using intersect(...)`、`Using union(...)` 和 `Using sort_union(...)`

      - 如果执行计划的 `Extra` 列出现了 `Using intersect(...)` 提示，说明准备使用 `Intersect` 索引合并的方式执行查询，括号中的 ... 表示需要进行索引合并的索引名称；
      - 如果出现了 `Using union(...)` 提示，说明准备使用 `Union` 索引合并的方式执行查询；
      - 出现了 `Using sort_union(...)` 提示，说明准备使用 `Sort-Union` 索引合并的方式执行查询。

    - `Zero limit`

      - 当我们的 `LIMIT` 子句参数为 0 时，表示压根不打算从表中读出任何记录，将会提示该额外信息

    - `Using filesort`

      - 但是很多情况下排序操作无法使用到索引，只能在内存中（记录较少的时候）或者磁盘中（记录较多的时候）进行排序，MySQL 把这种在内存中或者磁盘上进行排序的方式统称为文件排序（英文名：`filesort`）。
      - 如果某个查询需要使用文件排序的方式执行查询，就会在执行计划的 `Extra` 列中显示 `Using filesort` 提示

    - `Using temporary`

      - 在许多查询的执行过程中，MySQL 可能会借助临时表来完成一些功能，比如去重、排序之类的，比如我们在执行许多包含 `DISTINCT`、`GROUP BY`、`UNION` 等子句的查询过程中，如果不能有效利用索引来完成查询，MySQL 很有可能寻求通过建立内部的临时表来执行查询。
      - 如果查询中使用到了内部的临时表，在执行计划的 `Extra` 列将显示 `Using temporary` 提示

参考文档

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P136 ~ 138

## 锁的种类

- 对数据的操作类型划分
  - 读锁/共享锁
    - S 锁 `SELECT ... LOCK IN SHARE MODE;`
    - X 锁 `SELECT ... FOR UPDATE;`
  - 写锁/排他锁
    - `DELETE`
    - `UPDATE`
- 锁粒度角度划分
  - 表级锁
    - 表级别的 S 锁、X 锁
    - 意向锁
    - 自增锁
    - MDL 锁
  - 行级锁
    - Record Locks
    - Gap Locks
    - Next-Key Locks
    - 插入意向锁
  - 页级锁
- 对待锁的态度划分
  - 悲观锁
  - 乐观锁
- 加锁方式
  - 隐式锁
  - 显式锁
- 其他
  - 全局锁
  - 死锁

参考文档：

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P174

## 表锁

- **1、表级别的 S 锁、X 锁**

  - MyISAM 在执行查询语句（SELECT）前，会给涉及的所有表加读锁，在执行增删改操作前，会给涉及的表加写锁。InnoDB 存储引擎是不会为这个表添加表级别的**读锁**或者**写锁**的。
    - 一般情况下，不会使用 InnoDB 存储引擎提供的表级别的 **S 锁**和 **X 锁**。只会在一些特殊情况下，比方说**崩溃恢复**过程中用到。比如，在系统变量 `autocommit=0, innodb_table_locks = 1` 时，**手动**获取 InnoDB 存储引擎提供的表 t 的 **S 锁**或者 **X 锁**可以这么写：
      - `LOCK TABLES t READ`：InnoDB 存储引擎会对表 `t` 加表级别的 **S 锁**
      - `LOCK TABLES t WRITE`：InnoDB 存储引擎会对表 `t` 加表级别的 **X 锁**
    - 不过尽量避免在使用 InnoDB 存储引擎的表上使用 `LOCK TABLES` 这样的手动锁表语句，它们并不会提供什么额外的保护，只是会降低并发能力而已。InnoDB 的厉害之处还是实现了更细粒度的**行锁**，关于 InnoDB 表级别的 **S 锁**和 **X 锁**大家了解一下就可以了。

- **2、意向锁（Intention lock）**

  - InnoDB 支持**多粒度锁（multiple granularity locking）**, 它允许**行级锁**与**表级锁**共存，而**意向锁**就是其中的一种**表锁**。

    1. 意向锁的存在是为了协调行锁和表锁的关系，支持多粒度（表锁与行锁）的锁并存。
    2. 意向锁是一种**不与行级锁冲突表级锁**，这一点非常重要。
    3. 表明“某个事务正在某些行持有了锁或该事务准备去持有锁”

  - 意向锁分为两种：

    - **意向共享锁**（intention shared lock，IS）：事务有意向对表中的某些行加**共享锁**（S 锁）

      ```mysql
      -- 事务要获取某些行的 S 锁，必须先获得表的 IS 锁。
      SELECT column FROM table ... LOCK IN SHARE MODE;
      ```

    - **意向排他锁**（intention exclusive lock，IX）：事务有意向对表中的某些行加**排他锁**（X 锁）

      ```mysql
      -- 事务要获取某些行的 X 锁，必须先获得表的 IX 锁
      SELECT column FROM table ... FOR UPDATE;
      ```

  - 首先，我们需要知道意向锁之间的兼容互斥性，如下所示。

    |                  | 意向共享锁（IS） | 意向排他锁（IX） |
    | ---------------- | ---------------- | ---------------- |
    | 意向共享锁（IS） | 兼容             | 兼容             |
    | 意向排他锁（IX） | 兼容             | 兼容             |

    即意向锁之间是互相兼容的，虽然意向锁和自家兄弟互相兼容，但是它会与普通的排他/共享锁互斥。

    |              | 意向共享锁（IS） | 意向排他锁（IX） |
    | ------------ | ---------------- | ---------------- |
    | 共享锁（IS） | 兼容             | 互斥             |
    | 排他锁（IX） | 互斥             | 互斥             |

    注意这里的排他/共享锁指的都是表锁，**意向锁不会与行级的共享/排他锁互斥**。

  - 并发性

    - 意向锁不会与行级的共享/排他锁互斥！正因为如此，意向锁并不会影响到多个事务对不同数据行加排他锁的并发性。（不然我们直接用普通的表锁就行了）

- **3、自增锁（AUTO-INC 锁）**

  - 在使用 MySQL 过程中，我们可以为表的某个列添加 `AUTO_INCREMENT` 属性；由于这个表的 id 字段声明了 AUTO_INCREMENT，意味着在书写插入语句时不需要为其赋值

  - 所有插入数据的方式总共分为三类，分别是“`Simple inserts`”，“`Bulk inserts`” 和 “`Mixed-mode inserts`”。

    1. **“Simple inserts”（简单插入）**

       可以**预先确定要插入和行数**（当语句被初始处理时）的语句。包括没有嵌套子查询的单行和多行 `INSERT ... VALUES()` 和 `REPLACE` 语句。比如我们上面举的例子就属于该类插入，已经确定要插入的行数。

    2. **“Bulk inserts”（批量插入）**

       **事先不知道要插入的行数**（和所需自动递增的数量）的语句。比如 `INSERT ... SELECT`, `REPLACE ... SELECT` 和 `LOAD DATA` 语句，但不包括纯 INSERT。InnoDB 在每处理一行，为 AUTO_INCREMENT 列分配一个新值。

    3. **“Mixed-mode inserts”（混合模式插入）**

       这些是“Simple inserts”语句但是指定部分新行的自动递增值。例如 `INSERT INTO teacher (id, name) VALUES (1, 'a'), (NULL, 'b'), (5, 'c'), (NULL, 'd');` 只是指定了部分 id 的值。另一种类型的“混合模式插入”是 `INSERT ... ON DUPLICATE KEY UPDATE`。

  - 对于上面数据插入的案例，MySQL 中采用了**自增锁**的方式来实现，**AUTO-INC 锁是当向使用 AUTO_INCREMENT 列的表中插入数据时需要获取的一种特殊的表级锁**，在执行插入语句时就在表级别加一个 AUTO-INC 锁，然后为每条待插入记录的 AUTO_INCREMENT 修饰的列分配递增的值，在该语句执行结束后，再把 AUTO-INC 锁释放掉。

    - **一个事务在持有 AUTO-INC 锁的过程中，其他事务的插入语句都要被阻塞**，可以保证一个语句中分配的递增值是连续的。

    - 也正因为此，其并发性显然并不高，**当我们向一个有 AUTO_INCREMENT 关键字的主键插入值的时候，每条语句都要对这个表锁进行竞争**，这样的并发潜力其实是很低下的，所以 innodb 通过 `innodb_autoinc_lock_mode` 的不同取值来提供不同的锁定机制，来显著提高 SQL 语句的可伸缩性和性能。

      - **（1）innodb_autoinc_lock_mode = 0（“传统”锁定模式）**

        在此锁定模式下，所有类型的 insert 语句都会获得一个特殊的表级 AUTO-INC 锁，用于插入具有 AUTO_INCREMENT 列的表。

      - **（2）innodb_autoinc_lock_mode = 1（“连续”锁定模式）**

        在 MySQL 8.0 之前，连续锁定模式是**默认**的。

        在这个模式下，“bulk inserts”仍然使用 AUTO-INC 表级锁，并保持到语句结束。

        对于“Simple inserts”（要插入的行数事先已知），则通过在 `mutex（轻量锁）`的控制下获得所需数量的自动递增值来避免表级 AUTO-INC 锁，它只在分配过程的持续时间内保持，而不是直到语句完成。不使用表级 AUTO-INC 锁，除非 AUTO-INC 锁由另一个事务保持。如果另一个事务保持 AUTO-INC 锁，则“Simple inserts”等待 AUTO-INC 锁，如同它是一个“bulk inserts”

      - **（3）innodb_autoinc_lock_mode = 2（“交错”锁定模式）**

        从 MySQL 8.0 开始，交错锁模式是**默认**设置。

        在这种锁定模式下，所有类 INSERT 语句都不会使用表级 AUTO-INC 锁，并且可以同时执行多个语句。这是最快和最可扩展的锁定模式，但是当使用基于语句的复制或恢复方案时，**从二进制日志重播 SQL 语句时，这是不安全的**。

- **4、元数据锁（MDL 锁）**

  - MySQL 5.5 引入了 meta data lock，简称 MDL 锁，属于表锁范畴。
  - MDL 的作用是，保证读写的正确性。
    - 比如，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个**表结构做变更**，增加了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。
  - 因此，**当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。**
  - **不需要显式使用**，在访问一个表的时候会被自动加上。

参考文档：

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P175 ~ 176

## 行锁

- 行锁（Row Lock）也称为记录锁，顾名思义，就是锁住某一行（某条记录 row）。需要的注意的是，MySQL 服务器层并没有实现行锁机制，**行级锁只在存储引擎层实现**。
- InnoDB 与 MyISAM 的最大不同有两点：一是支持事务（TRANSACTION）；二是采用了行级锁。
- 分类
  1. 记录锁（Record Locks）
     - 记录锁也就是仅仅把一条记录锁上，官方的类型名称为：`LOCK_REC_NOT_GAP`。比如我们把 id 值为 8 的那条记录加一个记录锁，仅仅是锁住了 id 值为 8 的记录，对周围的数据没有影响。
     - 记录锁是有 S 锁和 X 锁之分的，称之为 **S 型记录锁**和 **X 型记录锁**
  2. 间隙锁（Gap Locks）
     - `MySQL` 在 `REPEATABLE READ` 隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用 `MVCC` 方案解决，也可以采用**加锁**方案解决。（分别对应**快照读**（一致性读）和**当前读**（锁定读）两种情况下的处理方式）
       - 但是在使用加锁方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些**幻影记录**加上**记录锁**。
       - InnoDB 提出了一种称之为 **Gap Locks** 的锁，官方的类型名称为：`LOCK_GAP`，我们可以简称为 **gap 锁**。
     - **gap 锁的提出仅仅是为了防止插入幻影记录而提出的。**虽然有**共享 gap 锁**和**独占 gap 锁**这样的说法，但是它们起到的作用是相同的。而且如果对一条记录加了 gap 锁（不论是共享 gap 锁还是独占 gap 锁），并不是限制其他事务对这条记录加记录锁或者继续加 gap 锁。
  3. 临键锁（Next-Key Locks）
     - 有时候我们既想**锁住某条记录**，又想**阻止**其他事务在该记录前边的**间隙插入新纪录**，所以 InnoDB 就提出了一种称之为 `Next-Key Locks` 的锁，官方的类型名称为：`LOCK_ORDINARY`，我们也可以简称为 **next-key 锁**。
     - Next-Key Locks 是在存储引擎 `innodb`、事务级别在**可重复读**的情况下使用的数据库锁，innodb 默认的锁就是 Next-Key locks。
     - **next-key 锁**的本质就是一个**记录锁**和一个 **gap 锁**的合体，它既能保护该条记录，又能阻止别的事务将新的记录插入被保护记录前边的**间隙**。
  4. 插入意向锁（Insert Intention Locks）
     - 我们说一个事务在**插入**一条记录时需要判断一下插入位置是不是被别的事务加了 **gap 锁**（**next-key 锁**也包含 **gap 锁**），如果有的话，插入操作需要等待，直到拥有 **gap 锁**的那个事务提交。
     - 但是 **InnoDB 规定事务在等待的时候也需要在内存中生成一个锁结构**，表明有事务想在某个**间隙**中**插入**新纪录，但是现在在等待。
     - InnoDB 就把这种类型的锁命名为 `Insert Intention Locks`，官方的类型名称为：`LOCK_INSERT_INTENTION`，我们称为**插入意向锁**。
     - 插入意向锁是一种 **Gap 锁**，不是意向锁，在 insert 操作时产生。
     - 总结来说，插入意向锁的特性可以分成两部分：

       1. 插入意向锁是一种**特殊的间隙锁**——间隙锁可以锁定开区间内的部分记录。
       2. 插入意向锁之间**互不排斥**，所以即使多个事务在同一区间插入多条记录，只要记录本身（主键、唯一索引）不冲突，那么事务之间就不会出现冲突等待。
     - 注意，虽然插入意向锁中含有意向锁三个字，但是它并不属于意向锁而属于间隙锁，因为意向锁是表锁而插入意向锁是**行锁**。

参考文档：

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P177 ~ 178

## 页锁

- 页锁就是在**页的粒度**上进行锁定，锁定的数据资源比行锁要多，因为一个页中可以有多个行记录。
  - 当我们使用页锁的时候，会出现数据浪费的现象，但这样的浪费最多也就是一个页上的数据行。
- **页锁的开销介于表锁和行锁之间，会出现死锁。锁定粒度介于表锁和行锁之间，并发度一般。**
- 每个层级的锁数量是有限制的，因为锁会占用内存空间，**锁空间的大小是有限的**。当某个层级的锁数量超过了这个层级的阈值时，就会进行**锁升级**。
  - 锁升级就是用更大粒度的锁替代多个更小粒度的锁，比如 InnoDB 中行锁升级为表锁，这样做的好处是占用的锁空间降低了，但同时数据的并发度也下降了。
- 目前只有 **BDB 引擎**支持页面锁，应用场景较少。
  - （也就是 InnoDB 没有页锁）

参考文档：

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P179

## 隐式锁

- 一个事务在执行 `INSERT` 操作时，如果即将插入的**间隙**已经被其他事务加了 **gap 锁**，那么本次 `INSERT` 操作会阻塞，并且当前事务会在该间隙上加一个**插入意向锁**，否则一般情况下 `INSERT` 操作是不加锁的。

- 那如果一个事务首先插入了一条记录（此时并没有在内存生产与该记录关联的锁结构），然后另一个事务：

  - 立即使用 `SELECT ... LOCK IN SHARE MODE` 语句读取这条记录，也就是要获取这条记录的 **S 锁**，或者使用 `SELECT ... FOR UPDATE` 语句读取这条记录，也就是要获取这条记录的 **X 锁**，怎么办？

    如果允许这种情况的发生，那么可能产生**脏读**问题。

  - 立即修改这条记录，也就是要获取这条记录的 **X 锁**，怎么办？

  - 如果允许这种情况的发生，那么可能产生**脏写**问题。

- 这时候我们前边提到过的**事务 id** 又要起作用了。我们把聚簇索引和二级索引中的记录分开看一下：

  - **情景一**：对于聚簇索引记录来说，有一个 `trx_id` 隐藏列，该隐藏列记录着最后改动该记录的**事务 id**。那么如果在当前事务中新插入一条聚簇索引记录后，该记录的 `trx_id` 隐藏列代表的就是当前事务的**事务 id**，如果其他事务此时想对该记录添加 **S 锁**或者 **X 锁**时，首先会看一下该记录的 `trx_id` 隐藏列代表的事务是否是当前的活跃事务，如果是的话，那么就帮助当前事务创建一个 **X 锁**（也就是为当前事务创建一个锁结构，`is_waiting` 属性是 `false`），然后自己进入等待状态（也就是为自己也创建一个锁结构，`is_waiting` 属性是 `true`）。
  - **情景二**：对于二级索引记录来说，本身并没有 `trx_id` 隐藏列，但是在二级索引页面的 `Page Header` 部分有一个 `PAGE_MAX_TRX_ID` 属性，该属性代表对该页面做改动的最大的**事务 id**，如果 `PAGE_MAX_TRX_ID` 属性值小于当前最小的活跃**事务 id**，那么说明对该页面做修改的事务都已经提交了，否则就需要在页面中定位到对应的二级索引记录，然后回表找到它对应的聚簇索引记录，然后再重复**情景一**的做法。
  - 即 ：一个事务对新插入的记录可以不显式的加锁（生成一个锁结构），但是由于**事务 id**的存在，相当于加了一个**隐式锁**。别的事务在对这条记录加 **S 锁**或者 **X 锁**时，由于**隐式锁**的存在，会先帮助当前事务生成一个锁结构，然后自己再生成一个锁结构后进入等待状态。隐式锁是一种**延迟加锁**的机制，从而来减少加锁的数量。
  - 隐式锁在实际内存对象中并不含有这个锁信息，只有当产生锁等待时，隐式锁转化为显式锁。

- 隐式锁的逻辑过程如下：

  1. InnoDB 的每条记录中都一个隐含的 trx_id 字段，这个字段存在于聚簇索引的 B+Tree 中。
  2. 在操作一条记录前，首先根据记录中的 trx_id 检查该事务是否是活动的事务（未提交或回滚）。如果是活动的事务，首先将**隐式锁**转换为**显式锁**（就是为该事务添加一个锁）。
  3. 检查是否有锁冲突，如果有冲突，创建锁，并设置为 waiting 状态。如果没有冲突不加锁，跳到 5.
  4. 等待加锁成功，被唤醒，或者超时
  5. 写数据，并将自己的 trx_id 写入 trx_id 字段。

参考文档：

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P180

## 全局锁

全局锁就是对**整个数据库实例**加锁。当你需要让整个库处于**只读状态**的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。全局锁的典型使用**场景**是：做**全库逻辑备份**。

全局锁的命令：

```mysql
Flush tables with read lock
```

参考文档：

1. [尚硅谷《MySQL数据库入门到大牛》笔记.md](../视频笔记/尚硅谷《MySQL数据库入门到大牛》笔记.md) P181

## 防范 SQL 注入

1. JDBC 的 PreparedStatement

   - 采用预编译语句集，它内置了处理SQL注入的能力

2. 使用 MyBatis 的 #

   - \# 将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。

   - $ **将传入的数据直接显示生成在sql中。**

   - MyBatis是如何做到SQL预编译的呢？

     其实在框架底层，是JDBC中的PreparedStatement类在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的SQL语句。

     - 简单说，#{}是经过预编译的，是安全的；
     - ${}是未经过预编译的，仅仅是取变量的值，是非安全的，存在SQL注入。

## 主从复制

- **主从复制原理**
  - (1) Master 将数据改变记录到**二进制日志(binary log)**中，也就是配置文件 log-bin 指定的文件， 这些记录叫做**二进制日志事件(binary log events)**；
  - (2) Slave 通过 I/O 线程读取 Master 中的 binary log events 并写入到它的**中继日志(relay log)**；
  - (3) Slave 重做中继日志中的事件，把中继日志中的事件信息一条一条的在本地执行一次，完 成数据在本地的存储，从而实现将改变反映到它自己的数据(数据重放)。
- **注意事项**
  - (1)主从服务器操作系统版本和位数一致；
  - (2) Master 和 Slave 数据库的版本要一致；
  - (3) Master 和 Slave 数据库中的数据要一致；
  - (4) Master 开启二进制日志，Master 和 Slave 的 server_id 在局域网内必须唯一；
- **配置主从复制步骤**
  - **Master 数据库**
    - (1) 安装数据库；
    - (2) 修改数据库配置文件，指明 server_id，开启二进制日志(log-bin)；
    - (3) 启动数据库，查看当前是哪个日志，position 号是多少；
    - (4) 登录数据库，授权数据复制用户（IP 地址为从机 IP 地址，如果是双向主从，这里的 还需要授权本机的 IP 地址，此时自己的 IP 地址就是从 IP 地址)；
    - (5) 备份数据库（记得加锁和解锁）；
    - (6) 传送备份数据到 Slave 上；
    - (7) 启动数据库；
    - 以上步骤，为单向主从搭建成功，想搭建双向主从需要的步骤：
      - (1) 登录数据库，指定 Master 的地址、用户、密码等信息（此步仅双向主从时需要）；
      - (2) 开启同步，查看状态；
  - **Slave 上的配置**
    - (1) 安装数据库；
    - (2) 修改数据库配置文件，指明 server_id（如果是搭建双向主从的话，也要开启二进制 日志 log bin）；
    - (3) 启动数据库，还原备份；
    - (4) 查看当前是哪个日志，position 号是多少（单向主从此步不需要，双向主从需要）；
    - (5) 指定 Master 的地址、用户、密码等信息；
    - (6) 开启同步，查看状态。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景六：讲讲 MySQL 主从复制的原理和注意事项？

# Redis

## 数据结构

1. redis 字符串（String）
   - String（字符串）
   - string 是 redis 最基本的类型，一个 key 对应一个 value。
   - string 类型是**二进制安全的**，意思是 redis 的 string 可以包含任何数据，比如 jpg 图片或者序列化的对象。
   - string 类型是 Redis 最基本的数据类型，一个 redis 中字符串 value 最多可以是 512M
2. redis 列表（List）
   - Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的**头部（左边）或者尾部（右边）**
   - 它的底层实际是个**双端链表**，最多可以包含 2^32 - 1 个元素（4294967295，每个列表超过 40 亿个元素）
3. redis 哈希表（Hash）
   - Redis hash 是一个 string 类型的 field（字段）和 value（值）的映射表，hash 特别适合用于存储对象。
   - Redis 中每个 hash 可以存储 2^32 - 1 键值对（40 多亿）
4. redis 集合（Set）
   - Redis 的 Set 是 String 类型的**无序集合**。集合成员是唯一的，这就意味着集合中不能出现重复的数据，集合对象的编码可以是 intset 或者 hashtable。
   - Redis 中 Set 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。
   - 集合中最大的成员数为 2^32 - 1（4294967295，每个集合可存储 40 多亿个成员）
5. redis 有序集合（ZSet）
   - zset（sorted set，有序集合）
   - Redis zset 和 set 一样也是 string 类型元素的集合，且不允许重复的成员。
   - **不同的是每个元素都会关联一个 double 类型的分数，**redis 正是通过分数来为集合中的成员进行从小到大的排序。
   - **zset 的成员是唯一的，但分数（score）却可以重复。**
   - **zset 集合是通过哈希表实现的，所以添加、删除，查找的复杂度都是 O(1)。集合中最大的成员数为 2^32 - 1**
6. redis 地理空间（GEO）
   - Redis GEO 主要用于存储地理位置信息，并对存储的信息进行操作，包括：
     - 添加地理位置的坐标
     - 获取地理位置的坐标
     - 计算两个位置之间的距离
     - 根据用户给定的经纬度坐标来获取指定范围内的地理位置集合
7. redis 基数统计（HypeLogLog）
   - HyperLogLog 是用来做**基数统计**的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定且是很小的。
   - 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。
   - 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。
8. redis 位图（bitmap）
   - Bit arrays (or simply bitmaps，我们可以称之为 位图)
   - 一个字节（一个 byte）= 8 位
   - 上图由许许多多的小格子组成，每一个格子里面只能放 1 或者 0，用它来判断 Y/N 状态。说的专业点，每一个个小格子就是一个个 bit
   - 由 0 和 1 状态表现的二进制位的 bit 数组
9. redis 位域（bitfield）
   - 通过 bitfield 命令可以一次性操作多个**比特位域（指的是连续的多个比特位）**，它会执行一系列操作并返回一个响应数组，这个数组中的元素对应参数列表中的相应操作的执行结果。
   - 说白了就是通过 bitfield 命令我们可以一次性对多个比特位域进行操作。
10. redis 流（Stream）
    - Redis Stream 是 Redis 5.0 版本新增加的数据结构。
    - Redis Stream 主要用于消息队列（MQ，Message Queue），Redis 本身是有一个 Redis 发布订阅（pub/sub）来实现消息队列的功能，但它有个缺点就是消息无法持久化，如果出现网络断开、Redis 宕机等，消息就会被丢弃。
    - 简单来说发布订阅（pub/sub）可以分发消息，但无法记录历史消息。
    - 而 Redis Stream 提供了消息的持久化和主备复制功能，可以让任何客户端访问任何时刻的数据，并且能记住每一个客户端的访问位置，还能保证消息不丢失

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P11

## 底层数据结构

- redis 6 类型-物理编码-对应表

  - （1：04）

    | 类型         | 编码                      | 对象                                             |
    | ------------ | ------------------------- | ------------------------------------------------ |
    | REDIS_STRING | REDIS_ENCODING_INT        | 使用整数值实现的字符串对象                       |
    | REDIS_STRING | REDIS_ENCODING_EMBSTR     | 使用 embstr 编码的简单动态字符串实现的字符串对象 |
    | REDIS_STRING | REDIS_ENCODING_RAW        | 使用简单动态字符串实现的字符串对象               |
    | REDIS_LIST   | REDIS_ENCODING_ZIPLIST    | 使用压缩列表实现的列表对象                       |
    | REDIS_LIST   | REDIS_ENCODING_LINKEDLIST | 使用双端链表实现的列表对象                       |
    | REDIS_HASH   | REDIS_ENCODING_ZIPLIST    | 使用压缩列表实现的哈希对象                       |
    | REDIS_HASH   | REDIS_ENCODING_HT         | 使用字典实现的哈希对象                           |
    | REDIS_SET    | REDIS_ENCODING_INTSET     | 使用整数集合实现的集合对象                       |
    | REDIS_SET    | REDIS_ENCODING_HT         | 使用字典实现的集合对象                           |
    | REDIS_ZSET   | REDIS_ENCODING_ZIPLIST    | 使用压缩列表实现的有序集合对象                   |
    | REDIS_ZSET   | REDIS_ENCODING_SKIPLIST   | 使用跳跃表和字典实现的有序集合对象               |

- redis 6 数据类型对应的底层数据结构

  - （1：57）

    1. 字符串

       **int**：8 个字节的长整型

       **embstr**：小于等于 44 个字节的字符串

       **raw**：大于 44 个字节的字符串

       - embstr 与 raw 类型底层的数据结构其实都是 SDS（简单动态字符串，Redis 内部定义 sdshdr 一种结构）。

    2. 哈希

       **ziplist**（压缩列表）：当哈希类型元素小于 hash-max-ziplist-entries 配置（默认 512 个）、同时所有值都小于 hash-max-ziplist-value 配置（默认 64 字节）时，Redis 会使用 ziplist 作为哈希的内部实现，ziplist 使用更加紧凑的结构实现多个元素的连续存储。所以在节省内存方面比 hashtable 更加优秀

       **hashtable**（哈希表）：当哈希类型无法满足 ziplist 的条件时，Redis 会使用 hashtable 作为哈希的内部实现，因为此时 ziplist 的读写效率会下降，而 hashtable 的读写时间复杂度为 O(1)

    3. 列表

       **ziplist**（压缩列表）：当列表的元素个数小于 list-max-ziplist-entries 配置（默认 512 个），同时列表中每个元素的值都小于 list-max-ziplist-value 配置时（默认 64 字节），Redis 会选用 ziplist 来作为列表的内部实现来减少内存的使用。

       **linkedlist**（链表）：当列表类型无法满足 ziplist 的条件时，Redis 会使用 linkedlist 作为列表的内部实现。

       3.2 版本之后只有使用：**quicklist**：ziplist 和 linkedlist 的结合以 ziplist 为节点的链表

    4. 集合

       **intset**（整数集合）：当集合中的元素都是整数且元素个数小于 set-max-intset-entries 配置（默认 512 个）时，Redis 会用 intset 来作为集合的内部实现，从而减少内存的使用。

       **hashtable**（哈希表）：当集合类型无法满足 intset 的条件时，Redis 会使用 hashtable 作为集合的内部实现

    5. 有序集合

       **ziplist**（压缩列表）：当有序集合的元素个数小于 zset-max-ziplist-entries 配置（默认 128 个），同时每个元素的值都小于 zset-max-ziplist-value 配置（默认 64 字节）时，Redis 会用 ziplist 来作为有序集合的内部实现，ziplist 可以有效减少内存的使用

       **skiplist**（跳跃表）：当 ziplist 条件不满足时，有序集合会使用 skiplist 作为内部实现，因为此时 ziplist 的读写效率会下降

- redis 6 数据类型以及数据结构的关系（2：35）

- redis 7 数据类型以及数据结构的关系（2：52）

  - ziplist -> listpack（所有地方，包括 quicklist 内）

  - listpack 避免了 ziplist 的连锁更新的问题

- redis 数据类型以及数据结构的时间复杂度（3：24）

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P163

## 单线程 / 多线程

- Redis 3.x 单线程时代但性能依旧很快的主要原因

  - 基于内存操作：Redis 的所有数据都存在内存中，因此所有的运算都是内存级别的，所以他的性能比较高；
  - 数据结构简单：Redis 的数据结构是专门设计的，而这些简单的数据结构的查找和操作的时间大部分复杂度都是 O(1)，因此性能比较高；
  - 多路复用和非阻塞 I/O：Redis 使用 I/O 多路复用功能来监听多个 socket 连接客户端，这样就可以使用一个线程连接来处理多个请求，减少线程切换带来的开销，同时也避免了 I/O 阻塞操作
  - 避免上下文切换：因为是单线程模型，因此就避免了不必要的上下文切换和多线程竞争，这就省去了多线程切换带来的时间和性能上的消耗，而且单线程不会导致死锁问题的发生

- **Redis 4.0 之前**一直采用单线程的主要原因有以下三个

  1. 使用单线程模型是 Redis 的开发和维护更简单，因为单线程模型方便开发和调试
  2. 即使使用单线程模型也并发的处理多客户端的请求，主要使用的是 IO 多路复用和非阻塞 IO
  3. 对于 Redis 系统来说，**主要的性能瓶颈是内存或者网络带宽而并非 CPU**.

- Redis 是基于内存操作的，**因此他的瓶颈可能是机器的内存或者网络带宽而并非 CPU**，既然 CPU 不是瓶颈，那么自然就采用单线程的解决方案了，况且使用多线程比较麻烦。**但是在 Redis 4.0 中开始支持多线程了，例如后台删除、备份等功能**。

  - RDB bgsave
  - AOF 重写
  - 惰性删除
    - 大 key 问题
    - 在 Redis 4.0 就引入了多个线程来实现数据的异步惰性删除等功能，但是其处理读写请求的仍然只有一个线程，所以仍然算是狭义上的单线程。

- **在 Redis 6/7 中，非常受关注的第一个新特性就是多线程**。

  - 随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 IO 的处理上，也就是说，单个主线程处理网络请求的速度跟不上底层网络硬件的速度

  - 为了应对这个问题：**采用多个 IO 线程来处理网络请求，提高网络请求处理的并行度，Redis 6/7 就是采用的这种方法**。

  - 但是，Redis 的多 IO 线程只是用来处理网络请求的，**对于读写操作命令 Redis 仍然使用单线程来处理**

    - 这是因为，Redis 处理请求时，网络处理经常是瓶颈，通过多个 IO 线程并行处理网络操作，可以提升实例的整体处理性能。而继续使用单线程执行命令操作，就不用为了保证 Lua 脚本、事务的原子性，额外开发多线程**互斥加锁机制了（不管加锁操作处理）**，这样一来，Redis 线程模型实现就简单了。

  - 主线程和 IO 线程四个阶段

    - **阶段一：服务端和客户端建立 Socket 连接，并分配处理线程**

      首先，主线程负责接收建立连接请求。当有客户端请求和实例建立 Socket 连接时，主线程会创建和客户端的连接，并把 Socket 放入全局等待队列中。紧接着，主线程通过轮询方法把 Socket 连接分配给 IO 线程。

    - **阶段二：IO 线程读取并解析请求**

      主线程一旦把 Socket 分配给 IO 线程，就会进入阻塞状态，等待 IO 线程完成客户端请求读取和解析。因为有多个 IO 线程在并行处理。所以，这个过程很快就可以完成。

    - **阶段三：主线程执行请求操作**

      等到 IO 线程解析完请求，主线程还是会以单线程的方式执行这些命令操作。

    - **阶段四：IO 线程回写 Socket 和主线程清空全局队列**

      当主线程执行完请求操作后，会把需要返回的结果写入缓冲区，然后，主线程会阻塞等待 IO 线程，把这些结果回写到 Socket 中，并返回给客户端。和 IO 线程读取和解析请求一样，IO 线程回写 Socket 时，也是有多个线程在并发执行，所以回写 Socket 的速度也很快。等到 IO 线程回写 Socket 完毕，主线程会清空全局队列，等待客户端和后续请求。

- Redis 采用 **Reactor 模式**的网络模型，对于一个客户端请求，主线程负责一个完整的处理过程：

  读取 socket -> 解析请求 -> 执行操作 -> 写入 socket

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P101 ~ 102

## 持久化（RDB、AOF）

- RDB（Redis DataBase）

  - **默认开启**

  - 自动触发

    - Redis 6.0.16 以下在 Redis.conf 配置文件中的 SNAPSHOTTING 下配置 save 参数，来触发 Redis 的 RDB 持久化条件，比如 “save m n”：表示 m 秒内数据集存在 n 次修改时，自动触发 bgsave

      - save 900 1：每隔 900s（15min），如果有超过 1 个 key 发生了变化，就写一份新的 RDB 文件

        save 300 10：每隔 300s（5min），如果有超过 10 个 key 发生了变化，就写一份新的 RDB 文件

        save 60 10000：每隔 60s（1min），如果有超过 10000 个 key 发生了变化，就写一份新的 RDB 文件

    - Redis 7 版本，按照 redis.conf 里配置的 `save <seconds> <changes>`

  - 手动触发

    - save（阻塞） 和 bgsave（默认，fork 子线程来持久化）

  - 如何恢复

    - 将备份文件（dump.rdb）移动到 redis 安装目录并启动服务即可

  - 如何检查修复 dump.rdb 文件

    - `redis-check-rdb /myredis/dumpfiles/dump6379.rdb`

- AOF（Append Only File）

  - **默认不开启**

  - 三种写回策略

    - | 配置项   | 写回时机           | 优点                     | 缺点                             |
      | -------- | ------------------ | ------------------------ | -------------------------------- |
      | Always   | 同步写回           | 可靠性高，数据基本不丢失 | 每个写命令都要落盘，性能影响较大 |
      | Everysec | 每秒写回           | 性能适中                 | 宕机时丢失 1 秒内的数据          |
      | No       | 操作系统控制的写回 | 性能好                   | 宕机时丢失数据较多               |

  - 重写机制

    - AOF 文件重写并不是对原文件进行重新整理，而是直接读取服务器现有的键值对，然后用一条命令去代替之前记录这个键值对的多条命令，生成一个新的文件后去替换原来的 AOF 文件

    - 自动触发

      - auto-aof-rewrite-percentage 100

        auto-aof-rewrite-min-size 64mb

        注意，**同时满足，且的关系**才会触发

    - 手动触发

      - 客户端向服务器发送 bgrewriteaof 命令

  - 异常修复命令：`redis-check-aof --fix` 进行修复

- RDB-AOF 混合持久化

  - 设置 `aof-use-rdb-preamble` 的值为 yes
  - RDB 镜像做全量持久化，AOF 做增量持久化

- 同时关闭 RDB + AOF

  - `save ""`
    - 禁用 rdb
    - 禁用 rdb 持久化模式下，我们仍然可以使用命令 save、bgsave 生成 rdb 文件
  - `appendonly no`
    - 禁用 aof
    - 禁用 aof 持久化模式下，我们仍然可以使用命令 bgrewriteaof 生成 aof 文件

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P28 ~ 46

## keys * 的问题 / 引出 scan

- 大批量往 redis 里面插入 2000W 测试数据 key

  - Linux Bash 下面执行，插入 100W

    - ```bash
      # 生成 100W 条 redis 批量设置 kv 的语句（key=kn, value=vn）写入到 /tmp 目录下的 redisTest.txt 文件中
      for((i=1;i<=100*10000;i++)); do echo "set k$i v$i" >> /tmp/redisTest.txt ;done;
      ```

  - 通过 redis 提高的管道 --pipe 命令插入 100W 大批量数据

    - 结合自己机器的地址

      ```shell
      cat /tmp/redisTest.txt | /opt/redis-7.0.0/src/redis-cli -h 127.0.0.1 -p 6379 -a 111111 --pipe
      redis-cli -a 111111 dbsize
      ```

      100w 数据插入 redis 花费 5.8 秒左右

- 某快递巨头真实生产案例新闻

  - 新闻

    - 对 Redis 稍微有点使用经验的人都知道线上是不能执行 keys * 相关命令的，虽然其模糊匹配功能使用非常方便也很强大，在小数据量情况下使用没什么问题，数据量大会导致 Redis 锁住及 CPU 飙升，在生产环境建议禁用或者重命名！

  - keys * 你试试 100W 花费多少秒遍历查询

    - keys * （34.74s） flushdb (2.16s)

    - keys * 这个指令有致命的弊端，在实际环境中最好不要使用

      > 这个指令没有 offset、limit 参数，是要一次性吐出所有满足条件的 key，由于 redis 是单线程的，其所有操作都是原子的，而 keys 算法是遍历算法，复杂度是 O(n)，如果实例中有千万级以上的 key，这个指令就会导致 Redis 服务卡顿，所有读写 Redis 的其它的指令都会被延后甚至会超时报错，可能会引起缓存雪崩甚至数据库宕机

  - 生产上限制 keys */flushdb/flushall 等危险命令以防止误删误用？

    - 通过配置设置禁用这些命令，redis.conf 在 SECURITY 这一项中

      - ```
        rename-command keys ""
        rename-command flushdb ""
        rename-command flushall ""
        ```

- 不用 keys * 避免卡顿，那该用什么

  - scan 命令登场

    - 一句话，类似 mysql limit 的**但不完全相同**

  - Scan 命令用于迭代数据库中的数据库键

    - 语法

      - SCAN cursor [MATCH pattern] [COUNT count]

        基于游标的迭代器，需要基于上一次的游标延续之前的迭代过程

        以 0 作为游标开始一次新的迭代，直到命令返回游标 0 完成一次遍历

        **不保证每次执行都返回某个给定数量的元素，支持模糊查询**

        一次返回的数量不可控，只能是大概率符合 count 参数

    - 特点

      - redis Scan 命令基本语法如下：

        SCAN cursor [MATCH pattern] [COUNT count]

        - cursor - 游标
        - pattern - 匹配的模式
        - count - 指定从数据集里返回多少元素，默认值为 10

        SCAN 命令是一个基于游标的迭代器，每次被调用之后，都会向用户返回一个新的游标，**用户在下次迭代时需要使用这个新游标作为 SCAN 命令的游标参数**，以此来延续之前的迭代过程。

        SCAN 返回一个包含**两个元素的数组**，第一个元素是用于进行下一次迭代的新游标，第二个元素则是一个数组，这个数组中包含了所有被迭代的元素。**如果新游标返回零表示迭代已结束。**

        SCAN 的遍历顺序**非常特别，它不是从第一维数组的第零位一直遍历到末尾，而是采用了高位进位加法来遍历。之所以使用这样特殊的方式进行遍历，是考虑到字典的扩容和缩容时避免槽位的遍历重复和遗漏**

    - 使用

      - ```
        127.0.0.1:6379> keys *
        1) "db_number"
        2) "key1"
        3) "myKey"
        127.0.0.1:6379> scan 0 MATCH * COUNT 1
        1) "2"
        2) 1) "db_number"
        127.0.0.1:6379> scan 2 MATCH * COUNT 1
        1) "1"
        2) 1) "myKey"
        127.0.0.1:6379> scan 1 MATCH * COUNT 1
        1) "3"
        2) 1) "key1"
        127.0.0.1:6379> scan 3 MATCH * COUNT 1
        1) "0"
        2) (empty list or set)
        ```

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P105

## Big Key 问题

- 多大算 Big

  - 参考《阿里云 Redis 开发规范》

    - 【强制】：拒绝 bigkey（防止网卡流量、慢查询）

      **string 类型控制在 10KB 以内，hash、list、set、zset 元素个数不要超过 5000**.

      反例：一个包含 200 万个元素的 list

      **非字符串的 bigkey，不要使用 del 删除，使用 hscan、sscan、zscan 方式渐进式删除，同时要注意防止 bigkey 过期时间自动删除问题**（例如一个 200 万的 zset 设置 1 小时过期，会触发 del 操作，造成阻塞，而且该操作不会出现在慢查询中（lantency 可查））

  - string 和二级结构

    - string 是 value，最大 512MB 但是 >= 10KB 就是 bigkey
    - list、hash、set 和 zset，个数超过 5000 就是 bigkey
      - 疑问？？？
        - list
          - 一个列表最多可以包含 2^32-1 个元素（4294967295，每个列表超过 40 亿个元素）。
        - hash
          - Redis 中每个 hash 可以存储 2^32 - 1 键值对（40 多亿）
        - set
          - 集合中最大的成员数为 2^32 - 1（4294967295，每个集合可存储 40 多亿个成员）。
        - ...

- 哪些危害

  - 内存不均，集群迁移困难
  - 超时删除，大 key 删除作梗
  - 网络流量阻塞

- 如何产生

  - 社交类
    - 王心凌粉丝列表，典型案例粉丝逐步递增
  - 汇总统计
    - 某个报表，月日年经年累月的积累

- 如何发现

  - **redis-cli --bigkeys**

    - **好处，见最下面总结**

      给出**每种数据结构 Top 1 bigkey**，同时给出**每种数据类型的键值个数 + 平均大小**

      **不足**

      想查询大于 10kb 的所有 key，--bigkeys 参数就无能为力了，需要用到 memory usage 来计算每个键值的字节数

      redis-cli --bigkeys -a 111111

      redis-cli -h 127.0.0.1 -p 6379 -a 111111 -bigkeys

      每隔 100 条 scan 指令就会休眠 0.1s，ops 就不会剧烈抬升，但是扫描的时间会变长

      redis-cli -h 127.0.0.1 -p 7001 --bigkeys -i 0.1

  - **MEMORY USAGE 键**

    - 计算每个键值的字节数
    - 官网

- 如何删除

  - 参考《阿里云 Redis 开发规范》

    - 【强制】：拒绝 bigkey（防止网卡流量、慢查询）

      **string 类型控制在 10KB 以内，hash、list、set、zset 元素个数不要超过 5000.**

      反例：一个包含 200 万个元素的 list

      **非字符串的 bigkey，不要使用 del 删除，使用 hscan、sscan、zscan 方式渐进式删除，同时要注意防止 bigkey 过期时间自动删除问题**（例如一个 200 万的 zset 设置 1 小时过期，会触发 del 操作，造成阻塞，而且该操作不会出现在慢查询中（lantency 可查））

  - 官网

  - 普通命令

    - String

      - 一般用 del，如果过于庞大 unlink

    - hash

      - 使用 hscan 每次获取少量 field-value，再使用 hdel 删除每个 field

      - 命令

        - Redis HSCAN 命令会用于迭代哈希表中的键值对

          **语法**

          redis HSCAN 命令基本语法如下：

          HSCAN key cursor [MATCH pattern] [COUNT count]

          - cursor - 游标
          - pattern - 匹配的模式
          - count - 指定从数据集里返回多少元素，默认值为 10

          **可用版本**

          大于等于 2.8.0

          **返回值**

          返回的每个元素都是一个元组，每一个元组元素由一个字段（field）和值（value）组成

      - 阿里手册

        - 1、Hash 删除：hsan + hdel

          ```java
          public void delBigHash(String host, int port, String password, String bigHashKey) {
              Jedis jedis = new Jedis(host, port);
              if (password != null && !"".equals(password)) {
                  jedis.auth(password);
              }
              ScanParams scanParams = new ScanParams().count(100);
              String cursor = "0";
              do {
                  ScanResult<Entry<String, String>> scanResult = jedis.hscan(bigHashKey, cursor, scanParams);
                  List<Entry<String, String>> entryList = scanResult.getResult();
                  if (entryList != null && !entryList.isEmpty()) {
                      for (Entry<String, String> entry : entryList) {
                          jedis.hdel(bigHashKey, entry.getKey());
                      }
                  }
                  cursor = scanResult.getStringCursor();
              } while (!"0".equals(cursor));
              
              // 删除 bigkey
              jedis.del(bigHashKey);
          }
          ```

    - list

      - 使用 ltrim 渐进式逐步删除，直到全部删除完成

      - 命令

        - Redis Ltrim 对一个列表进行修剪（trim），就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除。

          下标 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。你也可以使用负数下标，以 -1 表示列表的最后一个元素，-2 表示列表的倒数第二个元素，以此类推。

          **语法**

          redis Ltrim 命令基本语法如下：

          LTRIM KEY_NAME START STOP

          **可用版本**

          大于等于 1.0.0

          **返回值**

          命令执行成功时，返回 ok

      - 阿里手册

        - 2、List 删除：ltrim

          ```java
          public void delBigList(String host, int port, String password, String bigListKey) {
              Jedis jedis = new Jedis(host, port);
              if (password != null && !"".equals(password)) {
                  jedis.auth(password);
              }
              long llen =  jedis.llen(bigListKey);
              int counter = 0;
              int left = 100;
              while (counter < llen) {
                  // 每次从左侧截掉 100 个
                  jedis.ltrim(bigListKey, left, llen);
                  counter += left;
              }
              // 最终删除 key
              jedis.del(bigListKey);
          }
          ```

    - set

      - 使用 sscan 每次获取部分元素，再使用 srem 命令删除每个元素

      - 命令

      - 阿里手册

        - 3、Set 删除：sscan + srem

          ```java
          public void delBigSet(String host, int port, String password, String bigSetKey) {
              Jedis jedis = new Jedis(host, port);
              if (password != null && !"".equals(password)) {
                  jedis.auth(password);
              }
              ScanParams scanParams = new ScanParams().count(100);
              String cursor = "0";
              do {
                  ScanResult<String> scanResult = jedis.sscan(bigSetKey, cursor, scanParams);
                  List<String> memberList = scanResult.getResult();
                  if (memberList != null && !memberList.isEmpty()) {
                      for (String member : memberList) {
                          jedis.srem(bigSetKey, member);
                      }
                  }
                  cursor = scanResult.getStringCursor();
              } while (!"0".equals(cursor));
              
              // 删除 bigkey
              jedis.del(bigHashKey);
          }
          ```

    - zset

      - 使用 zscan 每次获取部分元素，再使用 ZREMRANGEBYRANK 命令删除每个元素

      - 命令

      - 阿里手册

        - ```java
          public void delBigZset(String host, int port, String password, String bigZsetKey) {
              Jedis jedis = new Jedis(host, port);
              if (password != null && !"".equals(password)) {
                  jedis.auth(password);
              }
              ScanParams scanParams = new ScanParams().count(100);
              String cursor = "0";
              do {
                  ScanResult<Tuple> scanResult = jedis.zscan(bigZsetKey, cursor, scanParams);
                  List<Tuple> tupleList = scanResult.getResult();
                  if (tupleList != null && !tupleList.isEmpty()) {
                      for (Tuple tuple : tupleList) {
                          jedis.srem(bigZsetKey, tuple.getElement());
                      }
                  }
                  cursor = scanResult.getStringCursor();
              } while (!"0".equals(cursor));
              
              // 删除 bigkey
              jedis.del(bigHashKey);
          }
          ```

BigKey 生产调优

- redis.conf 配置文件 LAZY FREEING 相关说明

  - 阻塞和非阻塞删除命令

    - Redis 有两个原语来删除键。一种称为 **DEL，是对象的阻塞删除**。这意味着服务器停止处理新命令，以便以同步方式回收与对象关联的所有内存。如果删除的键与一个小对象相关联的所有内存。如果删除的键与一个小对象相关联，则执行 DEL 命令所需的时间非常短，可与大多数其它命令相媲美

      Redis 中的 O(1) 或 O(log_N) 命令。但是，如果键与包含数百万个元素的聚合值相关联，则服务器可能会阻塞很长时间（甚至几秒钟）才能完成操作。

      基于上述原因，Redis 还提供了非阻塞删除原语，例如 **UNLINK（非阻塞 DEL）**以及 **FLUSHALL 和 FLUSHDB 命令的 ASYNC 选项**，以便在后台回收内存。这些命令在恒定时间内执行。另一个线程将尽可能快地逐步释放后台中的对象。

      FLUSHALL 和 FLUSHDB 的 DEL、UNLINK 和 ASYNC 选项是用户控制的。这取决于应用程序的设计，以了解何时使用其中一个是个好主意。然而，作为其它操作的副作用，Redis 服务器有时不得不删除键或刷新整个数据库。具体而言，Redis 在以下场景中独立于用户调用删除对象

  - 优化配置

    - ```
      lazyfree-lazy-server-del no 改为 Yes
      replica-lazy-flush no 改为 Yes
      
      lazyfree-lazy-user-del no 改为 Yes
      ```

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P106

## 缓存双写一致性

- 反馈回来的面试题

  - 一图

    - 通用查询方法三部曲

      1 OK，直接从 redis 获得并返回

      2 next，redis 无，从 mysql 获得并返回

      3 完成第 2 步同时，讲 mysql 数据回写 redis，两边数据一致

      问题，上面业务逻辑你用 Java 代码如何写

  - 你只要用缓存，就可能会涉及到 redis 缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？

  - 双写一致性，你先动缓存 redis 还是数据库 mysql 哪一个？why？

  - **延时双删**你做过吗？会有哪些问题？

  - 有这么一种情况 ，微服务查询 redis 无 mysql 有，为保证数据双写一致性回写 redis 你需要注意什么？**双检加锁**策略你了解过吗？如何尽量避免缓存击穿？

  - redis 和 mysql 双写 100% 会出纰漏，做不到强一致性，你如何保证**最终一致性**？

  - ……

- 缓存双写一致性，谈谈你的理解

  - 如果 redis 中**有数据**

    - 需要和数据库中的值相同

  - 如果 redis 中**无数据**

    - 数据库中的值要是最新值，且准备回写 redis

  - 缓存按照操作来分，细分 2 种

    - **只读缓存**
    - 读写缓存
      - **同步直写策略**
        - 写数据库后也同步写 redis 缓存，缓存和数据库中的数据一致；
        - 对于读写缓存来说，要想保证缓存和数据库中的数据一致，就要采用同步直写策略
      - **异步缓写策略**
        - 正常业务运行中，mysql 数据变动了，但是可以在业务上容许出现一定时间后才作用于 redis，比如仓库、物流系统
        - 异常情况出现了，不得不将失败的动作重新修补，有可能需要借助 kafka 或者 RabbitMQ 等消息中间件，实现重试重写

  - 一图代码你如何写

    - 问题》》》？

    - 采用双检加锁策略

      - ```java
        public String get(String key) {
            String value = redis.get(key); // 查询缓存
            if (value != null) {
                // 缓存存在直接返回
                return value;
            } else {
                // 缓存不存在则对方法加锁
                // 假设请求量很大，缓存过期
                synchronized (TestFuture.class) {
                    value = redis.get(key); // 再查一遍 redis
                    if (value != null) {
                        // 查到数据直接返回
                        return value;
                    } else {
                        // 二次查询缓存也不存在，直接查 DB
                        value = dao.get(key);
                        // 数据缓存
                        redis.setnx(key, value, time);
                        // 返回
                        return value;
                    }
                }
            }
        }
        ```

    - Code

- 数据库和缓存一致性的几种更新策略

  - 目的

    - 总之，我们要达到最终一致性！

      - **给缓存设置过期时间，定期清理缓存并回写，是保证最终一致性的解决方案。**

        我们可以对存入缓存的数据设置过期时间，所有的**写操作以数据库为准**，对缓存操作只是尽最大努力即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存，达到一致性，**切记，要以 mysql 的数据库写入库为准**。

        上述方案和后续落地案例是调研后的主流 + 成熟的做法，但是考虑到各个公司业务系统的差距，**不是 100% 绝对正确，不保证绝对适配全部情况，**请同学们自行酌情选择打法，合适自己的最好。

  - 可以停机的情况

    - 挂牌报错，凌晨升级，温馨提示，服务降级
    - 单线程，这样重量级的数据操作最好不要多线程

  - 我们讨论 4 种更新策略

    - X 先更新数据库，再更新缓存

      - 异常问题 1

        1. 先更新 mysql 的某商品的库存，当前商品的库存是 100，更新为 99 个
        2. 先更新 mysql 修改为 99 成功，然后更新 redis
        3. **此时假设异常出现**，更新 redis 失败了，这导致 mysql 里面的库存是 99 而 redis 里面的还是 100.
        4. 上述发生，会让数据库里面和缓存 redis 里面数据不一致，**读到 redis 脏数据**

      - 异常问题 2

        - 【先更新数据库，再更新缓存】，A、B 两个线程发起调用

          【正常逻辑】

          1. A update mysql 100
          2. A update redis 100
          3. B update mysql 80
          4. B update redis 80

          ============

          【异常逻辑】多线程环境下，A、B 两个线程有快有慢，有前有后有并行

          1. A update mysql 100
          2. B update mysql 80
          3. B update redis 80
          4. A update redis 100

          ============

          最终结果，mysql 和 redis 数据不一致，mysql 80，redis 100

    - X 先更新缓存，再更新数据库

      - X 不太推荐

        - 业务上一般把 mysql 作为**底单数据库**，保证最后解释

      - 异常问题 2

        - 【先更新数据库，再更新缓存】，A、B 两个线程发起调用

          【正常逻辑】

          1. A update redis 100
          2. A update mysql 100
          3. B update redis 80
          4. B update mysql 80

          ============

          【异常逻辑】多线程环境下，A、B 两个线程有快有慢，有前有后有并行

          1. A update redis 100
          2. B update redis 80
          3. B update mysql 80
          4. A update mysql 100

          ============

          --- mysql 100，redis 80

    - X 先删除缓存，再更新数据库

      - 异常问题

        - 步骤分析 1，先删除缓存，再更新数据库

          - （10:40）

            阳哥自己这里写 20 秒，是自己乱写的，表示更新数据库可能失败，实际中不可能，哈哈~

            1 A 线程先成功删除了 redis 里面的数据，然后去更新 mysql，此时 mysql 正在更新中，还没有结束。（比如网络延时）**B 突然出现要来读取缓存数据**。

        - 步骤分析 2，先删除缓存，再更新数据库（13:25）

        - 步骤分析 3，先删除缓存，再更新数据库（14:03）

        - 上面 3 步骤串讲梳理（16:30）

      - 解决方案

        - 采用**延时双删策略**（20:39）

        - 双删方案面试题

          - 这个删除该休眠多久呢？

            - （22:14）

              **第一种方法：**加百毫秒即可

              **第二种方法：**新启动一个后台监控程序

          - 这种同步淘汰策略，吞吐量降低怎么办？

            - （23:38）

              ```java
              CompletableFuture.supplyAsync(() -> {
                  
              }).whenComplete((t, u) -> {
                  
              }).exceptionally(e -> {
                  
              }).get();
              ```

          - 后续看门狗 WatchDog 源码分析

    - **先更新数据库，再删除缓存**

      - 异常问题（27:59）

      - 业务指导思想

        - 微软云
        - 我们后面的阿里巴巴 Canal 也是类似的思想
          - 上述的订阅 binlog 程序在 mysql 中有现成的中间件叫 canal，可以完成订阅 binlog 日志的功能。

      - 解决方案

        - （33:43）

          暂存到消息队列中

          没成功删除，从消息队列中重新读取

          成功删除，从消息队列中去除

          重试超过一定次数，向业务层发送报错信息

      - 类似经典的分布式事务问题，只有一个权威答案

        - 最终一致性
          - 流量充值，先下发短信实际充值可能滞后 5 分钟，可以接受
          - 电商发货，短信下发但是物流明天见

  - 小总结

    - 如何选择方案？利弊如何（36:56）
    - 一图总结（39:11）


参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P107 ~ 109

## 缓存预热、雪崩、穿透、击穿

- 缓存预热（5:37）

  - @PostConstruct 初始化白名单数据

- 缓存雪崩

  - 发生
    - redis 主机挂了，Redis 全盘崩溃，偏硬件运维
    - redis 中有大量 key 同时过期大面积失效，偏软件开发
  - 预防 + 解决
    - redis 中 key 设置为永不过期 or 过期时间错开
    - redis 缓存集群实现高可用
      - 主从 + 哨兵
      - Redis Cluster
      - 开启 Redis 持久化机制 aof/rdb，尽快恢复缓存集群
    - 多缓存结合预防雪崩
      - ehcache 本地缓存 + redis 缓存
    - 服务降级
      - Hystrix 或者阿里 sentinel 限流 & 降级（10:03）
    - **人民币玩家**
      - 阿里云-云数据库 Redis 版

- 缓存穿透

  - 是什么
    - 请求去查询一条记录，先查 redis 无，后查 mysql 无，**都查询不到该条记录，**但是请求每次都会打到数据库上去，导致后台数据库压力暴增，这种现象我们称为缓存穿透，这个 redis 变成了一个摆设……
    - 简单说就是本来无一物，两库都没有。既不在 Redis 缓存库，也不在 mysql，数据库存在被多次暴击风险
  - 解决
    - 缓存穿透 | 恶意攻击 | 空对象缓存、bloomfilter 过滤器
    - 一图（16:11）
    - 方案1：空对象缓存或者缺省值
      - 一般 OK（17:48）
      - But
        - 黑客或者恶意攻击
          - 黑客会对你的系统进行攻击，拿一个不存在的 id 去查询数据，会产生大量的请求到数据库去查询。可能会导致你的数据库由于压力过大而宕掉
          - key 相同打你系统
            - 第一次打到 mysql，空对象缓存后第二次就返回 defaultNull 缺省值，避免 mysql 被攻击，不用再到数据库中去走一圈了
          - **key 不同打你系统**
            - 由于存在空对象缓存和缓存回写（看自己业务不限死），redis 中的无关紧要的 key 也会越写越多**（记得设置 redis 过期时间）**
    - 方案2：Google 布隆过滤器 Guava 解决缓存穿透

- 缓存击穿

  - 是什么
    - 大量的请求同时查询一个 key 时，此时这个 key 正好失效了，就会导致大量的请求都打到数据库上面去
    - **简单说就是热点 key 突然失效了，暴打 mysql**
    - 备注
      - 穿透和击穿，截然不同
  - 危害
    - 会造成某一时刻数据库请求量过大，压力剧增
    - 一般技术部门需要知道**热点 key 是那些个**？做到心里有数防止击穿
  - 解决
    - 缓存击穿 | 热点 key 失效 | 互斥更新、随机退避、差异失效时间
    - 热点 key 失效
      - 时间到了自然清除但还没被访问到
      - delete 掉的 key，刚巧又被访问
    - 方案1：差异失效时间，对于访问频繁的热点 key，干脆就不设置过期时间
    - **方案2：互斥更新，采用双检加锁策略**（8:20）

- | 缓存问题     | 产生原因               | 解决方案                               |
  | ------------ | ---------------------- | -------------------------------------- |
  | 缓存更新方式 | 数据变更、缓存时效性   | 同步更新、失效更新、异步更新、定时更新 |
  | 缓存不一致   | 同步更新失败、异步更新 | 增加重试、补偿任务、最终一致           |
  | 缓存穿透     | 恶意攻击               | 空对象缓存、bloomfilter 过滤器         |
  | 缓存击穿     | 热点 key 失效          | 互斥更新、随机退避、差异失效时间       |
  | 缓存雪崩     | 缓存挂掉               | 快速失败熔断、主从模式、集群模式       |

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P124 ~ 127

## 布隆过滤器

- 原理

  - BloomFilter 的算法是，首先分配一块内存空间做 bit 数组，数组的 bit 位初始值全部设为 0。
  - 加入元素时，采用 k 个相互独立的 Hash 函数计算，然后将元素 Hash 映射的 K 个位置全部设置为 1。
  - 检测 key 是否存在，仍然用这 k 个 Hash 函数计算出 k 个位置，如果位置全部为 1，则表明 key 存在，否则不存在。

- Redis 实现

  - bitmaps

    - 基本命令

      - | 命令                        | 作用                                                  | 时间复杂度 |
        | --------------------------- | ----------------------------------------------------- | ---------- |
        | setbit key offset val       | 给指定 key 的值的第 offset 赋值 val                   | O(1)       |
        | getbit key offset           | 获取指定 key 的第 offset 位                           | O(1)       |
        | bitcount key start end      | 返回指定 key 中 [start, end] 中为 1 的数量            | O(n)       |
        | bitop operation destkey key | 对不同的二进制存储数据进行位运算（AND、OR、NOT、XOR） | O(n)       |

  - Redisson

    - Redis 实现布隆过滤器的底层就是通过 bitmap 这种数据结构，至于如何实现，这里就不重复造轮子了，介绍业界比较好用的一个客户端工具——Redisson。

    - ```java
      package com.ys.rediscluster.bloomfilter.redisson;
      
      import org.redisson.Redisson;
      import org.redisson.api.RBloomFilter;
      import org.redisson.api.RedissonClient;
      import org.redisson.config.Config;
      
      public class RedissonBloomFilter {
      
          public static void main(String[] args) {
              Config config = new Config();
              config.useSingleServer().setAddress("redis://192.168.14.104:6379");
              config.useSingleServer().setPassword("123");
              //构造Redisson
              RedissonClient redisson = Redisson.create(config);
      
              RBloomFilter<String> bloomFilter = redisson.getBloomFilter("phoneList");
              //初始化布隆过滤器：预计元素为100000000L,误差率为3%
              bloomFilter.tryInit(100000000L,0.03);
              //将号码10086插入到布隆过滤器中
              bloomFilter.add("10086");
      
              //判断下面号码是否在布隆过滤器中
              System.out.println(bloomFilter.contains("123456"));//false
              System.out.println(bloomFilter.contains("10086"));//true
          }
      }
      ```

    - 这是单节点的Redis实现方式，如果数据量比较大，期望的误差率又很低，那单节点所提供的内存是无法满足的，这时候可以使用分布式布隆过滤器，同样也可以用 Redisson 来实现

- 非 Redis 实现

  - Guava

    - ```java
      package com.ys.rediscluster.bloomfilter;
      
      import com.google.common.base.Charsets;
      import com.google.common.hash.BloomFilter;
      import com.google.common.hash.Funnel;
      import com.google.common.hash.Funnels;
      
      public class GuavaBloomFilter {
          public static void main(String[] args) {
              BloomFilter<String> bloomFilter = BloomFilter.create(Funnels.stringFunnel(Charsets.UTF_8),100000,0.01);
      
              bloomFilter.put("10086");
      
              System.out.println(bloomFilter.mightContain("123456"));
              System.out.println(bloomFilter.mightContain("10086"));
          }
      }
      ```

参考文档：

1. [Redis详解（十三）------ Redis布隆过滤器 -  博客园](https://www.cnblogs.com/ysocean/p/12594982.html)

## 过期删除策略

- 三种不同的删除策略：
  1. **立即删除**。在设置键的过期时间时，创建一个回调事件，当过期时间达到时，由时间处理器自动执行键的删除操作。
     - 立即删除能保证内存中数据的最大新鲜度，因为它保证过期键值会在过期后马上被删除，其所占用的内存也会随之释放。但是立即删除对cpu是最不友好的。因为删除操作会占用cpu的时间，如果刚好碰上了cpu很忙的时候，比如正在做交集或排序等计算的时候，就会给cpu造成额外的压力。
     - 而且目前redis事件处理器对时间事件的处理方式–无序链表，查找一个key的时间复杂度为O(n),所以并不适合用来处理大量的时间事件。
  2. **惰性删除**。键过期了就过期了，不管。每次从dict字典中按key取值时，先检查此key是否已经过期，如果过期了就删除它，并返回nil，如果没过期，就返回键值。
     - 惰性删除是指，某个键值过期后，此键值不会马上被删除，而是等到下次被使用的时候，才会被检查到过期，此时才能得到删除。所以惰性删除的缺点很明显:浪费内存。dict字典和expires字典都要保存这个键值的信息。
     - 举个例子，对于一些按时间点来更新的数据，比如log日志，过期后在很长的一段时间内可能都得不到访问，这样在这段时间内就要拜拜浪费这么多内存来存log。这对于性能非常依赖于内存大小的redis来说，是比较致命的。
  3. **定时删除**。每隔一段时间，对expires字典进行检查，删除里面的过期键。
     可以看到，第二种为被动删除，第一种和第三种为主动删除，且第一种实时性更高。
     - 从上面分析来看，立即删除会短时间内占用大量cpu，惰性删除会在一段时间内浪费内存，所以定时删除是一个折中的办法。
     - 定时删除是：每隔一段时间执行一次删除操作，并通过限制删除操作执行的时长和频率，来减少删除操作对cpu的影响。另一方面定时删除也有效的减少了因惰性删除带来的内存浪费。
- Redis 使用的策略
  - redis使用的过期键值删除策略是：**惰性删除 + 定期删除**，两者配合使用。

参考文档

1. [Redis过期键的删除策略 - CSDN](https://blog.csdn.net/ThinkWon/article/details/101522970)

## 内存淘汰策略

- redis 缓存淘汰策略

  - redis 配置文件（0：14）
  - lru 和 lfu 算法的区别是什么（1：30）
  - 有哪些（redis 7 版本）
    1. **noeviction**：不会驱逐任何 key，表示即使内存达到上限也不进行置换，所有能引起内存增加的命令都会返回 error
    2. **allkeys-lru**：对所有 key 使用 LRU 算法进行删除，优先删除掉最近最不经常使用的 key，用以保存新数据
    3. **volatile-lru**：对所有设置了过期时间的 key 使用 LRU 算法进行删除
    4. **allkeys-random**：对所有 key 随机删除
    5. **volatile-random**：对所有设置了过期时间的 key 随机删除
    6. **volatile-ttl**：删除马上要过期的 key
    7. **allkeys-lfu**：对所有 key 使用 LFU 算法进行删除
    8. **volatile-lfu**：对所有设置了过期时间的 key 使用 LFU 算法进行删除
  - 上面总结
    - 2 * 4 = 8
    - 2 个维度
      - 过期键中筛选，volatile
      - 所有键中筛选，allkeys
    - 4 个方面
      - LRU
      - LFU
      - random
      - ttl
    - 8 个选项
  - 你平时用哪一种（8:33）
  - 如何配置、修改
    - 直接使用 config 命令
    - 直接 redis.conf 配置文件

- redis 缓存淘汰策略配置性能建议
  - 避免存储 bigkey
  - 开启惰性淘汰，lazyfree-lazy-eviction=yes

参考文档：

1. [《尚硅谷Redis零基础到进阶》笔记.md](../视频笔记/《尚硅谷Redis零基础到进阶》笔记.md) P144

## 分布式锁（SETNX、Lua、Redisson、RedLock）

一把靠谱的分布式锁应该有哪些特征：

- **「互斥性」**: 任意时刻，只有一个客户端能持有锁。
- **「锁超时释放」**：持有锁超时，可以释放，防止不必要的资源浪费，也可以防止死锁。
- **「可重入性」**:一个线程如果获取了锁之后,可以再次对其请求加锁。
- **「高性能和高可用」**：加锁和解锁需要开销尽可能低，同时也要保证高可用，避免分布式锁失效。
- **「安全性」**：锁只能被持有的客户端删除，不能被其他客户端删除

方案：

- 方案一：SETNX + EXPIRE
  - 缺点
    - 但是这个方案中，`setnx`和`expire`两个命令分开了，**「不是原子操作」**。如果执行完`setnx`加锁，正要执行`expire`设置过期时间时，进程crash或者要重启维护了，那么这个锁就“长生不老”了，**「别的线程永远获取不到锁啦」**。
- 方案二：SETNX + value值是（系统时间+过期时间）
  - 缺点
    - 过期时间是客户端自己生成的（System.currentTimeMillis()是当前系统的时间），必须要求分布式环境下，每个客户端的时间必须同步。
    - 如果锁过期的时候，并发多个客户端同时请求过来，都执行 jedis.getSet()，最终只能有一个客户端加锁成功，但是该客户端锁的过期时间，可能被别的客户端覆盖
    - 该锁没有保存持有者的唯一标识，可能被别的客户端释放/解锁。
- 方案三：使用Lua脚本(包含SETNX + EXPIRE两条指令)
- 方案四：SET的扩展命令（SET EX PX NX）
  - 缺点
    - 问题一：**「锁过期释放了，业务还没执行完」**。假设线程a获取锁成功，一直在执行临界区的代码。但是100s过去后，它还没执行完。但是，这时候锁已经过期了，此时线程b又请求过来。显然线程b就可以获得锁成功，也开始执行临界区的代码。那么问题就来了，临界区的业务代码都不是严格串行执行的啦。
    - 问题二：**「锁被别的线程误删」**。假设线程a执行完后，去释放锁。但是它不知道当前的锁可能是线程b持有的（线程a去释放锁时，有可能过期时间已经到了，此时线程b进来占有了锁）。那线程a就把线程b的锁释放掉了，但是线程b临界区业务代码可能都还没执行完呢。
- 方案五：SET EX PX NX  + 校验唯一随机值,再释放锁
  - 缺点：
    - 方案五还是可能存在**「锁过期释放，业务没执行完」**的问题。
- 方案六: 开源框架~Redisson
  - 只要线程一加锁成功，就会启动一个`watch dog`看门狗，它是一个后台线程，会每隔10秒检查一下，如果线程1还持有锁，那么就会不断的延长锁key的生存时间。
  - 使用Redisson解决了**「锁过期释放，业务没执行完」**问题。
- 方案七：多机实现的分布式锁Redlock
  - 搞多个Redis master部署，以保证它们不会同时宕掉。并且这些master节点是完全相互独立的，相互之间不存在数据同步。同时，需要确保在这多个master实例上，是与在Redis单实例，使用相同方法来获取和释放锁。

参考文档：

1. [Redis实现分布式锁的7种方案，及正确使用姿势！ - 博客园](https://www.cnblogs.com/wangyingshuo/p/14510524.html)

## 部署方式（单机、主从复制、哨兵、集群）

- 单机模式
  - 优点
    - 架构简单，部署方便；
    - 高性价比：缓存使用时无需备用节点（单实例可用性可以用supervisor或crontab保证），当然为了满足业务的高可用性，也可以牺牲一个备用节点，但同时刻只有一个实例对外提供服务；
    - 高性能。
  - 缺点
    - 不保证数据的可靠性；
    - 在缓存使用，进程重启后，数据丢失，即使有备用的节点解决高可用性，但是仍然不能解决缓存预热问题，因此不适用于数据可靠性要求高的业务；
    - 高性能受限于单核CPU的处理能力（Redis是单线程机制），CPU为主要瓶颈，所以适合操作命令简单，排序、计算较少的场景。也可以考虑用Memcached替代。
- 主从模式（复制）
  - 优点
    - 高可靠性：一方面，采用双机主备架构，能够在主库出现故障时自动进行主备切换，从库提升为主库提供服务，保证服务平稳运行；另一方面，开启数据持久化功能和配置合理的备份策略，能有效的解决数据误操作和数据异常丢失的问题；
    - 读写分离策略：从节点可以扩展主库节点的读能力，有效应对大并发量的读操作。
  - 缺点
    - 故障恢复复杂，如果没有RedisHA系统（需要开发），当主库节点出现故障时，需要手动将一个从节点晋升为主节点，同时需要通知业务方变更配置，并且需要让其它从库节点去复制新主库节点，整个过程需要人为干预，比较繁琐；
    - 主库的写能力受到单机的限制，可以考虑分片；
    - 主库的存储能力受到单机的限制，可以考虑Pika；
    - 原生复制的弊端在早期的版本中也会比较突出，如：Redis复制中断后，Slave会发起psync，此时如果同步不成功，则会进行全量同步，主库执行全量备份的同时可能会造成毫秒或秒级的卡顿；又由于COW机制，导致极端情况下的主库内存溢出，程序异常退出或宕机；主库节点生成备份文件导致服务器磁盘IO和CPU（压缩）资源消耗；发送数GB大小的备份文件导致服务器出口带宽暴增，阻塞请求，建议升级到最新版本。
- 哨兵模式
  - 优点
    - Redis Sentinel集群部署简单；
    - 能够解决Redis主从模式下的高可用切换问题；
    - 很方便实现Redis数据节点的线形扩展，轻松突破Redis自身单线程瓶颈，可极大满足Redis大容量或高性能的业务需求；
    - 可以实现一套Sentinel监控一组Redis数据节点或多组数据节点。
  - 缺点
    - 部署相对Redis主从模式要复杂一些，原理理解更繁琐；
    - 资源浪费，Redis数据节点中slave节点作为备份节点不提供服务；
    - Redis Sentinel主要是针对Redis数据节点中的主节点的高可用切换，对Redis的数据节点做失败判定分为主观下线和客观下线两种，对于Redis的从节点有对节点做主观下线操作，并不执行故障转移。
    - 不能解决读写分离问题，实现起来相对复杂。
- 集群模式
  - 优点：
    - 无中心架构；
    - 数据按照slot存储分布在多个节点，节点间数据共享，可动态调整数据分布；
    - 可扩展性：可线性扩展到1000多个节点，节点可动态添加或删除；
    - 高可用性：部分节点不可用时，集群仍可用。通过增加Slave做standby数据副本，能够实现故障自动 failover，节点之间通过gossip协议交换状态信息，用投票机制完成Slave到Master的角色提升；
    - 降低运维成本，提高系统的扩展性和可用性。
  - 缺点：
    - Client实现复杂，驱动要求实现Smart Client，缓存slots mapping信息并及时更新，提高了开发难度，客户端的不成熟影响业务的稳定性。目前仅JedisCluster相对成熟，异常处理部分还不完善，比如常见的“max redirect exception”。
    - 节点会因为某些原因发生阻塞（阻塞时间大于clutser-node-timeout），被判断下线，这种failover是没有必要的。
    - 数据通过异步复制，不保证数据的强一致性。
    - 多个业务使用同一套集群时，无法根据统计区分冷热数据，资源隔离性较差，容易出现相互影响的情况。
    - Slave在集群中充当“冷备”，不能缓解读压力，当然可以通过SDK的合理设计来提高Slave资源的利用率。
    - Key批量操作限制，如使用mset、mget目前只支持具有相同slot值的Key执行批量操作。对于映射为不同slot值的Key由于Keys不支持跨slot查询，所以执行mset、mget、sunion等操作支持不友好。
    - Key事务操作支持有限，只支持多key在同一节点上的事务操作，当多个Key分布于不同的节点上时无法使用事务功能。
    - Key作为数据分区的最小粒度，不能将一个很大的键值对象如hash、list等映射到不同的节点。
      不支持多数据库空间，单机下的redis可以支持到16个数据库，集群模式下只能使用1个数据库空间，即db 0。
    - 复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构。
    - 避免产生hot-key，导致主库节点成为系统的短板。
    - 避免产生big-key，导致网卡撑爆、慢查询等。
    - 重试时间应该大于cluster-node-time时间。
    - Redis Cluster不建议使用pipeline和multi-keys操作，减少max redirect产生的场景。

参考文档：

1. [Redis的几种部署方式及持久化策略 - CSDN](https://blog.csdn.net/zwdwinter/article/details/88636397)

## 脑裂问题

- 哨兵+主从

- 原主服务器接受到客户端的信息后，还未同步到从服务器上就是去连接了，但是重启后又由主变为了从服务器，无法同步数据了，所以这部分数据就丢失了，这就是脑裂问题

- 解决办法

  - 在配置文件中添加如下配置

    ```
    min-slaves-to-write 1
    min-slaves-max-lag 10
    ```

  - 这种方法**不可能百分百避免数据丢失**

参考文档

1. [Redis常见问题——脑裂问题 - CSDN](https://blog.csdn.net/cxy_t/article/details/110825175)

## 分布式一致性协议（Raft、Gossip）

- Raft
  - Raft是一种为分布式系统设计的共识算法，用于管理复制日志的一致性。
  - 在Redis的Sentinel模式下，Raft协议被用于选举一个Leader Sentinel来执行自动故障转移操作。
  - 在Raft协议中，节点有三种状态
    - Leader（领导）
    - Follower（跟随者）
    - Candidate（候选人）。
  - 系统的时间被划分为一个个的Term（任期），每个Term开始时，所有节点都成为Follower。
  - 如果一个Follower在一段时间内没有收到来自Leader的心跳信息，它将转变为Candidate状态并发起选举。其他节点收到选举请求后，将比较自己的Term和请求中的Term，如果请求中的Term更大，则认可该Candidate为Leader。
- Gossip
  - 在Redis Cluster模式下，Gossip协议被用于实现无中心式的节点通信。
  - Gossip协议是一种最终一致性算法，它不要求节点知道所有其他节点的状态，因此具有去中心化的特点。
    - 节点之间通过交换信息来维护系统状态的一致性，虽然无法保证在某个时刻所有节点状态一致，但可以保证在“最终”所有节点一致。
  - 在实际应用中，Redis Cluster通过Gossip协议实现了节点间的动态信息交换，包括节点状态、槽位信息等。
    - 这种去中心化的通信方式使得Redis Cluster具有很高的可扩展性和容错性。
    - 当一个节点出现故障时，其他节点可以通过Gossip协议感知到这一变化，并自动调整集群状态，保证服务的可用性。
- 这两种协议如何协同工作
  - 在Redis的分布式环境中，Sentinel和Cluster模式可以同时使用。
    - Sentinel负责监控主从节点的健康状况，并在必要时进行故障转移。
    - 而Cluster则负责数据的分片存储和负载均衡。
  - 当Sentinel检测到主节点故障时，它会通过Raft协议选举出一个Leader Sentinel来执行故障转移操作。
  - 在这个过程中，Cluster中的节点通过Gossip协议保持通信，确保集群状态的一致性。

参考文档：

1. [Redis中的分布式一致性协议：Raft与Gossip的协同工作](https://cloud.baidu.com/article/3193422)

## 虚拟内存

- 应用场景

  - 在Redis中，有一个非常重要的概念，即keys一般不会被交换，所以如果你的数据库中有大量的keys，其中每个key仅仅关联很小的value，那么这种场景就不是非常适合使用虚拟内存。
  - 如果恰恰相反，**数据库中只是包含少量的keys，而每一个key所关联的value却非常大**，那么这种场景对于使用虚拟内存就非常合适了。
  - 在实际的应用中，为了能让虚拟内存更为充分的发挥作用以帮助我们提高系统的运行效率，我们可以将带有很多较小值的Keys合并为带有少量较大值的Keys。其中最主要的方法就是将原有的Key/Value模式改为基于Hash的模式，这样可以让很多原来的Keys成为Hash中的属性。

- 配置 Redis 虚拟内存

  - （1）在配置文件中添加以下配置项，以使当前Redis服务器在启动时**打开虚拟内存功能**。

    ```
    vm-enabled yes
    ```

  - （2）**在配置文件中设定Redis最大可用的虚拟内存字节数**。如果内存中的数据大于该值，则有部分对象被持久化到磁盘中，其中被持久化对象所占用的内存将被释放，直到已用内存小于该值时才停止持久化。

    ```
    vm-max-memory (bytes)
    ```

    Redis的交换规则是尽量考虑"最老"的数据，即最长时间没有使用的数据将被持久化。如果两个对象的 age 相同，那么Value较大的数据将先被持久化。需要注意的是，Redis不会将Keys持久化到磁盘，因此如果仅仅keys的数据就已经填满了整个虚拟内存，那么这种数据模型将不适合使用虚拟内存机制，或者是将该值设置的更大，以容纳整个Keys的数据。在实际的应用，如果考虑使用Redis虚拟内存，我们应尽可能的分配更多的内存交给Redis使用，以避免频繁的将数据持久化到磁盘上。

  - （3）**在配置文件中设定页的数量及每一页所占用的字节数**。为了将内存中的数据传送到磁盘上，我们需要使用交换文件。这些文件与数据持久性无关，Redis会在退出前会将它们全部删除。由于对交换文件的访问方式大多为随机访问，因此建议将交换文件存储在固态磁盘上，这样可以大大提高系统的运行效率。

    ```
    vm-pages 134217728
    vm-page-size 32
    ```

    在上面的配置中，Redis将需要持久化的文件划分为vm-pages个页，其中每个页所占用的字节为vm-page-size，那么Redis最终可用的交换文件大小为：vm-pages * vm-page-size。由于一个value可以存放在一个或多个页上，但是一个页不能持有多个value，鉴于此，我们在设置vm-page-size时需要充分考虑Redis的该特征。

  - （4）在Redis的配置文件中有一个非常重要的配置参数，即：

    ```
    vm-max-threads 4
    ```

    该参数表示Redis在**对交换文件执行IO操作时所应用的最大线程数量**。通常而言，我们推荐该值等于主机的CPU cores。如果将该值设置为0，那么Redis在与交换文件进行IO交互时，将以同步的方式执行此操作。

- **Redis 同步数据方式**

  - 对于Redis而言，如果操作交换文件是以同步的方式进行，那么当某一客户端正在访问交换文件中的数据时，其它客户端如果再试图访问交换文件中的数据，该客户端的请求就将被挂起，直到之前的操作结束为止。特别是在相对较慢或较忙的磁盘上读取较大的数据值时，这种阻塞所带来的影响就更为突兀了。
  - 然而同步操作也并非一无是处，事实上，从全局执行效率视角来看，同步方式要好于异步方式，毕竟同步方式节省了线程切换、线程间同步，以及线程拉起等操作产生的额外开销。特别是当大部分频繁使用的数据都可以直接从主内存中读取时，同步方式的表现将更为优异。
  - 至于最终选用哪种配置方式，最好的方式是不断的实验和调优。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景七：讲讲 Redis 的虚拟内存？

# Kafka

## 高可用

- Kafka 一个最基本的架构认识
  - 由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。
- 这就是**天然的分布式消息队列**，就是说一个 topic 的数据，是**分散放在多个机器上的，每个机器就放一部分数据**。
- 实际上 RabbmitMQ 之类的，并不是分布式消息队列，它就是传统的消息队列，只不过提供了一些集群、HA(High Availability, 高可用性) 的机制而已，因为无论怎么玩儿，RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。
- Kafka 0.8 以前，是没有 HA 机制的，就是任何一个 broker 宕机了，那个 broker 上的 partition 就废了，没法写也没法读，没有什么高可用性可言。
  - 比如说，我们假设创建了一个 topic，指定其 partition 数量是 3 个，分别在三台机器上。但是，如果第二台机器宕机了，会导致这个 topic 的 1/3 的数据就丢了，因此这个是做不到高可用的。
- Kafka 0.8 以后，提供了 HA 机制，就是 replica（复制品） 副本机制。
  - 每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。
  - 所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。
  - 写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader？很简单，**要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题**，系统复杂度太高，很容易出问题。Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。
- 这么搞，就有所谓的**高可用性**了，因为如果某个 broker 宕机了，没事儿，那个 broker上面的 partition 在其他机器上都有副本的。如果这个宕机的 broker 上面有某个 partition 的 leader，那么此时会从 follower 中**重新选举**一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了
  - **写数据**的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为）
  - **消费**的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - Kafka 的高可用

## 不重复消费（幂等性）

其实还是得结合业务来思考，我这里给几个思路：

- 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。
- 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。
- 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。
- 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - 如何保障消息不重复消费（幂等性）？

## 可靠传输（不丢失）

- **消费者丢数据**
  - 唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边**自动提交了 offset**，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。
  - 这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要**关闭自动提交** offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是**可能会有重复消费**，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。
  - 生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。
- **Kafka 丢数据**
  - 这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。
  - 生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。
  - 所以此时一般是要求起码设置如下 4 个参数：
    - 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。
    - 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。
    - 在 producer 端设置 acks=all ：这个是要求每条数据，必须是**写入所有 replica 之后，才能认为是写成功了**。
    - 在 producer 端设置 retries=MAX （很大很大很大的一个值，无限次重试的意思）：这个是**要求一旦写入失败，就无限重试**，卡在这里了。
  - 我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。
- **生产者丢数据**
  - 如果按照上述的思路设置了 acks=all ，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - Kafka 如何保障消息的可靠

## 顺序性

- 先看看顺序会错乱的场景：
  - **Kafka**：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。 消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。
  - 接着，我们在消费者里可能会搞**多个线程来并发处理消息**。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。
- **Kafka 解决方案**
  - 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。
  - 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - 如何保证消息的顺序性？ - Kafka 解决方案

## 消息堆积

- **大量消息在 mq 里积压了几个小时了还没解决**
  - 一个消费者一秒是 1000 条，一秒 3 个消费者是 3000 条，一分钟就是 18 万条。所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概 1 小时的时间才能恢复过来。
  - 一般这个时候，只能临时紧急扩容了，具体操作步骤和思路如下：
    - 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。
    - 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。
    - 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，**消费之后不做耗时的处理**，直接均匀轮询写入临时建立好的 10 倍数量的 queue。
    - 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。
    - 等快速消费完积压数据之后，**得恢复原先部署的架构**，**重新**用原先的 consumer 机器来消费消息。
- **mq 中的消息过期失效了**
  - 假设你用的是 RabbitMQ，RabbtiMQ 是可以设置过期时间的，也就是 TTL。如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在 mq 里，而是**大量的数据会直接搞丢**。
  - 这个情况下，就不是说要增加 consumer 消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是**批量重导**，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来。也只能是这样了。
  - 假设 1 万个订单积压在 mq 里面，没有处理，其中 1000 个订单都丢了，你只能手动写程序把那 1000 个订单给查出来，手动发到 mq 里去再补一次。
- **mq 都快写满了**
  - 如果消息积压在 mq 里，你很长时间都没有处理掉，此时导致 mq 都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，**消费一个丢弃一个，都不要了**，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - 如何处理消息堆积？

# RabbitMQ

## 高可用

- RabbitMQ的高可用是**基于主从**（非分布式）做高可用性。RabbitMQ 有三种模式：**单机模式（Demo 级别）、普通集群模式（无高可用性）、镜像集群模式（高可用性）**。
- **普通集群模式**
  - 普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你**创建的queue，只会放在一个 RabbitMQ 实例上**，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。
  - 这种方式确实很麻烦，也不怎么好，**没做到所谓的分布式**，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有**数据拉取的开销**，后者导致**单实例性能瓶颈**。
  - 而且如果那个放 queue 的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你**开启了消息持久化**，让 RabbitMQ 落地存储消息的话，**消息不一定会丢**，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。
  - 所以这个事儿就比较尴尬了，这就**没有什么所谓的高可用性**，**这方案主要是提高吞吐量的**，就是说让集群中多个节点来服务某个 queue 的读写操作。
- **镜像集群模式**
  - 这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会**存在于多个实例上**，就是说，每个 RabbitMQ 节点都有这个 queue 的一个**完整镜像**，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把**消息同步**到多个实例的 queue 上。
  - 那么**如何开启这个镜像集群模式**呢？其实很简单，RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是**镜像集群模式的策略**，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。
  - 好处
    - 你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。
  - 坏处
    - 第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！
    - 第二，这么玩儿，不是分布式的，就**没有扩展性可言**了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并**没有办法线性扩展**你的 queue。你想，如果这个 queue 的数据量很大，大到这个机器上的容量无法容纳了，此时该怎么办呢？

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - RabbitMQ 的高可用

## 可靠传输（不丢失）

- **生产者丢数据**
  - 生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。此时可以选择用 RabbitMQ 提供的事务功能，就是生产者**发送数据之前**开启 RabbitMQ 事务 channel.txSelect ，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务 channel.txRollback ，然后重试发送消息；如果收到了消息，那么可以提交事务 channel.txCommit 。
    - 但是问题是，RabbitMQ 事务机制（同步）一搞，基本上**吞吐量会下来，因为太耗性能**。
  - 所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。
  - 事务机制和 confirm 机制最大的不同在于，**事务机制是同步的**，你提交一个事务之后会**阻塞**在那儿，但是 confirm 机制是**异步**的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。
  - 所以一般在生产者这块**避免数据丢失**，都是用 confirm 机制的。
- **RabbitMQ 丢数据**
  - 就是 RabbitMQ 自己弄丢了数据，这个你必须**开启** **RabbitMQ** **的持久化**，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，**恢复之后会自动读取之前存储的数据**，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，**可能导致少量数据丢失**，但是这个概率较小。
  - 设置持久化有**两个步骤**：
    - 创建 queue 的时候将其设置为持久化 这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。
    - 第二个是发送消息的时候将消息的 deliveryMode 设置为 2 就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。
  - 必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。
  - 注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。
  - 所以，持久化可以跟生产者那边的 confirm 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 ack 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 ack ，你也是可以自己重发的。
- **消费者丢数据**
  - RabbitMQ 如果丢失了数据，主要是因为你消费的时候，**刚消费到，还没处理，结果进程挂了**，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。
  - 这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack ，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - RabbitMQ 如何保障消息的可靠

## 顺序性

- 先看看顺序会错乱的场景：
  - **RabbitMQ**：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。
- 解决方案
  - 拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二：说说消息队列的高可用、不重复消费、可靠传输、顺序消费、消息堆积？ - 如何保证消息的顺序性？ - Kafka 解决方案

# Nginx

## 负载均衡种类

- 根据负载均衡所作用在 OSI 模型的位置不同，负载均衡可以大概分为以下几类：
  - **二层负载均衡（mac）**
    - 根据 OSI 模型分的二层负载，一般是用虚拟 MAC 地址方式，外部对虚拟 MAC 地址请求，负载均衡接收后分配后端实际的 MAC 地址响应。
  - **三层负载均衡（ip）**
    - 一般采用虚拟 IP 地址方式，外部对虚拟的 IP 地址请求，负载均衡接收后分配后端实际的 IP 地址响应。
  - **四层负载均衡（tcp）**
    - 在三层负载均衡的基础上，用 ip+port 接收请求，再转发到对应的机器。
    - 四层负载均衡（基于IP+端口的负载均衡）
      - 所谓四层负载均衡，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。
      - 对应的负载均衡器称为四层交换机（L4 switch），主要分析 IP 层及 TCP/UDP 层，实现四层负载均衡。此种负载均衡器不理解应用协议（如 HTTP/FTP/MySQL 等等）。要处理的流量进行 NAT 处理，转发至后台服务器，并记录下这个 TCP 或者 UDP 的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。
      - 实现四层负载均衡的软件有：
        - F5：硬件负载均衡器，功能很好，但是成本很高
        - lvs：重量级的四层负载软件
        - nginx：轻量级的四层负载软件，带缓存功能，正则表达式较灵活
        - haproxy：模拟四层转发，较灵活
  - **七层负载均衡（http）**
    - 根据虚拟的 url 或 IP，主机名接收请求，再转向相应的处理服务器。
    - 七层的负载均衡（基于虚拟的 URL 或主机 IP 的负载均衡)
      - 所谓七层负载均衡，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。
      - 对应的负载均衡器称为七层交换机（L7 switch），除了支持四层负载均衡以外，还有分析应用层的信息，如 HTTP 协议 URI 或 Cookie 信息，实现七层负载均衡。此种负载均衡器能理解应用协议。
      - 实现七层负载均衡的软件有：
        - haproxy：天生负载均衡技能，全面支持七层代理，会话保持，标记，路径转移
        - nginx：只在 http 协议和 mail 协议上功能比较好，性能与 haproxy 差不多
        - apache：功能较差
        - Mysql proxy：功能尚可
- **在实际应用中，比较常见的就是四层负载及七层负载**。这里也重点说下这两种负载。

参考文档：

1. [负载均衡总结（四层负载与七层负载的区别）（转）- 知乎](https://zhuanlan.zhihu.com/p/64777456)

## 负载均衡算法

1. **轮询（默认）**

   - 每个请求按时间顺序逐一分配到不同的后端服务，如果后端某台服务器死机，自动剔除故障系统，使用户访问不受影响。

   - ```nginx
     upstream bakend {  
         server 192.168.0.1;    
         server 192.168.0.2;  
     }
     ```

2. **weight（轮询权值）**

   - weight的值越大分配到的访问概率越高，主要用于后端每台服务器性能不均衡的情况下。或者仅仅为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源。

   - 指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。

   - ```nginx
     upstream bakend {  
         server 192.168.0.1 weight=10;  
         server 192.168.0.2 weight=10;  
     }
     ```

3. **ip_hash**

   - 每个请求按访问IP的哈希结果分配，使来自同一个IP的访客固定访问一台后端服务器，并且可以有效解决动态网页存在的session共享问题。

   - 每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。

   - ```nginx
     upstream bakend {  
         ip_hash;  
         server 192.168.0.1:88;  
         server 192.168.0.2:80;  
     } 
     ```

4. **fair**（第三方）

   - 比 weight、ip_hash更加智能的负载均衡算法，fair 算法可以根据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间 来分配请求，响应时间短的优先分配。Nginx本身不支持fair，如果需要这种调度算法，则必须安装 upstream_fair 模块。

   - 按后端服务器的响应时间来分配请求，响应时间短的优先分配。

   - ```nginx
     upstream backend {  
         server 192.168.0.1:88;  
         server 192.168.0.2:80;  
         fair;  
     }
     ```

5. **url_hash**（第三方）

   - 按访问的URL的哈希结果来分配请求，使每个URL定向到一台后端服务器，可以进一步提高后端缓存服务器的效率。Nginx本身不支持url_hash，如果需要这种调度算法，则必须安装Nginx的hash软件包。

   - 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。

     > 注意：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法。

   - ```nginx
     upstream backend {  
         server 192.168.0.1:88;  
         server 192.168.0.2:80;  
         hash $request_uri;  
         hash_method crc32;  
     }
     ```

在Nginx upstream模块中，可以设定每台后端服务器在负载均衡调度中的状态，常用的状态有：

1. down，表示当前的server暂时不参与负载均衡
2. weight 默认为1，weight越大，负载的权重就越大。
3. backup，预留的备份机器。当其他所有的非backup机器出现故障或者忙的时候，才会请求backup机器，因此这台机器的访问压力最低
4. max_fails，允许请求失败的次数，默认为1，当超过最大次数时，返回proxy_next_upstream模块定义的错误。
5. fail_timeout，请求失败超时时间，在经历了max_fails次失败后，暂停服务的时间。max_fails和fail_timeout可以一起使用。

例如：

```nginx
upstream bakend{ 
      ip_hash; 
      server 192.168.0.1:90 down; 
      server 192.168.0.1:80 weight=2; 
      server 192.168.0.2:90; 
      server 192.168.0.2:80 backup; 
}
```

参考文档：

1. [nginx负载均衡的五种算法 - CSDN](https://blog.csdn.net/apple9005/article/details/79961391)

## 实现四层负载均衡

- 负载均衡可以分为**静态负载均衡**和**动态负载均衡**，接下来，我们就一起来分析下Nginx如何实现四层静态负载均衡和四层动态负载均衡。

- 静态负载均衡

  - Nginx的四层静态负载均衡需要启用ngx_stream_core_module模块，默认情况下，ngx_stream_core_module是没有启用的，需要在安装Nginx时，添加--with-stream配置参数启用，如下所示。

    - ```shell
      ./configure --prefix=/usr/local/nginx-1.17.2 --with-openssl=/usr/local/src/openssl-1.0.2s --with-pcre=/usr/local/src/pcre-8.43 --with-zlib=/usr/local/src/zlib-1.2.11 --with-http_realip_module --with-http_stub_status_module --with-http_ssl_module --with-http_flv_module --with-http_gzip_static_module --with-cc-opt=-O3 --with-stream --with-http_ssl_module
      ```

  - 配置四层负载均衡

    - 配置HTTP负载均衡时，都是配置在http指令下，配置四层负载均衡，则是在stream指令下，结构如下所示.

    - ```nginx
      stream {
          upstream mysql_backend {
          	......
          }
          server {
          	......
          }
      }
      ```

  - 配置 upstream

    - ```nginx
      upstream mysql_backend {
          server 192.168.175.201:3306 max_fails=2 fail_timeout=10s weight=1;
          server 192.168.175.202:3306 max_fails=2 fail_timeout=10s weight=1;
          least_conn;
      }
      ```

  - 配置 server

    - ```nginx
      server {
          #监听端口，默认使用的是tcp协议，如果需要UDP协议，则配置成listen 3307 udp;
          listen 3307;
          #失败重试
          proxy_next_upstream on;
          proxy_next_upstream_timeout 0;
          proxy_next_upstream_tries 0;
          #超时配置
          #配置与上游服务器连接超时时间，默认60s
          proxy_connect_timeout 1s;
          #配置与客户端上游服务器连接的两次成功读/写操作的超时时间，如果超时，将自动断开连接
          #即连接存活时间，通过它可以释放不活跃的连接，默认10分钟
          proxy_timeout 1m;
          #限速配置
          #从客户端读数据的速率，单位为每秒字节数，默认为0，不限速
          proxy_upload_rate 0;
          #从上游服务器读数据的速率，单位为每秒字节数，默认为0，不限速
          proxy_download_rate 0;
          #上游服务器
          proxy_pass mysql_backend;
      }
      ```

  - 配置完之后，就可以连接Nginx的3307端口，访问数据库了。

- 动态负载均衡

  - 配置Nginx四层静态负载均衡后，重启Nginx时，Worker进程一直不退出，会报错，如下所示。

    - `nginx: worker process is shutting down;`
    - 这是因为Worker进程维持的长连接一直在使用，所以无法退出，只能杀掉进程。可以使用Nginx的四层动态负载均衡解决这个问题。

  - 使用Nginx的四层动态负载均衡有两种方案：使用商业版的Nginx和使用开源的nginx-stream-upsync-module模块。

    - 注意：四层动态负载均衡可以使用nginx-stream-upsync-module模块，七层动态负载均衡可以使用nginx-upsync-module模块。

  - 使用如下命令为Nginx添加nginx-stream-upsync-module模块和nginx-upsync-module模块，此时，Nginx会同时支持四层动态负载均衡和HTTP七层动态负载均衡。

    - ```shell
      git clone https://github.com/xiaokai-wang/nginx-stream-upsync-module.git
      git clone https://github.com/weibocom/nginx-upsync-module.git
      git clone https://github.com/CallMeFoxie/nginx-upsync.git
      cp -r nginx-stream-upsync-module/* nginx-upsync/nginx-stream-upsync-module/
      cp -r nginx-upsync-module/* nginx-upsync/nginx-upsync-module/
      
      ./configure --prefix=/usr/local/nginx-1.17.2 --with-openssl=/usr/local/src/openssl-1.0.2s --with-pcre=/usr/local/src/pcre-8.43 --
      with-zlib=/usr/local/src/zlib-1.2.11 --with-http_realip_module --with-http_stub_status_module --with-http_ssl_module --with-http_flv_module --with-http_gzip_static_module --with-cc-opt=-O3 --with-stream --add-module=/usr/local/src/nginx-upsync --with-http_ssl_module
      ```

  - 配置四层负载均衡

    - 配置HTTP负载均衡时，都是配置在http指令下，配置四层负载均衡，则是在stream指令下，结构如下所示，

    - ```nginx
      stream {
          upstream mysql_backend {
          	......
          }
          server {
          	......
          }
      }
      ```

  - 配置 upstream

    - ```nginx
      upstream mysql_backend {
          server 127.0.0.1:1111; #占位server
          upsync 192.168.175.100:8500/v1/kv/upstreams/mysql_backend upsync_timeout=6m
          upsync_interval=500ms upsync_type=consul strong_dependency=off;
          upsync_dump_path /usr/local/nginx-1.17.2/conf/mysql_backend.conf;
      }
      ```

    - upsync指令指定从consul哪个路径拉取上游服务器配置；

    - upsync_timeout配置从consul拉取上游服务器配置的超时时间；

    - upsync_interval配置从consul拉取上游服务器配置的间隔时间；

    - upsync_type指定使用consul配置服务器；

    - strong_dependency配置nginx在启动时是否强制依赖配置服务器，如果配置为on，则拉取配置失败时Nginx启动同样失败。

    - upsync_dump_path指定从consul拉取的上游服务器后持久化到的位置，这样即使consul服务器出现问题，本地还有一个备份。

  - 配置 server

    - ```nginx
      server {
          #监听端口，默认使用的是tcp协议，如果需要UDP协议，则配置成listen 3307 udp;
          listen 3307;
          #失败重试
          proxy_next_upstream on;
          proxy_next_upstream_timeout 0;
          proxy_next_upstream_tries 0;
          #超时配置
          #配置与上游服务器连接超时时间，默认60s
          proxy_connect_timeout 1s;
          #配置与客户端上游服务器连接的两次成功读/写操作的超时时间，如果超时，将自动断开连接
          #即连接存活时间，通过它可以释放不活跃的连接，默认10分钟
          proxy_timeout 1m;
          #限速配置
          #从客户端读数据的速率，单位为每秒字节数，默认为0，不限速
          proxy_upload_rate 0;
          #从上游服务器读数据的速率，单位为每秒字节数，默认为0，不限速
          proxy_download_rate 0;
          #上游服务器
          proxy_pass mysql_backend;
      }
      ```

  - 从 Consul 添加上游服务器

    - ```shell
      curl -X PUT -d "{\"weight\":1, \"max_fails\":2, \"fail_timeout\":10}"
      http://192.168.175.100:8500/v1/kv/upstreams/mysql_backend/192.168.175.201:3306
      curl -X PUT -d "{\"weight\":1, \"max_fails\":2, \"fail_timeout\":10}"
      http://192.168.175.100:8500/v1/kv/upstreams/mysql_backend/192.168.175.202:3306
      ```

  - 从 Consul 删除上游服务器

    - ```shell
      curl -X DELETE
      http://192.168.175.100:8500/v1/kv/upstreams/mysql_backend/192.168.175.202:3306
      ```

  - 配置 upstream_show

    - ```nginx
      server {
      	listen 13307;
      	upstream_show;
      }
      ```

    - 配置upstream_show指令后，可以通过curl http://192.168.175.100:13307/upstream_show查看当前动态负载均衡上游服务器列表。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十三：讲讲 Nginx 如何实现四层负载均衡？

## 配置静态文件访问

- 一.首先安装好nginx,启动nginx服务且能够正常访问 Welcome to nginx!界面

- 二.配置静态资源访问核心是配置nginx.conf文件，找到nginx.conf文件

- 三.配置nginx.conf

  - 3.1.在nginx.conf的http节点中添加配置，参考下方格式：

    ```nginx
    server {
        listen       8000;
        listen       somename:8080;
        server_name  somename  alias  another.alias;
    
        location / {
            root   html;
            index  index.html index.htm;
        }
    }
    ```

  - 3.2 解读server节点各参数含义

    - listen：代表nginx要监听的端口
    - server_name:代表nginx要监听的域名
    - location ：nginx拦截路径的匹配规则
    - location块：location块里面表示已匹配请求需要进行的操作

- 四.实例

- 五.重点

  - 重点是理解alias与root的区别，root与alias主要区别在于nginx如何解释location后面的uri，这使两者分别以不同的方式将请求映射到服务器文件上。

  - alias（别名）是一个目录别名。

    ```nginx
    location /123/abc/ {
        root /ABC;
    }
    ```

    当请求 http://qingshan.com/123/abc/logo.png 时，会返回 /ABC/123/abc/logo.png 文件，即用 /ABC 加上 /123/abc。

  - root（根目录）是最上层目录的定义。

    ```nginx
    location /123/abc/ {
        alias /ABC;
    }
    ```

    当请求 http://qingshan.com/123/abc/logo.png 时，会返回 /ABC/logo.png 文件，即用 /ABC 替换 /123/abc。

参考文档：

1. [nginx配置静态资源访问 - 博客园](https://www.cnblogs.com/qingshan-tang/p/12763522.html)

## 限流

- Nginx作为一款高性能的Web代理和负载均衡服务器，往往会部署在一些互联网应用比较前置的位置。此时，我们就可以在Nginx上进行设置，对访问的IP地址和并发数进行相应的限制。

- Nginx官方版本限制IP的连接和并发分别有两个模块：

  - limit_req_zone 用来限制单位时间内的请求数，即速率限制,采用的漏桶算法 "leaky bucket"。
  - limit_req_conn 用来限制同一时间连接数，即并发限制。

- limit_req_zone 参数配置

  - limit_req_zone 参数说明

    - ```nginx
      Syntax: limit_req zone=name [burst=number] [nodelay];
      Default: —
      Context: http, server, location
      limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s;
      ```

      - 第一个参数：`$binary_remote_addr` 表示通过remote_addr这个标识来做限制，“binary_”的目的是缩写内存占用量，是限制同一客户端ip地址。
      - 第二个参数：`zone=one`:10m表示生成一个大小为 10M，名字为 one 的内存区域，用来存储访问的频次信息。
      - 第三个参数：`rate=1r/s` 表示允许相同标识的客户端的访问频次，这里限制的是每秒 1 次，还可以有比如 30r/m 的。

    - ```nginx
      limit_req zone=one burst=5 nodelay;
      ```

      - 第一个参数：`zone=one` 设置使用哪个配置区域来做限制，与上面limit_req_zone 里的name对应。
      - 第二个参数：burst=5，重点说明一下这个配置，burst爆发的意思，这个配置的意思是设置一个大小为 5 的缓冲区当有大量请求（爆发）过来时，超过了访问频次限制的请求可以先放到这个缓冲区内。
      - 第三个参数：nodelay，如果设置，超过访问频次而且缓冲区也满了的时候就会直接返回 503，如果没有设置，则所有请求会等待排队。

  - limit_req_zone示例

    - ```nginx
      http {
          limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s;
          server {
              location /search/ {
              	limit_req zone=one burst=5 nodelay;
              }
          }
      }
      ```

    - 下面配置可以限制特定UA（比如搜索引擎）的访问：

      - ```nginx
        limit_req_zone $anti_spider zone=one:10m rate=10r/s;
        limit_req zone=one burst=100 nodelay;
        if ($http_user_agent ~* "googlebot|bingbot|Feedfetcher-Google") {
        	set $anti_spider $http_user_agent;
        }
        ```

    - 其它参数

      - ```nginx
        Syntax: limit_req_log_level info | notice | warn | error;
        Default:
        limit_req_log_level error;
        Context: http, server, location
        ```

        - 当服务器由于limit被限速或缓存时，配置写入日志。延迟的记录比拒绝的记录低一个级别。例子：limit_req_log_level notice 延迟的的基本是info。

      - ```nginx
        Syntax: limit_req_status code;
        Default:
        limit_req_status 503;
        Context: http, server, location
        ```

        - 设置拒绝请求的返回值。值只能设置 400 到 599 之间。

- ngx_http_limit_conn_module 参数配置

  - ngx_http_limit_conn_module 参数说明

    - 这个模块用来限制单个IP的请求数。并非所有的连接都被计数。只有在服务器处理了请求并且已经读取了整个请求头时，连接才被计数。

    - ```nginx
      Syntax: limit_conn zone number;
      Default: —
      Context: http, server, location
      limit_conn_zone $binary_remote_addr zone=addr:10m;
      
      server {
          location /download/ {
          	limit_conn addr 1;
          }
      }
      ```

      - 一次只允许每个IP地址一个连接。

    - ```nginx
      limit_conn_zone $binary_remote_addr zone=perip:10m;
      limit_conn_zone $server_name zone=perserver:10m;
      server {
          ...
          limit_conn perip 10;
          limit_conn perserver 100;
      }
      ```

      - 可以配置多个limit_conn指令。例如，以上配置将限制每个客户端IP连接到服务器的数量，同时限制连接到虚拟服务器的总数。

    - ```nginx
      Syntax: limit_conn_zone key zone=name:size;
      Default: —
      Context: http
      limit_conn_zone $binary_remote_addr zone=addr:10m;
      ```

      - 在这里，客户端IP地址作为关键。请注意，不是 $remote_addr，而是使用 $binary_remote_addr 变量。
        - $remote_addr 变量的大小可以从7到15个字节不等。存储的状态在32位平台上占用32或64字节的内存，在64位平台上总是占用64字节。
        - 对于IPv4地址，$ binary_remote_addr变量的大小始终为4个字节，对于IPv6地址则为16个字节。存储状态在32位平台上始终占用32或64个字节，在64位平台上占用64个字节。
      - 一个兆字节的区域可以保持大约32000个32字节的状态或大约16000个64字节的状态。如果区域存储耗尽，服务器会将错误返回给所有其他请求。

    - ```nginx
      Syntax: limit_conn_log_level info | notice | warn | error;
      Default:
      limit_conn_log_level error;
      Context: http, server, location
      ```

      - 当服务器限制连接数时，设置所需的日志记录级别。

    - ```nginx
      Syntax: limit_conn_status code;
      Default:
      limit_conn_status 503;
      Context: http, server, location
      ```

      - 设置拒绝请求的返回值。

- Nginx 限流实战

  - 限制访问速率
  - burst 缓存处理
  - nodelay 降低排队时间
    - nodelay 参数要跟 burst 一起使用才有作用。
  - 自定义返回值

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十七：如何使用 Nginx 实现限流？

## 跨域

- **为何会跨域？**

  - 出于浏览器的同源策略限制。
  - **同源策略（Sameoriginpolicy）**是一种约定，它是浏览器最核心也最基本的安全功能，如果缺少了同源策略，则浏览器的正常功能可能都会受到影响。可以说Web是构建在同源策略基础之上的，浏览器只是针对同源策略的一种实现。
  - 同源策略会阻止一个域的 javascript 脚本和另外一个域的内容进行交互。所谓同源（即指在同一个域）就是两个页面具有**相同的协议（protocol），主机（host）和端口号（port）**。

- **Nginx 如何解决跨域？**

  - Nginx 作为反向代理服务器，就是把 http 请求转发到另一个或者一些服务器上。通过把本地一个 url 前缀映射到要跨域访问的web服务器上，就可以实现跨域访问。
  - 对于浏览器来说，访问的就是同源服务器上的一个 url。而 Nginx 通过检测 url 前缀，把 http 请求转发到后面真实的物理服务器。并通过 rewrite 命令把前缀再去掉。这样真实的服务器就可以正确处理请求，并且并不知道这个请求是来自代理服务器的。

- **Nginx 解决跨域案例**

  - 使用 Nginx 解决跨域问题时，我们可以编译 Nginx 的 nginx.conf 配置文件，例如，将 nginx.conf 文件的 server 节点的内容编辑成如下所示。

  - ```nginx
    server {
        location / {
            root html;
            index index.html index.htm;
            //允许cros跨域访问
            add_header 'Access-Control-Allow-Origin' '*';
        }
        //自定义本地路径
        location /apis {
            rewrite ^.+apis/?(.*)$ /$1 break;
            include uwsgi_params;
            proxy_pass http://www.binghe.com;
        }
    }
    ```

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景三十二：Nginx 如何解决跨域问题？

# Docker

## Namespace

- docker 的实现方式

  - 很多开发者都知道，docker容器本质上是宿主机的进程
  - Docker 通过 **namespace 实现了资源隔离**
  - 通过 **cgroups 实现了资源限制**
  - 通过**写时复制机制（copy-on-write）实现了高效的文件操作**。

- 当进一步深入 namespace 和 cgroups 等技术细节时，大部分开发者都会感到茫然无措。尤其是接下来解释libcontainer 的工作原理时，我们会接触大量容器核心知识。所以在这里，希望先带领大家走进linux内核，了解 namespace 和 cgroups 的技术细节。

  - namespace 资源隔离

    - linux内核提拱了6种namespace隔离的系统调用，如下图所示，但是真正的容器还需要处理许多其他工作。

      | namespace | 系统调用参数  | 隔离内容                   |
      | --------- | ------------- | -------------------------- |
      | UTS       | CLONE_NEWUTS  | 主机名或域名               |
      | IPC       | CLONE_NEWIPC  | 信号量、消息队列和共享内存 |
      | PID       | CLONE_NEWPID  | 进程编号                   |
      | Network   | CLONE_NEWNET  | 网络设备、网络战、端口等   |
      | Mount     | CLONE_NEWNS   | 挂载点（文件系统）         |
      | User      | CLONE_NEWUSER | 用户组和用户组             |

    - 实际上，**linux 内核实现 namespace 的主要目的，就是为了实现轻量级虚拟化技术服务**。在同一个 namespace 下的进程合一感知彼此的变化，而对外界的进程一无所知。这样就可以让容器中的进程产生错觉，仿佛自己置身一个独立的系统环境中，以达到隔离的目的。

    - 需要注意的是，本文所讨论的namespace实现针对的是linux内核3.8及以后版本。

    - PID namespace

      - PID namespace隔离非常实用，它对进程PID重新标号，即两个不同namespace下的进程可以有相同的PID。每个PID namespace都有自己的计数程序。
      - 内核为所有的PID namespace维护了一个树状结构，最顶层的是系统初始时创建的，被称为root namespace，它创建的新PID namespace被称为child namespace(树的子节点)，而原来的PID namespace就是新创建的PID namespace的parent namespace(树的父节点)。通过这种方式，不同的PID namespace会形成一个层级体系。
        - 所属的父节点可以看到子节点中的进程，并可以通过信号等方式对子节点中的进程产生影响。反过来，子节点却不能看到父节点PID namespace中的任何内容

    - mount namespace

      - mount namespace通过隔离文件系统挂载点对隔离文件系统提供支持，它是历史上第一个Linux namespace，所以标示位比较特殊，就是CLONE_NEWNS。
      - 隔离后，不同的mount namespace中的文件结构发生变化也互不影响。也可以通过/proc/[pid]/mounts查看到所有挂载在当前namespace中的文件系统，还可以通过/proc/[pid]/mountstats看到mount namespace中文件设备的统计信息，包括挂载文件的名字、文件系统的类型、挂载位置等。

    - 主机上查看一下，docker-run 的进程2117和docker exec进程2354 在同一个namespace (pid对应的namespace的id 是一样)

      - 所以在两个进程中看到的内容是一样的

参考文档：

1. [docker的namespace深入理解 - 博客园](https://www.cnblogs.com/pu20065226/p/13514513.html)

# Linux

## 创建文件

- `touch a.txt`
- `vi a.txt`
- `vim a.txt`
- `> a.txt`
  - 覆盖
- `>> a.txt`
  - 追加
  - 两个箭头指令前面常用的
    - `echo`
    - `cat`
- `cp a.txt /test`
  - 复制一个空文件

参考文档：

1. [Linux创建文件的5种方式 - CSDN](https://blog.csdn.net/xtho62/article/details/118194873)

## 查看端口占用情况

- Linux 查看端口占用情况可以使用 **lsof** 和 **netstat** 命令。

- lsof

  - lsof(list open files) 是一个列出当前系统打开文件的工具。

  - lsof 查看端口占用语法格式：

    ```
    lsof -i:端口号
    ```

  - 更多 lsof 的命令如下：

    ```
    lsof -i:8080：查看8080端口占用
    lsof abc.txt：显示开启文件abc.txt的进程
    lsof -c abc：显示abc进程现在打开的文件
    lsof -c -p 1234：列出进程号为1234的进程所打开的文件
    lsof -g gid：显示归属gid的进程情况
    lsof +d /usr/local/：显示目录下被进程开启的文件
    lsof +D /usr/local/：同上，但是会搜索目录下的目录，时间较长
    lsof -d 4：显示使用fd为4的进程
    lsof -i -U：显示所有打开的端口和UNIX domain文件
    ```

- netstat

  - **netstat -tunlp** 用于显示 tcp，udp 的端口和进程等相关情况。

  - netstat 查看端口占用语法格式：

    ```
    netstat -tunlp | grep 端口号
    ```

    - -t (tcp) 仅显示tcp相关选项
    - -u (udp)仅显示udp相关选项
    - -n 拒绝显示别名，能显示数字的全部转化为数字
    - -l 仅列出在Listen(监听)的服务状态
    - -p 显示建立相关链接的程序名

参考文档：

1. [Linux 查看端口占用情况 - RUNOOB](https://www.runoob.com/w3cnote/linux-check-port-usage.html)

## IO 多路复用

epoll 是现在最先进的 IO 多路复用器，Redis、Nginx、Linux 中的 Java NIO 都使用的是 epoll

|                      | select                                                | poll                                                | epoll                                                        |
| -------------------- | ----------------------------------------------------- | --------------------------------------------------- | ------------------------------------------------------------ |
| 操作方式             | 遍历                                                  | 遍历                                                | 回调                                                         |
| 数据结构             | bitmap                                                | 数组                                                | 红黑树                                                       |
| 最大连接数           | 1024(x86) 或 2048（x64）                              | 无上限                                              | 无上限                                                       |
| 最大支持文件描述符数 | 一般有最大值限制                                      | 65535                                               | 65535                                                        |
| fd 拷贝              | 每次调用 select，都需要把 fd 集合从用户态拷贝到内核态 | 每次调用 poll，都需要把 fd 集合从用户态拷贝到内核态 | fd 首次调用 epoll_ctl 拷贝，每次调用 epoll_wait 不拷贝       |
| 工作效率             | 每次调用都进行线性遍历，时间复杂度为 O(n)             | 每次调用都进行线性遍历，时间复杂度 O(n)             | 事件通知方式，每当 fd 就绪，系统注册的回调函数就会被调用，将就绪 fd 放到 readyList 里面，时间复杂度 O(1) |

# 计算机网络

## HTTP 和 HTTPS 区别

- 加密性
  - HTTP 明文传输，数据都是未加密的，安全性较差
  - HTTPS（SSL+HTTP） 数据传输过程是加密的，安全性较好。
- 认证
  - 使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、GoDaddy 和 GlobalSign 等。
- 速度
  - HTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包
  - 而 HTTPS除了 TCP 的三个包，还要加上 ssl 握手需要的 9 个包，所以一共是 12 个包。
- 端口
  - http 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。
- 服务器资源
  - HTTPS 其实就是建构在 SSL/TLS 之上的 HTTP 协议，所以，要比较 HTTPS 比 HTTP 要更耗费服务器资源。

参考文档：

1. [HTTP 与 HTTPS 的区别 - RUNOOB](https://www.runoob.com/w3cnote/http-vs-https.html)

## HTTPS 保证安全

- HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。
  - SSL：（Secure Socket Layer，安全套接字层），位于可靠的面向连接的**网络层协议**和**应用层协议**之间的一种协议层。SSL通过互相认证、使用数字签名确保完整性、使用加密确保私密性，以实现客户端和服务器之间的安全通讯。该协议由两层组成：SSL记录协议和SSL握手协议。
    - SSL是TLS的前世。
  - TLS：(Transport Layer Security，传输层安全协议)，用于两个应用程序之间提供保密性和数据完整性。该协议由两层组成：TLS 记录协议和 TLS 握手协议。
    - TLS 以 SSL 3.0 为基础于 1999 年作为 SSL 的新版本推出。
    - SSL2.0 和 SSL3.0 已经被 IEFT 组织废弃（分别在 2011 年，2015 年）多年来，在被废弃的 SSL 协议中一直存在漏洞并被发现 (e.g. POODLE, DROWN)。大多数现代浏览器遇到使用废弃协议的 web 服务时，会降低用户体验（红线穿过挂锁标志或者https表示警告）来表现。
      - 因为这些原因，你应该在服务端禁止使用 SSL 协议，仅仅保留 TLS 协议开启。
- 常规的 HTTP 通信，有以下的问题。
  1. **窃听风险**（eavesdropping）：第三方可以获知通信内容。
  2. **篡改风险**（tampering）：第三方可以修改通信内容。
  3. **冒充风险**（pretending）：第三方可以冒充他人身份参与通信。
- 因此，SSL/TLS 协议就是为了解决这三大风险而设计的，希望达到：
  1. 所有信息都是**加密传播**，第三方无法窃听。
  2. 具有**校验机制**，一旦被篡改，通信双方会立刻发现。
  3. 配备**身份证书**，防止身份被冒充。

参考文档：

1. [Https 是如何保证安全的？ - 知乎](https://zhuanlan.zhihu.com/p/110216210)

## HTTP 1.0、1.1 和 2.0 的区别

- 结论
  - 结论1：从 HTTP/1.0 到 HTTP/2，都是利用 TCP 作为底层协议进行通信的。
  - 结论2：HTTP/1.1，引进了长连接(keep-alive)，减少了建立和关闭连接的消耗和延迟。
  - 结论3：HTTP/2，引入了多路复用：连接共享，提高了连接的利用率，降低延迟。

参考文档：

1. [HTTP1.0、HTTP1.1 和 HTTP2.0 的区别 - 知乎](https://zhuanlan.zhihu.com/p/370862112)

## TCP 可靠传输

- 实现了可靠性传输
  - 检验和
    - 通过检验和的方式，接收端可以检测出来数据是否有差错和异常，假如有差错就会直接丢弃TCP段，重新发送
  - 序列号/确认应答
    - 只要发送端有一个包传输，接收端没有回应确认包（ACK包），都会重发。或者接收端的应答包，发送端没有收到也会重发数据。这就可以保证数据的完整性。
  - 超时重传
    - 超时重传是指发送出去的数据包到接收到确认包之间的时间，如果超过了这个时间会被认为是丢包了，需要重传。
  - 最大消息长度
    - 在建立TCP连接的时候，双方约定一个最大的长度（MSS）作为发送的单位，重传的时候也是以这个单位来进行重传。理想的情况下是该长度的数据刚好不被网络层分块。
  - 滑动窗口控制
    - 我们上面提到的超时重传的机制存在效率低下的问题，发送一个包到发送下一个包要经过一段时间才可以。所以我们就想着能不能不用等待确认包就发送下一个数据包呢？这就提出了一个滑动窗口的概念。
    - 窗口的大小就是在无需等待确认包的情况下，发送端还能发送的最大数据量。这个机制的实现就是使用了大量的缓冲区，通过对多个段进行确认应答的功能。通过下一次的确认包可以判断接收端是否已经接收到了数据，如果已经接收了就从缓冲区里面删除数据。
  - 拥塞控制
    - 窗口控制解决了 两台主机之间因传送速率而可能引起的丢包问题，在一方面保证了TCP数据传送的可靠性。然而如果网络非常拥堵，此时再发送数据就会加重网络负担，那么发送的数据段很可能超过了最大生存时间也没有到达接收方，就会产生丢包问题。为此TCP引入慢启动机制，先发出少量数据，就像探路一样，先摸清当前的网络拥堵状态后，再决定按照多大的速度传送数据。

参考文档：

1. [TCP的可靠性传输是如何保证的 - 知乎](https://zhuanlan.zhihu.com/p/112317245)

## TCP 三次握手

- 过程
  - **第一次握手**：
    - 客户端将TCP报文**标志位SYN置为1**，随机产生一个序号值seq=J，保存在TCP首部的序列号(Sequence Number)字段里，指明客户端打算连接的服务器的端口，并将该数据包发送给服务器端
    - 发送完毕后，客户端进入`SYN_SENT`状态，等待服务器端确认。
  - **第二次握手**：
    - 服务器端收到数据包后由标志位SYN=1知道客户端请求建立连接
    - 服务器端将TCP报文**标志位SYN和ACK都置为1**，ack=J+1，随机产生一个序号值seq=K，并将该数据包发送给客户端以确认连接请求
    - 服务器端进入`SYN_RCVD`状态。
  - **第三次握手**：
    - 客户端收到确认后，检查ack是否为 J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给服务器端
    - 服务器端检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，客户端和服务器端进入`ESTABLISHED`状态
    - 完成三次握手，随后客户端与服务器端之间可以开始传输数据了。
- 为什么需要三次
  - 我们假设client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。
  - 假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。
  - 所以，采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。

参考文档：

1. [一文彻底搞懂 TCP三次握手、四次挥手过程及原理 - 知乎](https://zhuanlan.zhihu.com/p/108504297)

## TCP 半连接状态

- 客户端向服务端发起连接请求，服务器第一次收到客户端的SYN之后，就会处于**SYN_RCVD状态**，此时双方还没有完全建立其连接，服务器会把此种状态下请求连接放到一个队列里，我们将这种队列称之为**半连接队列**。
- **全连接队列**就是已经完成了三次握手，建立起连接就会放到全连接队列中去，如果全连接队列满了就可能会出现丢包的现象。
- 补充关于SYN-ACK重传次数的问题
  - 服务器发送完SYN-ACK包，如果未收到客户端确认包，服务器进行首次重传，等待一段时间仍未收到客户确认包，进行第二次重传。如果重传次数超过系统规定的最大重传次数，系统将该连接信息从半连接队列中删除。
  - 每次重传等待的时间一般不同，一般是指数增长。如时间间隔是1，2，4，8

参考文档：

1. [TCP之半连接状态 - CSDN](https://blog.csdn.net/weixin_44390164/article/details/119077124)

## TCP 四次挥手

- 四次挥手即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。

- 挥手请求可以是 Client 端，也可以是 Server 端发起的，我们假设是Client端发起：

  - **第一次挥手**： 
    - Client 端发起挥手请求，向Server端发送标志位是FIN报文段，设置序列号seq
    - 此时，Client端进入`FIN_WAIT_1`状态，这表示Client端没有数据要发送给Server端了。
  - **第二次分手**
    - Server 端收到了 Client 端发送的 FIN 报文段进入 `CLOSE_WAIT` 状态
    - Server 向 Client 端返回一个标志位是ACK的报文段，ack 设为 seq 加 1，Client 端收到后进入`FIN_WAIT_2`状态
    - Server端告诉Client端，我确认并同意你的关闭请求。
  - **第三次分手**： 
    - Server 端向 Client 端发送标志位是FIN的报文段，请求关闭连接
    - 同时 Server 端进入`LAST_ACK`状态。
  - **第四次分手** ： 
    - Client 端收到 Server 端发送的FIN报文段，向 Server 端发送标志位是 ACK 的报文段，然后 Client 端进入`TIME_WAIT` 状态。
    - Server端收到Client端的 ACK 报文段以后，就关闭连接。
    - 此时，Client 端等待 **2MSL** 的时间后依然没有收到回复，则证明 Server 端已正常关闭，那好，Client端也可以关闭连接了。

- 为什么需要四次

  - 建立连接时因为当Server端收到Client端的SYN连接请求报文后，可以直接发送**SYN+ACK**报文。其中ACK报文是用来应答的，SYN报文是用来同步的。所以建立连接只需要三次握手。
  - 由于TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议，TCP是**全双工模式**。这就意味着，关闭连接时，当Client端发出FIN报文段时，只是表示Client端告诉Server端数据已经发送完毕了。当Server端收到FIN报文并返回ACK报文段，表示它已经知道Client端没有数据发送了，但是Server端还是可以发送数据到Client端的，所以Server很可能并不会立即关闭SOCKET，直到Server端把数据也发送完毕。当Server端也发送了FIN报文段时，这个时候就表示Server端也没有数据要发送了，就会告诉Client端，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。

- 为什么要等待2MSL？

  **MSL**：报文段最大生存时间，它是任何报文段被丢弃前在网络内的最长时间。有以下两个原因：

  - **第一点：保证TCP协议的全双工连接能够可靠关闭**：
    由于IP协议的不可靠性或者是其它网络原因，导致了Server端没有收到Client端的ACK报文，那么Server端就会在超时之后重新发送FIN，如果此时Client端的连接已经关闭处于`CLOESD`状态，那么重发的FIN就找不到对应的连接了，从而导致连接错乱，所以，Client端发送完最后的ACK不能直接进入`CLOSED`状态，而要保持`TIME_WAIT`，当再次收到FIN的收，能够保证对方收到ACK，最后正确关闭连接。
  - **第二点：保证这次连接的重复数据段从网络中消失**
    如果Client端发送最后的ACK直接进入`CLOSED`状态，然后又再向Server端发起一个新连接，这时不能保证新连接的与刚关闭的连接的端口号是不同的，也就是新连接和老连接的端口号可能一样了，那么就可能出现问题：如果前一次的连接某些数据滞留在网络中，这些延迟数据在建立新连接后到达Client端，由于新老连接的端口号和IP都一样，TCP协议就认为延迟数据是属于新连接的，新连接就会接收到脏数据，这样就会导致数据包混乱。所以TCP连接需要在TIME_WAIT状态等待2倍MSL，才能保证本次连接的所有数据在网络中消失。

参考文档：

1. [一文彻底搞懂 TCP三次握手、四次挥手过程及原理 - 知乎](https://zhuanlan.zhihu.com/p/108504297)

## OSI 七层架构

- 七层模型，亦称OSI（Open System Interconnection）参考模型，是参考模型是国际标准化组织（ISO）制定的一个用于计算机或通信系统间互联的标准体系。它是一个七层的、抽象的模型体，不仅包括一系列抽象的术语或概念，也包括具体的协议。
- 分层如下。
  - **应用层 (Application)**
    - 网络服务与最终用户的一个接口。协议有：HTTP FTP TFTP SMTP SNMP DNS TELNET HTTPS POP3 DHCP
  - **表示层（Presentation Layer）**
    - 数据的表示、安全、压缩。（在五层模型里面已经合并到了应用层）格式有，JPEG、ASCll、DECOIC、加密格式等
  - **会话层（Session Layer）**
    - 建立、管理、终止会话。（在五层模型里面已经合并到了应用层）对应主机进程，指本地主机与远程主机正在进行的会话
  - **传输层 (Transport)**
    - 定义传输数据的协议端口号，以及流控和差错校验。协议有：TCP UDP，数据包一旦离开网卡即进入网络传输层
  - **网络层 (Network)**
    - 进行逻辑地址寻址，实现不同网络之间的路径选择。协议有：ICMP IGMP IP（IPV4 IPV6） ARP RARP
  - **数据链路层 (Link)**
    - 建立逻辑连接、进行硬件地址寻址、差错校验等功能。（由底层网络定义协议）将比特组合成字节进而组合成帧，用MAC地址访问介质，错误发现但不能纠正。
  - **物理层（Physical Layer）**
    - 建立、维护、断开物理连接。（由底层网络定义协议）

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景十二：讲讲七层网络模型与 TCP 三次握手与四次断开？ - OSI 七层架构

## 从输入 URL 到页面展示的过程

- 1、输入网址
- 2、DNS 解析
- 3、建立 tcp 连接
- 4、客户端发送 HTTP 请求
- 5、服务器处理请求
  - （可能出现重定向，可能的 Nginx 反向代理）
- 6、服务器响应请求
  - 状态码
    - 1xx：信息性状态码，表示服务器已接收了客户端请求，客户端可继续发送请求。
      - 100 Continue
      - 101 Switching Protocols
    - 2xx：成功状态码，表示服务器已成功接收到请求并进行处理。
      - 200 OK 表示客户端请求成功
      - 204 No Content 成功，但不返回任何实体的主体部分
      - 206 Partial Content 成功执行了一个范围（Range）请求
    - 3xx：重定向状态码，表示服务器要求客户端重定向。
      - 301 Moved Permanently 永久性重定向，响应报文的Location首部应该有该资源的新URL
      - 302 Found 临时性重定向，响应报文的Location首部给出的URL用来临时定位资源
      - 303 See Other 请求的资源存在着另一个URI，客户端应使用GET方法定向获取请求的资源
      - 304 Not Modified 服务器内容没有更新，可以直接读取浏览器缓存
      - 307 Temporary Redirect 临时重定向。与302 Found含义一样。302禁止POST变换为GET，但实际使用时并不一定，307则更多浏览器可能会遵循这一标准，但也依赖于浏览器具体实现
    - 4xx：客户端错误状态码，表示客户端的请求有非法内容。
      - 400 Bad Request 表示客户端请求有语法错误，不能被服务器所理解
      - 401 Unauthonzed 表示请求未经授权，该状态代码必须与 WWW-Authenticate 报头域一起使用
      - 403 Forbidden 表示服务器收到请求，但是拒绝提供服务，通常会在响应正文中给出不提供服务的原因
      - 404 Not Found 请求的资源不存在，例如，输入了错误的URL
    - 5xx：服务器错误状态码，表示服务器未能正常处理客户端的请求而出现意外错误。
      - 500 Internel Server Error 表示服务器发生不可预期的错误，导致无法完成客户端的请求
      - 503 Service Unavailable 表示服务器当前不能够处理客户端的请求，在一段时间之后，服务器可能会恢复正常
- 7、浏览器展示 HTML
  - 解析过程中，浏览器首先会解析HTML文件构建DOM树，然后解析CSS文件构建渲染树，等到渲染树构建完成后，浏览器开始布局渲染树并将其绘制到屏幕上。
- 8、浏览器发送请求获取其他在 HTML 中的资源
  - （部分可能从 CDN 获取；部分可能浏览器缓存）

参考文档：

1. [从输入URL到页面展示的详细过程 - CSDN](https://blog.csdn.net/wlk2064819994/article/details/79756669)

# 分布式系统

## CAP 理论

- CAP
  - Consistency（一致性）
    - 对于客户端的每次读操作，要么读到的是最新的数据，要么读取失败。换句话说，一致性是站在分布式系统的角度，对访问本系统的客户端的一种承诺：要么我给您返回一个错误，要么我给你返回绝对一致的最新数据，不难看出，其强调的是数据正确。
  - Availability（可用性）
    - 任何客户端的请求都能得到响应数据，不会出现响应错误。换句话说，可用性是站在分布式系统的角度，对访问本系统的客户的另一种承诺：我一定会给您返回数据，不会给你返回错误，但不保证数据最新，强调的是不出错。
  - Partition tolerance（分区容忍性）
    - 由于分布式系统通过网络进行通信，网络是不可靠的。当任意数量的消息丢失或延迟到达时，系统仍会继续提供服务，不会挂掉。换句话说，分区容忍性是站在分布式系统的角度，对访问本系统的客户端的再一种承诺：我会一直运行，不管我的内部出现何种数据同步问题，强调的是不挂掉。
- 权衡 C、A
  - 对于一个分布式系统而言，P是前提，必须保证，因为只要有网络交互就一定会有延迟和数据丢失，这种状况我们必须接受，必须保证系统不能挂掉。
  - 当选择了C（一致性）时，如果由于网络分区而无法保证特定信息是最新的，则系统将返回错误或超时。
  - 当选择了A（可用性）时，系统将始终处理客户端的查询并尝试返回最新的可用的信息版本，即使由于网络分区而无法保证其是最新的。
- 为什么不能同时满足三个
  - 可以举例：A 更新数据，此时 A、B 网络断开，B 未能同步，此时需要保证分区容错性的前提下，B 无法在满足一致性和可用性的同时返回最新的数据

参考文档：

1. [轻松理解CAP理论 - 知乎](https://zhuanlan.zhihu.com/p/50990721)

# 设计模式

## 单例模式

- 实现方式

  1. 懒汉式，线程不安全

     **是否 Lazy 初始化：**是

     **是否多线程安全：**否

     ```java
     public class Singleton {  
         private static Singleton instance;  
         private Singleton (){}  
       
         public static Singleton getInstance() {  
             if (instance == null) {  
                 instance = new Singleton();  
             }  
             return instance;  
         }  
     }
     ```

  2. 懒汉式，线程安全

     **是否 Lazy 初始化：**是

     **是否多线程安全：**是

     ```java
     public class Singleton {  
         private static Singleton instance;  
         private Singleton (){}  
         public static synchronized Singleton getInstance() {  
             if (instance == null) {  
                 instance = new Singleton();  
             }  
             return instance;  
         }  
     }
     ```

  3. 饿汉式

     **是否 Lazy 初始化：**否

     **是否多线程安全：**是

     ```java
     public class Singleton {  
         private static Singleton instance = new Singleton();  
         private Singleton (){}  
         public static Singleton getInstance() {  
         return instance;  
         }  
     }
     ```

  4. 双检锁/双重校验锁（DCL，即 double-checked locking）

     **是否 Lazy 初始化：**是

     **是否多线程安全：**是

     ```java
     public class Singleton {  
         private volatile static Singleton singleton;  
         private Singleton (){}  
         public static Singleton getSingleton() {  
             if (singleton == null) {  
                 synchronized (Singleton.class) {  
                     if (singleton == null) {  
                         singleton = new Singleton();  
                     }  
                 }  
             }  
             return singleton;  
         }  
     }
     ```

  5. 登记式/静态内部类

     **是否 Lazy 初始化：**是

     **是否多线程安全：**是

     ```java
     public class Singleton {  
         private static class SingletonHolder {  
         private static final Singleton INSTANCE = new Singleton();  
         }  
         private Singleton (){}  
         public static final Singleton getInstance() {  
             return SingletonHolder.INSTANCE;  
         }  
     }
     ```

  6. 枚举

     **是否 Lazy 初始化：**否

     **是否多线程安全：**是

     ```java
     public enum Singleton {  
         INSTANCE;  
         public void whateverMethod() {  
         }  
     }
     ```

  7. 单例注册表

     单例注册表是Spring中Bean单例的核心实现方案。可以通过一个ConcurrentHashMap存储Bean对象，保证Bean名称唯一的情况下也能保证线程安全。

参考文档：

1. [单例模式 - RUNOOB](https://www.runoob.com/design-pattern/singleton-pattern.html)
2. [详解单例模式及其在Sping中的最优实践 - 知乎](https://zhuanlan.zhihu.com/p/442496004)

# 系统设计

## 大数据量：两个文件对比找相同

- **题目描述**
  - 给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，内存限制是 4G。请找出 a、b 两个文件共同的 URL。
- **解答思路**
  - 每个 URL 占 64B，那么 50 亿个 URL占用的空间大小约为 320GB。
    - 5,000,000,000 * 64B ≈ 5GB * 64 = 320GB
  - 由于内存大小只有 4G，因此，我们不可能一次性把所有 URL 加载到内存中处理。对于这种类型的题目，一般采用分治策略，即：把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。
- **思路如下：**
  - 首先遍历文件 a，对遍历到的 URL 求 hash(URL) % 1000，根据计算结果把遍历到的 URL 存储到 a0, a1, a2, …, a999，这样每个大小约为 300MB。
  - 使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, …, b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, …, a999 对应 b999，不对应的小文件不可能有相同的 URL。
  - 那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。
  - 接着遍历 ai( i∈[0,999])，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。
- **方法总结**
  - 分而治之，进行哈希取余；
  - 对每个子文件进行 HashSet 统计。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十六：BAT 大数据面试题 - 1、如何从大量的 URL 中找出相同的 URL？

## 大数据量：频度排序

- **题目描述**
  - 有 10 个文件，每个文件大小为 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求按照 query 的频度排序。
- **解答思路**
  - 如果 query 的重复度比较大，可以考虑一次性把所有 query 读入内存中处理；如果 query 的重复率不高，那么可用内存不足以容纳所有的 query，这时候就需要采用分治法或其他的方法来解决。
- **方法一：HashMap 法**
  - 如果 query 重复率高，说明不同 query 总数比较小，可以考虑把所有的 query 都加载到内存中的HashMap 中。接着就可以按照 query 出现的次数进行排序。
- **方法二：分治法**
  - 分治法需要根据数据量大小以及可用内存的大小来确定问题划分的规模。
  - 对于这道题，可以顺序遍历 10 个文件中的 query，通过 Hash 函数 hash(query) % 10 把这些 query 划分到 10 个小文件中。
  - 之后对每个小文件使用 HashMap 统计 query 出现次数，根据次数排序并写入到零外一个单独文件中。
  - 接着对所有文件按照 query 的次数进行排序，这里可以使用归并排序（由于无法把所有 query 都读入内存，因此需要使用外排序）。
- **方法总结**
  - 内存若够，直接读入进行排序；
  - 内存不够，先划分为小文件，小文件排好序后，整理使用外排序进行归并。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十六：BAT 大数据面试题 - 2、如何按照 query 的频度排序？（百度）

## 大数据量：去重后个数

- 8 位数
  - **题目描述**
    - 已知某个文件内包含一些电话号码，每个号码为 8 位数字，统计不同号码的个数。
  - **解答思路**
    - 这道题本质还是求解数据重复的问题，对于这类问题，一般首先考虑位图法。
    - 对于本题，8 位电话号码可以表示的号码个数为 10^8 个，即 1 亿个。我们每个号码用一个 bit 来表示，则总共需要 1 亿个 bit，内存占用约 100M。
  - **思路如下：**
    - 申请一个位图数组，长度为 1 亿，初始化为 0。
    - 然后遍历所有电话号码，把号码对应的位图中的位置置为 1。
    - 遍历完成后，如果 bit 为 1，则表示这个电话号码在文件中存在，否则不存在。bit 值为 1 的数量即为 不同电话号码的个数。
  - **方法总结**
    - 求解数据重复问题，记得考虑位图法。
- 2.5 亿个整数
  - **题目描述**
    - 在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。
  - **解答思路**
    - **方法一：分治法**
      - 与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。
    - **方法二：位图法**
      - 位图，就是用一个或多个 bit 来标记某个元素对应的值，而键就是该元素。采用位作为单位来存储数据，可以大大节省存储空间。
      - 位图通过使用位数组来表示某些元素是否存在。它可以用于快速查找，判重，排序等。不是很清楚？我先举个小例子。
    - 对于整数相关的算法的求解，位图法是一种非常实用的算法。假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 2^32。
    - 那么对于这道题，我们用 2 个 bit 来表示各个数字的状态：
      - 00 表示这个数字没出现过；
      - 01 表示这个数字出现过一次（即为题目所找的不重复整数）；
      - 10 表示这个数字出现了多次。
    - 那么这 2^32 个整数，总共所需内存为 2^32*2b=1GB。因此，当可用内存超过 1GB 时，可以采用位图法。
    - 假设内存满足位图法需求，进行下面的操作：
      - 遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。
  - **方法总结**
    - 判断数字是否重复的问题，位图法是一种非常高效的方法。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十六：BAT 大数据面试题 - 3、如何统计不同电话号码的个数？（百度）; 7、如何在大量的数据中找出不重复的整数？（百度）

## 大数据量：判断存在性

- **题目描述**
  - 给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中？
- **解答思路**
  - **方法一：分治法**
    - 依然可以用分治法解决，方法与前面类似，就不再次赘述了。
  - **方法二：位图法**
    - 40 亿个不重复整数，我们用 40 亿个 bit 来表示，初始位均为 0，那么总共需要内存：4,000,000,000b≈512M。
    - 我们读取这 40 亿个整数，将对应的 bit 设置为 1。接着读取要查询的数，查看相应位是否为 1，如果为 1 表示存在，如果为 0 表示不存在。
- **方法总结**
  - 判断数字是否存在、判断数字是否重复的问题，位图法是一种非常高效的方法。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十六：BAT 大数据面试题 - 8、如何在大量的数据中判断一个数是否存在？（腾讯）

## 大数据量：中位数

- **题目描述**
  - 从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为第 (N+1)/2 个数；当样本数为偶数时，中位数为 第 N/2 个数与第 1+N/2 个数的均值。
- **解答思路**
  - 如果这道题没有内存大小限制，则可以把所有数读到内存中排序后找出中位数。但是最好的排序算法的时间复杂度都为 O(NlogN)。这里使用其他方法。
  - **方法一：双堆法**
    - 维护两个堆，一个大顶堆，一个小顶堆。大顶堆中最大的数小于等于小顶堆中最小的数；保证这两个堆中的元素个数的差不超过 1。
    - 若数据总数为偶数，当这两个堆建好之后，中位数就是这两个堆顶元素的平均值。
    - 当数据总数为奇数时，根据两个堆的大小，中位数一定在数据多的堆的堆顶。
  - 以上这种方法，需要把所有数据都加载到内存中。当数据量很大时，就不能这样了，因此，这种方法适用于数据量较小的情况。5 亿个数，每个数字占用 4B，总共需要 2G 内存。如果可用内存不足 2G，就不能使用这种方法了，下面介绍另一种方法。
  - **方法二：分治法**
    - 分治法的思想是把一个大的问题逐渐转换为规模较小的问题来求解。
    - 对于这道题，顺序读取这 5 亿个数字，对于读取到的数字 num，如果它对应的二进制中最高位为1，则把这个数字写到 f1 中，否则写入 f0 中。通过这一步，可以把这 5 亿个数划分为两部分，而且 f0 中的数都大于 f1 中的数（最高位是符号位）。
    - 划分之后，可以非常容易地知道中位数是在 f0 还是 f1 中。假设 f1 中有 1 亿个数，那么中位数一定在 f0 中，且是在 f0 中，从小到大排列的第 1.5 亿个数与它后面的一个数的平均值。
    - 提示，5 亿数的中位数是第 2.5 亿与右边相邻一个数求平均值。若 f1 有一亿个数，那么中位数就是 f0 中从第 1.5 亿个数开始的两个数求得的平均值。
    - 对于 f0 可以用次高位的二进制继续将文件一分为二，如此划分下去，直到划分后的文件可以被加载到内存中，把数据加载到内存中以后直接排序，找出中位数。
    - 注意，当数据总数为偶数，如果划分后两个文件中的数据有相同个数，那么中位数就是数据较小的文件中的最大值与数据较大的文件中的最小值的平均值。
- **方法总结**
  - 分治法，真香！

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十六：BAT 大数据面试题 - 4、如何从 5 亿个数中找出中位数？（百度）

## 大数据量：Top N

- Top N
  - **题目描述**
    - 有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)。
  - **解答思路**
    - 由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用分治策略，把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。
  - **思路如下：**
    - 首先遍历大文件，对遍历到的每个词x，执行 hash(x) % 5000，将结果为 i 的词存放到文件 ai 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。
    - 接着统计每个小文件中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。具体方法是：对于遍历到的词 x，如果在 map 中不存在，则执行 map.put(x, 1)；若存在，则执行 map.put(x, map.get(x)+1)，将该词频数加 1。
    - 上面我们统计了每个小文件单词出现的频数。接下来，我们可以通过维护一个小顶堆来找出所有词中出现频数最高的 100 个。具体方法是：依次遍历每个小文件，构建一个小顶堆，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为小顶堆，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。
  - **方法总结**
    - 分而治之，进行哈希取余；
    - 使用 HashMap 统计频数；
    - 求解最大的 TopN 个，用小顶堆；求解最小的 TopN 个，用大顶堆。
- Top 1
  - **题目描述**
    - 现有海量日志数据保存在一个超大文件中，该文件无法直接读入内存，要求从中提取某天访问百度次数最多的那个 IP。
  - **解答思路**
    - 这道题只关心某一天访问百度最多的 IP，因此，可以首先对文件进行一次遍历，把这一天访问百度 IP 的相关信息记录到一个单独的大文件中。接下来采用的方法与上一题一样，大致就是先对 IP 进行哈希映射，接着使用 HashMap 统计重复 IP 的次数，最后计算出重复次数最多的 IP。
    - 注：这里只需要找出出现次数最多的 IP，可以不必使用堆，**直接用一个变量 max 即可**。
  - **方法总结**
    - 分而治之，进行哈希取余；
    - 使用 HashMap 统计频数；
    - 求解最大的 TopN 个，用小顶堆；求解最小的 TopN 个，用大顶堆。
- 字符串 Top N
  - **题目描述**
    - 搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询床的长度不超过 255 字节。
    - 假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。）
  - **解答思路**
    - 每个查询串最长为 255B，1000w 个串需要占用 约 2.55G 内存，因此，我们无法将所有字符串全部读入到内存中处理。
    - **方法一：分治法**
      - 分治法依然是一个非常实用的方法。
      - 划分为多个小文件，保证单个小文件中的字符串能被直接加载到内存中处理，然后求出每个文件中出现次数最多的 10 个字符串；最后通过一个小顶堆统计出所有文件中出现最多的 10 个字符串。
      - 方法可行，但不是最好，下面介绍其他方法。
    - **方法二：HashMap 法**
      - 虽然字符串总数比较多，但去重后不超过 300w，因此，可以考虑把所有字符串及出现次数保存在一个 HashMap 中，所占用的空间为 300w*(255+4)≈777M（其中，4表示整数占用的4个字节）。由此可见，1G 的内存空间完全够用。
      - 思路如下：
        - 首先，遍历字符串，若不在 map 中，直接存入 map，value 记为 1；若在 map 中，则把对应的 value 加 1，这一步时间复杂度 O(N)。
        - 接着遍历 map，构建一个 10 个元素的小顶堆，若遍历到的字符串的出现次数大于堆顶字符串的出现次数，则进行替换，并将堆调整为小顶堆。
        - 遍历结束后，堆中 10 个字符串就是出现次数最多的字符串。这一步时间复杂度 O(Nlog10)。
    - **方法三：前缀树法**
      - 方法二使用了 HashMap 来统计次数，当这些字符串有大量相同前缀时，可以考虑使用前缀树来统计字符串出现的次数，树的结点保存字符串出现次数，0 表示没有出现。
      - **思路如下：**
        - 在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构建新结点，构建完成后把叶子结点中字符串的出现次数置为 1。
        - 最后依然使用小顶堆来对字符串的出现次数进行排序。
      - **方法总结**
        - 前缀树经常被用来统计字符串的出现次数。它的另外一个大的用途是字符串查找，判断是否有重复的字符串等。

参考文档：

1. 《我要进大厂系列之面试圣经（第1版）.pdf》场景二十六：BAT 大数据面试题 - 5、如何从大量数据中找出高频词？（百度）；6、如何找出某一天访问百度网站最多的 IP？（百度）；9、如何查询最热门的查询串？（腾讯）

# 算法

## 堆排序

```java
/**
 * 堆排序演示
 *
 * @author Lvan
 */
public class HeapSort {
    public static void main(String[] args) {
//        int[] arr = {5, 1, 7, 3, 1, 6, 9, 4};
        int[] arr = {16, 7, 3, 20, 17, 8};
        heapSort(arr);
        for (int i : arr) {
            System.out.print(i + " ");
        }
    }

    /**
     * 创建堆，
     * @param arr 待排序列
     */
    private static void heapSort(int[] arr) {
        //创建堆
        for (int i = (arr.length - 1) / 2; i >= 0; i--) {
            //从第一个非叶子结点从下至上，从右至左调整结构
            adjustHeap(arr, i, arr.length);
        }
        //调整堆结构+交换堆顶元素与末尾元素
        for (int i = arr.length - 1; i > 0; i--) {
            //将堆顶元素与末尾元素进行交换
            int temp = arr[i];
            arr[i] = arr[0];
            arr[0] = temp;
            //重新对堆进行调整
            adjustHeap(arr, 0, i);
        }
    }

    /**
     * 调整堆
     * @param arr 待排序列
     * @param parent 父节点
     * @param length 待排序列尾元素索引
     */
    private static void adjustHeap(int[] arr, int parent, int length) {
        //将temp作为父节点
        int temp = arr[parent];
        //左孩子
        int lChild = 2 * parent + 1;
        while (lChild < length) {
            //右孩子
            int rChild = lChild + 1;
            // 如果有右孩子结点，并且右孩子结点的值大于左孩子结点，则选取右孩子结点
            if (rChild < length && arr[lChild] < arr[rChild]) {
                lChild++;
            }
            // 如果父结点的值已经大于孩子结点的值，则直接结束
            if (temp >= arr[lChild]) {
                break;
            }
            // 把孩子结点的值赋给父结点
            arr[parent] = arr[lChild];

            //选取孩子结点的左孩子结点,继续向下筛选
            parent = lChild;
            lChild = 2 * lChild + 1;
        }
        arr[parent] = temp;
    }
}
```

参考文档：

1. [堆排序——Java实现 - 博客园](https://www.cnblogs.com/luomeng/p/10618709.html)

## 快速排序

```java
public class QuickSort {
	private void swap(int[] arr, int i, int j) {
		int temp = arr[i];
		arr[i] = arr[j];
		arr[j] = temp;
	}
	
	public void quickSort(int[] arr, int start, int end) {
		if (start >= end)
			return;
		int k = arr[start];
		int i = start, j = end;
		while (i != j) {
			while (i < j && arr[j] >= k)
				--j;
			swap(arr, i, j);
			while (i < j && arr[i] <= k)
				++i;
			swap(arr, i, j);
		}
		quickSort(arr, start, i - 1);
		quickSort(arr, i + 1, end);
	}
	
	public static void main(String[] args) {
		int[] arr = {5, 2, 6, 9, 1, 3, 4, 8, 7, 10};
		new QuickSort().quickSort(arr, 0, arr.length - 1);
		System.out.println(Arrays.toString(arr));
	}
}
```

参考文档：

1. [快速排序Java代码简洁实现 - 知乎](https://zhuanlan.zhihu.com/p/144738954)

## Java 排序实现

- Collections.sort()排序有两种实现方式
  - 一是让元素类实现Comparable接口并覆盖compareTo()方法
  - 二是给Collecitons.sort()方法传入比较器，通常采用匿名内部内的方式传入。
- Collections.sort() 通过调用Arrays.sort()方法进行排序
  - 在Java1.6+中，如果集合大小<32则采用Tim-Sort算法，如果>=32则采用归并排序。
  - ComparableTimSort.sort()
    - 如果**2<= nRemaining <=32**,即MIN_MERGE的初始值，表示需要排序的数组是小数组，**可以使用mini-TimSort方法进行排序**
      - mini-TimSort排序方法：先找出数组中从下标为0开始的第一个升序序列，或者找出降序序列后转换为升序重新放入数组，将这段升序数组作为初始数组，将之后的每一个元素通过二分法排序插入到初始数组中。注意，这里就调用到了我们重写的compareTo()方法了。
    - **否则 nRemaining > 32 需要使用归并排序**。

参考文档

1. [Java集合排序功能实现分析 - 博客园](https://www.cnblogs.com/dudadi/p/8007167.html)

